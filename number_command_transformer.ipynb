{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb41ff5d-9d85-4c94-9437-83b674b9425b",
   "metadata": {},
   "source": [
    "\n",
    "Exploration of a \"number command\" transformer.\n",
    "\n",
    "Depending on which command token is after a sequence of digit's, the network is asked to perform different tasks.\n",
    "\n",
    "example sequences:\n",
    "```\n",
    "<sos> 33358 <sum> 22 <eos>\n",
    "<sos> 00487 <sum> 19 <eos>\n",
    "<sos> 840 <sum> 12 <eos>\n",
    "<sos> 70996 <sum> 31 <eos>\n",
    "<sos> 2778 <sum> 24 <eos>\n",
    "```\n",
    "\n",
    "The transformer model is based on Adrej Karpathy's NanoGPT\n",
    "https://github.com/karpathy/nanoGPT/tree/master\n",
    "\n",
    "\n",
    "\n",
    "Notebook by Justin Thomas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e21ebe7-b0ca-4b5a-9583-ee38857d4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 33358 <sum> 22 <eos>\n",
      "<sos> 00487 <sum> 19 <eos>\n",
      "<sos> 840 <sum> 12 <eos>\n",
      "<sos> 70996 <sum> 31 <eos>\n",
      "<sos> 2778 <sum> 24 <eos>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_numbers_and_sum():\n",
    "    \"\"\"\n",
    "    Generate 3–5 random digits and output:\n",
    "    <sos> digits <sum> total <eos>\n",
    "    \"\"\"\n",
    "    # Pick a random length between 3 and 5\n",
    "    length = random.randint(3, 5)\n",
    "    \n",
    "    # Generate that many random digits\n",
    "    numbers = [random.randint(0, 9) for _ in range(length)]\n",
    "    \n",
    "    # Calculate the sum of the numbers\n",
    "    total_sum = sum(numbers)\n",
    "    \n",
    "    # Join the numbers into a single string without commas\n",
    "    numbers_str = ''.join(map(str, numbers))\n",
    "    \n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <sum> {total_sum} <eos>\"\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "for _ in range(5):\n",
    "    print(generate_random_numbers_and_sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4af0b637-dcaf-416a-aa97-96b8baeb91e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 204 <even> 222 <eos>\n",
      "<sos> 528 <even> 282828 <eos>\n",
      "<sos> 0972 <even> 000 <eos>\n",
      "<sos> 0503 <even> 000 <eos>\n",
      "<sos> 4617 <even> 444 <eos>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sequence():\n",
    "    \"\"\"\n",
    "    Generate a sequence like:\n",
    "    <sos> 1237 <even> 137137137 <eos>\n",
    "    - First part is a random digit string (length 3–5).\n",
    "    - Second part is a repeated subsequence of the first part.\n",
    "    \"\"\"\n",
    "    # Generate 3–5 random digits for the first sequence\n",
    "    numbers = [random.randint(0, 9) for _ in range(random.randint(3, 5))]\n",
    "    numbers_str = ''.join(map(str, numbers))\n",
    "    \n",
    "    # Pick a random subsequence (at least 2 digits long)\n",
    "    start = random.randint(0, len(numbers) - 2)\n",
    "    end = random.randint(start + 1, len(numbers))\n",
    "    subseq = numbers_str[start:end]\n",
    "    \n",
    "    # Repeat the subsequence 3 times\n",
    "    repeated = subseq * 3\n",
    "    \n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <even> {repeated} <eos>\"\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "for _ in range(5):\n",
    "    print(generate_sequence())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6a3ed4b-a3ea-412b-af6a-2e0a9932602c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 4355 <reverse> 5534 <eos>\n",
      "<sos> 16505 <reverse> 50561 <eos>\n",
      "<sos> 78667 <reverse> 76687 <eos>\n",
      "<sos> 7467 <reverse> 7647 <eos>\n",
      "<sos> 80364 <reverse> 46308 <eos>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_numbers_and_reverse():\n",
    "    \"\"\"\n",
    "    Generate 3–5 random digits and output:\n",
    "    <sos> digits <reverse> reversed_digits <eos>\n",
    "    \"\"\"\n",
    "    # Pick a random length between 3 and 5\n",
    "    length = random.randint(3, 5)\n",
    "    \n",
    "    # Generate that many random digits\n",
    "    numbers = [random.randint(0, 9) for _ in range(length)]\n",
    "    \n",
    "    # Join the numbers into a string\n",
    "    numbers_str = ''.join(map(str, numbers))\n",
    "    \n",
    "    # Reverse the string\n",
    "    reversed_str = numbers_str[::-1]\n",
    "    \n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <reverse> {reversed_str} <eos>\"\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "for _ in range(5):\n",
    "    print(generate_random_numbers_and_reverse())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "751eb4b9-aa9a-4574-86bc-e65e35d6664c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 19425 <descending> 95421 <eos>\n",
      "<sos> 5554 <ascending> 4555 <eos>\n",
      "<sos> 465 <descending> 654 <eos>\n",
      "<sos> 418 <ascending> 148 <eos>\n",
      "<sos> 67816 <ascending> 16678 <eos>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_random_numbers_and_sort():\n",
    "    \"\"\"\n",
    "    Generate 3–5 random digits and output either:\n",
    "    <sos> digits <ascending> sorted_digits <eos>\n",
    "    or\n",
    "    <sos> digits <descending> sorted_digits <eos>\n",
    "    \"\"\"\n",
    "    # Pick a random length between 3 and 5\n",
    "    length = random.randint(3, 5)\n",
    "    \n",
    "    # Generate that many random digits\n",
    "    numbers = [random.randint(0, 9) for _ in range(length)]\n",
    "    numbers_str = ''.join(map(str, numbers))\n",
    "    \n",
    "    # Randomly choose ascending or descending\n",
    "    if random.choice([True, False]):\n",
    "        sorted_digits = ''.join(map(str, sorted(numbers)))\n",
    "        command = \"ascending\"\n",
    "    else:\n",
    "        sorted_digits = ''.join(map(str, sorted(numbers, reverse=True)))\n",
    "        command = \"descending\"\n",
    "    \n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <{command}> {sorted_digits} <eos>\"\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "for _ in range(5):\n",
    "    print(generate_random_numbers_and_sort())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78c083-84a7-426b-967f-952a9672d880",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f872868-8648-4af3-9486-ae0ea0c70e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        # Define vocabulary \n",
    "        self.tokens = [\n",
    "            \"<sos>\", \"<eos>\", \"<sum>\", \"<reverse>\", \"<ascending>\", \"<descending>\", \"<even>\"\n",
    "        ] + [str(i) for i in range(10)]\n",
    "        \n",
    "        # Token ↔ ID mappings\n",
    "        self.token2id = {tok: idx for idx, tok in enumerate(self.tokens)}\n",
    "        self.id2token = {idx: tok for tok, idx in self.token2id.items()}\n",
    "        self.vocab_size = len(self.tokens)\n",
    "    \n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert string to list of token IDs.\n",
    "        - Matches tags and digits\n",
    "        - Ignores spaces\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            if text[i] == \" \":\n",
    "                i += 1\n",
    "                continue\n",
    "            \n",
    "            if text[i] == \"<\":  # possible special token\n",
    "                j = text.find(\">\", i)\n",
    "                if j != -1:\n",
    "                    tok = text[i:j+1]\n",
    "                    if tok in self.token2id:\n",
    "                        tokens.append(self.token2id[tok])\n",
    "                        i = j + 1\n",
    "                        continue\n",
    "            \n",
    "            if text[i].isdigit():\n",
    "                tokens.append(self.token2id[text[i]])\n",
    "                i += 1\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected character: {text[i]}\")\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Convert list of IDs back to string, preserving original format.\"\"\"\n",
    "        tokens = [self.id2token[i] for i in token_ids]\n",
    "        result = \"\"\n",
    "        current_number = \"\"\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "                # Finish any current number\n",
    "                if current_number:\n",
    "                    if result and not result.endswith(\" \"):\n",
    "                        result += \" \"\n",
    "                    result += current_number\n",
    "                    current_number = \"\"\n",
    "                \n",
    "                # Add special token with spaces\n",
    "                if result and not result.endswith(\" \"):\n",
    "                    result += \" \"\n",
    "                result += token\n",
    "                \n",
    "            else:\n",
    "                # Accumulate digits into current number\n",
    "                current_number += token\n",
    "        \n",
    "        # Add final number if exists\n",
    "        if current_number:\n",
    "            if result and not result.endswith(\" \"):\n",
    "                result += \" \"\n",
    "            result += current_number\n",
    "        \n",
    "        return result\n",
    "\n",
    "def generate_sum_example():\n",
    "    \"\"\"Generate sum command example\"\"\"\n",
    "    length = random.randint(3, 5)\n",
    "    numbers = [random.randint(0, 9) for _ in range(length)]\n",
    "    total_sum = sum(numbers)\n",
    "    numbers_str = ''.join(map(str, numbers))\n",
    "    return f\"<sos> {numbers_str} <sum> {total_sum} <eos>\"\n",
    "\n",
    "def generate_reverse_example():\n",
    "    \"\"\"Generate reverse command example\"\"\"\n",
    "    length = random.randint(3, 5)\n",
    "    numbers = [random.randint(0, 9) for _ in range(length)]\n",
    "    numbers_str = ''.join(map(str, numbers))\n",
    "    reversed_str = numbers_str[::-1]\n",
    "    return f\"<sos> {numbers_str} <reverse> {reversed_str} <eos>\"\n",
    "\n",
    "def generate_sort_example():\n",
    "    \"\"\"Generate ascending or descending sort example\"\"\"\n",
    "    length = random.randint(3, 5)\n",
    "    numbers = [random.randint(0, 9) for _ in range(length)]\n",
    "    numbers_str = ''.join(map(str, numbers))\n",
    "    \n",
    "    if random.choice([True, False]):\n",
    "        sorted_digits = ''.join(map(str, sorted(numbers)))\n",
    "        command = \"ascending\"\n",
    "    else:\n",
    "        sorted_digits = ''.join(map(str, sorted(numbers, reverse=True)))\n",
    "        command = \"descending\"\n",
    "    \n",
    "    return f\"<sos> {numbers_str} <{command}> {sorted_digits} <eos>\"\n",
    "\n",
    "def generate_even_example():\n",
    "    \"\"\"Generate even (repetition) command example\"\"\"\n",
    "    numbers = [random.randint(0, 9) for _ in range(random.randint(3, 5))]\n",
    "    numbers_str = ''.join(map(str, numbers))\n",
    "    \n",
    "    # Pick a random subsequence (at least 1 digit long)\n",
    "    start = random.randint(0, len(numbers) - 1)\n",
    "    end = random.randint(start + 1, len(numbers))\n",
    "    subseq = numbers_str[start:end]\n",
    "    \n",
    "    # Repeat the subsequence 3 times\n",
    "    repeated = subseq * 3\n",
    "    \n",
    "    return f\"<sos> {numbers_str} <even> {repeated} <eos>\"\n",
    "\n",
    "def generate_dataset(num_examples: int = 50000) -> List[str]:\n",
    "    \"\"\"Generate a balanced dataset with all command types\"\"\"\n",
    "    examples = []\n",
    "    generators = [generate_sum_example, generate_reverse_example, \n",
    "                 generate_sort_example, generate_even_example]\n",
    "    \n",
    "    examples_per_type = num_examples // len(generators)\n",
    "    \n",
    "    for generator in generators:\n",
    "        for _ in range(examples_per_type):\n",
    "            examples.append(generator())\n",
    "    \n",
    "    # Add remaining examples to reach exact count\n",
    "    remaining = num_examples - len(examples)\n",
    "    for _ in range(remaining):\n",
    "        generator = random.choice(generators)\n",
    "        examples.append(generator())\n",
    "    \n",
    "    # Shuffle the dataset\n",
    "    random.shuffle(examples)\n",
    "    return examples\n",
    "\n",
    "def create_training_data(examples: List[str], tokenizer: CustomTokenizer) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"Convert examples to tokenized training data\"\"\"\n",
    "    tokenized_examples = []\n",
    "    max_length = 0\n",
    "    \n",
    "    # Tokenize all examples and find max length\n",
    "    for example in examples:\n",
    "        tokens = tokenizer.encode(example)\n",
    "        tokenized_examples.append(tokens)\n",
    "        max_length = max(max_length, len(tokens))\n",
    "    \n",
    "    print(f\"Maximum sequence length: {max_length}\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "    \n",
    "    # Convert to numpy array (pad sequences to max length)\n",
    "    data = np.full((len(examples), max_length), tokenizer.token2id[\"<eos>\"], dtype=np.int64)\n",
    "    \n",
    "    for i, tokens in enumerate(tokenized_examples):\n",
    "        data[i, :len(tokens)] = tokens\n",
    "    \n",
    "    # Create metadata\n",
    "    meta = {\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'max_length': max_length,\n",
    "        'num_examples': len(examples),\n",
    "        'token2id': tokenizer.token2id,\n",
    "        'id2token': tokenizer.id2token\n",
    "    }\n",
    "    \n",
    "    return data, meta\n",
    "\n",
    "def test_tokenizer():\n",
    "    \"\"\"Test the tokenizer with sample examples\"\"\"\n",
    "    print(\"Testing Tokenizer...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    tokenizer = CustomTokenizer()\n",
    "    \n",
    "    # Test with each type of example\n",
    "    test_examples = [\n",
    "        generate_sum_example(),\n",
    "        generate_reverse_example(), \n",
    "        generate_sort_example(),\n",
    "        generate_even_example()\n",
    "    ]\n",
    "    \n",
    "    for i, example in enumerate(test_examples):\n",
    "        print(f\"\\nTest {i+1}:\")\n",
    "        print(f\"Original: {example}\")\n",
    "        \n",
    "        # Encode\n",
    "        encoded = tokenizer.encode(example)\n",
    "        print(f\"Encoded:  {encoded}\")\n",
    "        \n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        print(f\"Decoded:  {decoded}\")\n",
    "        \n",
    "        # Check if round-trip works (normalize whitespace for comparison)\n",
    "        original_normalized = \" \".join(example.split())\n",
    "        decoded_normalized = \" \".join(decoded.split())\n",
    "        success = original_normalized == decoded_normalized\n",
    "        print(f\"Round-trip successful: {success}\")\n",
    "        \n",
    "        if not success:\n",
    "            print(f\"ERROR: Round-trip failed!\")\n",
    "            print(f\"Expected: '{original_normalized}'\")\n",
    "            print(f\"Got:      '{decoded_normalized}'\")\n",
    "    \n",
    "    print(f\"\\nVocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Tokens: {tokenizer.tokens}\")\n",
    "\n",
    "def save_dataset(train_data: np.ndarray, val_data: np.ndarray, meta: dict, data_dir: str = \"data/number_commands\"):\n",
    "    \"\"\"Save dataset to files\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    \n",
    "    # Save binary data\n",
    "    train_data.astype(np.uint16).tofile(os.path.join(data_dir, 'train.bin'))\n",
    "    val_data.astype(np.uint16).tofile(os.path.join(data_dir, 'val.bin'))\n",
    "    \n",
    "    # Save metadata\n",
    "    with open(os.path.join(data_dir, 'meta.pkl'), 'wb') as f:\n",
    "        pickle.dump(meta, f)\n",
    "    \n",
    "    print(f\"Dataset saved to {data_dir}\")\n",
    "    print(f\"Train examples: {len(train_data)}\")\n",
    "    print(f\"Val examples: {len(val_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ff174b9-6bfa-40c6-9358-0cb3920a5a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: <sos> 482 <ascending> 248 <eos>\n",
      "Encoded: [0, 11, 15, 9, 4, 9, 11, 15, 1]\n",
      "Decoded: <sos> 482 <ascending> 248 <eos>\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "sample = \"<sos> 482 <ascending> 248 <eos>\"\n",
    "encoded = tokenizer.encode(sample)\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(\"Original:\", sample)\n",
    "print(\"Encoded:\", encoded)\n",
    "print(\"Decoded:\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5292ce41-fdec-40dc-a2c9-8fdccbf8f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tokenizer...\n",
      "==================================================\n",
      "\n",
      "Test 1:\n",
      "Original: <sos> 8704 <sum> 19 <eos>\n",
      "Encoded:  [0, 15, 14, 7, 11, 2, 8, 16, 1]\n",
      "Decoded:  <sos> 8704 <sum> 19 <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Test 2:\n",
      "Original: <sos> 2626 <reverse> 6262 <eos>\n",
      "Encoded:  [0, 9, 13, 9, 13, 3, 13, 9, 13, 9, 1]\n",
      "Decoded:  <sos> 2626 <reverse> 6262 <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Test 3:\n",
      "Original: <sos> 590 <ascending> 059 <eos>\n",
      "Encoded:  [0, 12, 16, 7, 4, 7, 12, 16, 1]\n",
      "Decoded:  <sos> 590 <ascending> 059 <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Test 4:\n",
      "Original: <sos> 210 <even> 111 <eos>\n",
      "Encoded:  [0, 9, 8, 7, 6, 8, 8, 8, 1]\n",
      "Decoded:  <sos> 210 <even> 111 <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Vocabulary size: 17\n",
      "Tokens: ['<sos>', '<eos>', '<sum>', '<reverse>', '<ascending>', '<descending>', '<even>', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
      "\n",
      "==================================================\n",
      "Generating Dataset...\n",
      "==================================================\n",
      "Generating examples...\n",
      "Generated 60000 total examples\n",
      "Train: 54000, Val: 6000\n",
      "\n",
      "Tokenizing training data...\n",
      "Maximum sequence length: 23\n",
      "Vocabulary size: 17\n",
      "Tokenizing validation data...\n",
      "Maximum sequence length: 23\n",
      "Vocabulary size: 17\n",
      "Dataset saved to data/number_commands\n",
      "Train examples: 54000\n",
      "Val examples: 6000\n",
      "\n",
      "Dataset Statistics:\n",
      "Vocabulary size: 17\n",
      "Max sequence length: 23\n",
      "Train data shape: (54000, 23)\n",
      "Val data shape: (6000, 23)\n",
      "\n",
      "Sample training examples:\n",
      "1: <sos> 613 <even> 333 <eos>\n",
      "2: <sos> 940 <even> 000 <eos>\n",
      "3: <sos> 157 <even> 777 <eos>\n",
      "4: <sos> 96540 <descending> 96540 <eos>\n",
      "5: <sos> 18507 <reverse> 70581 <eos>\n"
     ]
    }
   ],
   "source": [
    "# Test tokenizer first\n",
    "test_tokenizer()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Generating Dataset...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate dataset\n",
    "print(\"Generating examples...\")\n",
    "all_examples = generate_dataset(num_examples=60000)\n",
    "\n",
    "# Split into train/val\n",
    "split_idx = int(0.9 * len(all_examples))\n",
    "train_examples = all_examples[:split_idx]\n",
    "val_examples = all_examples[split_idx:]\n",
    "\n",
    "print(f\"Generated {len(all_examples)} total examples\")\n",
    "print(f\"Train: {len(train_examples)}, Val: {len(val_examples)}\")\n",
    "\n",
    "# Create tokenizer and convert to training data\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "print(\"\\nTokenizing training data...\")\n",
    "train_data, meta = create_training_data(train_examples, tokenizer)\n",
    "\n",
    "print(\"Tokenizing validation data...\")\n",
    "val_data, _ = create_training_data(val_examples, tokenizer)\n",
    "\n",
    "# Save dataset\n",
    "save_dataset(train_data, val_data, meta)\n",
    "\n",
    "# Show some statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Vocabulary size: {meta['vocab_size']}\")\n",
    "print(f\"Max sequence length: {meta['max_length']}\")\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Val data shape: {val_data.shape}\")\n",
    "\n",
    "# Show sample training examples\n",
    "print(\"\\nSample training examples:\")\n",
    "for i in range(5):\n",
    "    tokens = train_data[i]\n",
    "    # Remove padding\n",
    "    tokens = tokens[tokens != tokenizer.token2id[\"<eos>\"]]\n",
    "    tokens = np.append(tokens, tokenizer.token2id[\"<eos>\"])  # Add back one EOS\n",
    "    decoded = tokenizer.decode(tokens.tolist())\n",
    "    print(f\"{i+1}: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b4f33c-dc03-483a-8dcf-14d567017eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class NumberCommandConfig:\n",
    "    block_size: int = 32  # Small sequences for number commands\n",
    "    vocab_size: int = 17  # Will be set from dataset\n",
    "    n_layer: int = 4      # Smaller model for this simple task\n",
    "    n_head: int = 4       # Fewer attention heads\n",
    "    n_embd: int = 128     # Smaller embedding dimension\n",
    "    dropout: float = 0.1  # Some dropout for regularization\n",
    "    bias: bool = True     # Keep bias terms\n",
    "\n",
    "class NumberCommandTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying - share embeddings with output layer\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\"Estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS\"\"\"\n",
    "        # First estimate the number of flops we do per iteration.\n",
    "        # See PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # Express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt)  # per second\n",
    "        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
    "        batch_size = idx.size(0)\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool, device=idx.device)\n",
    "    \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "    \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "    \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "            # Append only for unfinished sequences\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "            if eos_token_id is not None:\n",
    "                # Update finished mask\n",
    "                finished = finished | (idx_next.squeeze(1) == eos_token_id)\n",
    "                # If all sequences finished, we can break early\n",
    "                if finished.all():\n",
    "                    break\n",
    "    \n",
    "            # Optionally, you could replace tokens in finished sequences with eos_token_id to pad\n",
    "            idx_next[finished.unsqueeze(1)] = eos_token_id\n",
    "    \n",
    "        return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc790478-7d33-444e-a1d0-28319aa2d2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.8.0+cpu\n",
      "CUDA available: False\n",
      "✓ Training data found\n",
      "✓ Vocab size: 17\n",
      "✓ Max sequence length: 23\n",
      "✓ Using device: cpu\n",
      "✓ Model compilation: False\n",
      "✓ Using dtype: float16\n",
      "✓ Testing data loading...\n",
      "✓ Batch shape: torch.Size([2, 23]), torch.Size([2, 23])\n",
      "number of parameters: 0.00M\n",
      "✓ Model created with 0.00M parameters\n",
      "num decayed parameter tensors: 18, with 3,392 parameters\n",
      "num non-decayed parameter tensors: 34, with 432 parameters\n",
      "using fused AdamW: False\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrax\\AppData\\Local\\Temp\\ipykernel_14288\\2254958311.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
      "C:\\Users\\abrax\\anaconda3\\Lib\\site-packages\\torch\\amp\\grad_scaler.py:136: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial: train loss 2.7825, val loss 2.7827\n",
      "step 0: train loss 2.7826, val loss 2.7827, lr 1.49e-06\n",
      "✓ Saved checkpoint (val_loss: 2.7827)\n",
      "iter 0: loss 2.7831, time 4895.1ms\n",
      "iter 50: loss 2.7581, time 2883.7ms\n",
      "iter 100: loss 2.7063, time 2912.5ms\n",
      "iter 150: loss 2.6333, time 2926.0ms\n",
      "iter 200: loss 2.5404, time 2893.1ms\n",
      "iter 250: loss 2.3827, time 2912.6ms\n",
      "iter 300: loss 2.2332, time 2904.1ms\n",
      "iter 350: loss 2.0792, time 2886.0ms\n",
      "iter 400: loss 2.0018, time 2915.3ms\n",
      "iter 450: loss 1.8971, time 2939.1ms\n",
      "step 500: train loss 1.7412, val loss 1.7450, lr 2.97e-04\n",
      "✓ Saved checkpoint (val_loss: 1.7450)\n",
      "iter 500: loss 1.7572, time 5273.7ms\n",
      "iter 550: loss 1.7201, time 2926.8ms\n",
      "iter 600: loss 1.5668, time 2836.5ms\n",
      "iter 650: loss 1.4624, time 2865.6ms\n",
      "iter 700: loss 1.4315, time 2859.4ms\n",
      "iter 750: loss 1.3545, time 2883.5ms\n",
      "iter 800: loss 1.3681, time 2850.3ms\n",
      "iter 850: loss 1.2914, time 2863.8ms\n",
      "iter 900: loss 1.3104, time 2882.0ms\n",
      "iter 950: loss 1.3066, time 2871.1ms\n",
      "step 1000: train loss 1.2771, val loss 1.2737, lr 2.82e-04\n",
      "✓ Saved checkpoint (val_loss: 1.2737)\n",
      "iter 1000: loss 1.3241, time 5236.2ms\n",
      "iter 1050: loss 1.2219, time 2878.4ms\n",
      "iter 1100: loss 1.2620, time 2847.2ms\n",
      "iter 1150: loss 1.2570, time 2887.6ms\n",
      "iter 1200: loss 1.3085, time 2874.9ms\n",
      "iter 1250: loss 1.2557, time 2882.8ms\n",
      "iter 1300: loss 1.2630, time 2845.3ms\n",
      "iter 1350: loss 1.2194, time 2873.6ms\n",
      "iter 1400: loss 1.2246, time 2905.9ms\n",
      "iter 1450: loss 1.2869, time 2894.2ms\n",
      "step 1500: train loss 1.2192, val loss 1.2290, lr 2.54e-04\n",
      "✓ Saved checkpoint (val_loss: 1.2290)\n",
      "iter 1500: loss 1.1815, time 5242.7ms\n",
      "iter 1550: loss 1.2337, time 2900.8ms\n",
      "iter 1600: loss 1.2701, time 2872.3ms\n",
      "iter 1650: loss 1.2356, time 2862.6ms\n",
      "iter 1700: loss 1.1908, time 2898.6ms\n",
      "iter 1750: loss 1.2492, time 2876.6ms\n",
      "iter 1800: loss 1.1957, time 2918.8ms\n",
      "iter 1850: loss 1.2412, time 2909.2ms\n",
      "iter 1900: loss 1.2529, time 2892.6ms\n",
      "iter 1950: loss 1.1607, time 2861.0ms\n",
      "step 2000: train loss 1.1938, val loss 1.1920, lr 2.17e-04\n",
      "✓ Saved checkpoint (val_loss: 1.1920)\n",
      "iter 2000: loss 1.2229, time 5328.9ms\n",
      "iter 2050: loss 1.2110, time 2889.4ms\n",
      "iter 2100: loss 1.2201, time 2935.8ms\n",
      "iter 2150: loss 1.1784, time 2852.0ms\n",
      "iter 2200: loss 1.1368, time 2909.1ms\n",
      "iter 2250: loss 1.2032, time 2905.2ms\n",
      "iter 2300: loss 1.1411, time 2881.1ms\n",
      "iter 2350: loss 1.1596, time 2880.3ms\n",
      "iter 2400: loss 1.1580, time 2909.7ms\n",
      "iter 2450: loss 1.1421, time 2939.0ms\n",
      "step 2500: train loss 1.1706, val loss 1.1784, lr 1.74e-04\n",
      "✓ Saved checkpoint (val_loss: 1.1784)\n",
      "iter 2500: loss 1.2376, time 5273.2ms\n",
      "iter 2550: loss 1.1589, time 2911.8ms\n",
      "iter 2600: loss 1.1038, time 2911.9ms\n",
      "iter 2650: loss 1.1601, time 2925.1ms\n",
      "iter 2700: loss 1.1803, time 2910.8ms\n",
      "iter 2750: loss 1.1622, time 2870.1ms\n",
      "iter 2800: loss 1.2320, time 2888.5ms\n",
      "iter 2850: loss 1.1530, time 2885.5ms\n",
      "iter 2900: loss 1.1697, time 2883.9ms\n",
      "iter 2950: loss 1.1972, time 2884.7ms\n",
      "step 3000: train loss 1.1534, val loss 1.1551, lr 1.30e-04\n",
      "✓ Saved checkpoint (val_loss: 1.1551)\n",
      "iter 3000: loss 1.1676, time 5247.0ms\n",
      "iter 3050: loss 1.1436, time 2885.2ms\n",
      "iter 3100: loss 1.1625, time 2878.4ms\n",
      "iter 3150: loss 1.1756, time 2874.8ms\n",
      "iter 3200: loss 1.1758, time 2890.6ms\n",
      "iter 3250: loss 1.1476, time 2844.6ms\n",
      "iter 3300: loss 1.2190, time 2879.0ms\n",
      "iter 3350: loss 1.1213, time 2907.0ms\n",
      "iter 3400: loss 1.1307, time 2870.9ms\n",
      "iter 3450: loss 1.1521, time 2884.8ms\n",
      "step 3500: train loss 1.1479, val loss 1.1465, lr 9.00e-05\n",
      "✓ Saved checkpoint (val_loss: 1.1465)\n",
      "iter 3500: loss 1.1780, time 5228.2ms\n",
      "iter 3550: loss 1.1068, time 2862.1ms\n",
      "iter 3600: loss 1.1739, time 2876.4ms\n",
      "iter 3650: loss 1.1420, time 2884.8ms\n",
      "iter 3700: loss 1.1476, time 2927.7ms\n",
      "iter 3750: loss 1.1256, time 2913.6ms\n",
      "iter 3800: loss 1.2033, time 2882.1ms\n",
      "iter 3850: loss 1.1834, time 2860.4ms\n",
      "iter 3900: loss 1.1473, time 2888.2ms\n",
      "iter 3950: loss 1.1271, time 2885.3ms\n",
      "step 4000: train loss 1.1377, val loss 1.1389, lr 5.79e-05\n",
      "✓ Saved checkpoint (val_loss: 1.1389)\n",
      "iter 4000: loss 1.1561, time 5269.7ms\n",
      "iter 4050: loss 1.1458, time 2876.8ms\n",
      "iter 4100: loss 1.1283, time 2904.5ms\n",
      "iter 4150: loss 1.1751, time 2902.2ms\n",
      "iter 4200: loss 1.1734, time 2915.6ms\n",
      "iter 4250: loss 1.1545, time 2910.2ms\n",
      "iter 4300: loss 1.1664, time 2925.8ms\n",
      "iter 4350: loss 1.1293, time 2899.4ms\n",
      "iter 4400: loss 1.1251, time 2913.3ms\n",
      "iter 4450: loss 1.1981, time 2916.3ms\n",
      "step 4500: train loss 1.1356, val loss 1.1332, lr 3.72e-05\n",
      "✓ Saved checkpoint (val_loss: 1.1332)\n",
      "iter 4500: loss 1.1638, time 5330.9ms\n",
      "iter 4550: loss 1.2275, time 2896.7ms\n",
      "iter 4600: loss 1.1481, time 2968.0ms\n",
      "iter 4650: loss 1.1132, time 2870.6ms\n",
      "iter 4700: loss 1.1534, time 2879.5ms\n",
      "iter 4750: loss 1.1636, time 2894.5ms\n",
      "iter 4800: loss 1.1254, time 2879.2ms\n",
      "iter 4850: loss 1.1575, time 2874.3ms\n",
      "iter 4900: loss 1.2608, time 2863.3ms\n",
      "iter 4950: loss 1.1678, time 2883.2ms\n",
      "\n",
      "Final: train loss 1.1253, val loss 1.1320\n",
      "Training completed! Best val loss: 1.1332\n",
      "Model saved to: out_number_commands/ckpt.pt\n"
     ]
    }
   ],
   "source": [
    "# ALL-IN-ONE TRAINING SCRIPT FOR NUMBER COMMAND TRANSFORMER\n",
    "# \n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import platform\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name())\n",
    "\n",
    "# Check if we have the required data\n",
    "data_dir = 'data/number_commands'\n",
    "if not os.path.exists(os.path.join(data_dir, 'train.bin')):\n",
    "    raise FileNotFoundError(f\"Training data not found at {data_dir}. Please run dataset generation first.\")\n",
    "\n",
    "print(\"✓ Training data found\")\n",
    "\n",
    "# Load metadata\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "with open(meta_path, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "vocab_size = meta['vocab_size']\n",
    "max_length = meta['max_length']\n",
    "\n",
    "print(f\"✓ Vocab size: {vocab_size}\")\n",
    "print(f\"✓ Max sequence length: {max_length}\")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'block_size': min(max_length, 32),  # Use actual max length or 32, whichever is smaller\n",
    "    'n_layer': 4,\n",
    "    'n_head': 4, \n",
    "    'n_embd': 8, #worked iwth 128, let's try smaller for our massive 17 token vocab\n",
    "    'dropout': 0.1,\n",
    "    'bias': True,\n",
    "    'batch_size': 64,\n",
    "    'learning_rate': 3e-4,\n",
    "    'max_iters': 5000,\n",
    "    'eval_interval': 500,\n",
    "    'log_interval': 50,\n",
    "    'warmup_iters': 200,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'compile': torch.cuda.is_available() and platform.system() != 'Windows',\n",
    "    'out_dir': 'out_number_commands'\n",
    "}\n",
    "\n",
    "print(f\"✓ Using device: {config['device']}\")\n",
    "print(f\"✓ Model compilation: {config['compile']}\")\n",
    "\n",
    "# Set up device and dtype\n",
    "device = config['device']\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "\n",
    "print(f\"✓ Using dtype: {dtype}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config['out_dir'], exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Data loading function\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    \n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Test data loading\n",
    "print(\"✓ Testing data loading...\")\n",
    "test_x, test_y = get_batch('train', 2, config['block_size'])\n",
    "print(f\"✓ Batch shape: {test_x.shape}, {test_y.shape}\")\n",
    "\n",
    "# Create and initialize model (model classes should be defined already)\n",
    "try:\n",
    "    model_config = NumberCommandConfig(\n",
    "        vocab_size=config['vocab_size'],\n",
    "        block_size=config['block_size'],\n",
    "        n_layer=config['n_layer'],\n",
    "        n_head=config['n_head'],\n",
    "        n_embd=config['n_embd'],\n",
    "        dropout=config['dropout'],\n",
    "        bias=config['bias']\n",
    "    )\n",
    "    \n",
    "    model = NumberCommandTransformer(model_config)\n",
    "    model.to(device)\n",
    "    \n",
    "    print(f\"✓ Model created with {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"❌ Model classes not found! Please run the model definition cell first.\")\n",
    "    raise\n",
    "\n",
    "# Initialize training components\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=1e-2,\n",
    "    learning_rate=config['learning_rate'], \n",
    "    betas=(0.9, 0.95),\n",
    "    device_type=device\n",
    ")\n",
    "\n",
    "# Compile model with error handling\n",
    "if config['compile']:\n",
    "    try:\n",
    "        print(\"Compiling model...\")\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model)\n",
    "        print(\"✓ Model compilation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Model compilation failed: {e}\")\n",
    "        print(\"Continuing without compilation...\")\n",
    "\n",
    "# Loss estimation function\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, config['batch_size'], config['block_size'])\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(it):\n",
    "    if it < config['warmup_iters']:\n",
    "        return config['learning_rate'] * (it + 1) / (config['warmup_iters'] + 1)\n",
    "    if it > config['max_iters']:\n",
    "        return config['learning_rate'] * 0.1\n",
    "    decay_ratio = (it - config['warmup_iters']) / (config['max_iters'] - config['warmup_iters'])\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config['learning_rate'] * 0.1 + coeff * (config['learning_rate'] - config['learning_rate'] * 0.1)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = float('inf')\n",
    "t0 = time.time()\n",
    "\n",
    "# Initial evaluation\n",
    "losses = estimate_loss()\n",
    "print(f\"Initial: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "while iter_num < config['max_iters']:\n",
    "    # Set learning rate\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    \n",
    "    # Evaluate and save checkpoints\n",
    "    if iter_num % config['eval_interval'] == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.2e}\")\n",
    "        \n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model_args': config,\n",
    "                'iter_num': iter_num,\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(config['out_dir'], 'ckpt.pt'))\n",
    "            print(f\"✓ Saved checkpoint (val_loss: {best_val_loss:.4f})\")\n",
    "    \n",
    "    # Training step\n",
    "    X, Y = get_batch('train', config['batch_size'], config['block_size'])\n",
    "    \n",
    "    with ctx:\n",
    "        logits, loss = model(X, Y)\n",
    "    \n",
    "    # Backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    \n",
    "    # Logging\n",
    "    if iter_num % config['log_interval'] == 0:\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"iter {iter_num}: loss {loss.item():.4f}, time {dt*1000:.1f}ms\")\n",
    "    \n",
    "    iter_num += 1\n",
    "\n",
    "# Final evaluation\n",
    "losses = estimate_loss()\n",
    "print(f\"\\nFinal: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "print(f\"Training completed! Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"Model saved to: {config['out_dir']}/ckpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d35e581-ed3a-4a63-8e5f-621ff7e228f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_trained_model(checkpoint_path: str = 'out_number_commands/ckpt.pt'):\n",
    "    \"\"\"Load the trained model and tokenizer\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load metadata first\n",
    "        meta_path = 'data/number_commands/meta.pkl'\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Dataset metadata not found. Please run dataset generation first.\")\n",
    "    \n",
    "    # Recreate tokenizer\n",
    "    class CustomTokenizer:\n",
    "        def __init__(self, token2id, id2token):\n",
    "            self.token2id = token2id\n",
    "            self.id2token = id2token\n",
    "            self.vocab_size = len(token2id)\n",
    "        \n",
    "        def encode(self, text: str) -> List[int]:\n",
    "            tokens = []\n",
    "            i = 0\n",
    "            while i < len(text):\n",
    "                if text[i] == \" \":\n",
    "                    i += 1\n",
    "                    continue\n",
    "                \n",
    "                if text[i] == \"<\":\n",
    "                    j = text.find(\">\", i)\n",
    "                    if j != -1:\n",
    "                        tok = text[i:j+1]\n",
    "                        if tok in self.token2id:\n",
    "                            tokens.append(self.token2id[tok])\n",
    "                            i = j + 1\n",
    "                            continue\n",
    "                \n",
    "                if text[i].isdigit():\n",
    "                    tokens.append(self.token2id[text[i]])\n",
    "                    i += 1\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected character: {text[i]}\")\n",
    "            \n",
    "            return tokens\n",
    "        \n",
    "        def decode(self, token_ids: List[int]) -> str:\n",
    "            tokens = [self.id2token[i] for i in token_ids]\n",
    "            result = \"\"\n",
    "            current_number = \"\"\n",
    "            \n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "                    # Finish any current number\n",
    "                    if current_number:\n",
    "                        if result and not result.endswith(\" \"):\n",
    "                            result += \" \"\n",
    "                        result += current_number\n",
    "                        current_number = \"\"\n",
    "                    \n",
    "                    # Add special token with spaces\n",
    "                    if result and not result.endswith(\" \"):\n",
    "                        result += \" \"\n",
    "                    result += token\n",
    "                    \n",
    "                else:\n",
    "                    # Accumulate digits into current number\n",
    "                    current_number += token\n",
    "            \n",
    "            # Add final number if exists\n",
    "            if current_number:\n",
    "                if result and not result.endswith(\" \"):\n",
    "                    result += \" \"\n",
    "                result += current_number\n",
    "            \n",
    "            return result\n",
    "    \n",
    "    tokenizer = CustomTokenizer(meta['token2id'], meta['id2token'])\n",
    "    \n",
    "    # Load model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}. Please train the model first.\")\n",
    "    \n",
    "    model_args = checkpoint['model_args']\n",
    "    \n",
    "    # Handle both old and new checkpoint formats\n",
    "    if isinstance(model_args, dict):\n",
    "        # New format - direct config dict\n",
    "        try:\n",
    "            config = NumberCommandConfig(**model_args)\n",
    "        except TypeError:\n",
    "            # Fallback for missing config class\n",
    "            print(\"Warning: Using fallback model configuration\")\n",
    "            config = type('Config', (), model_args)()\n",
    "    else:\n",
    "        # Old format - might be a config object\n",
    "        config = model_args\n",
    "    \n",
    "    try:\n",
    "        model = NumberCommandTransformer(config)\n",
    "    except NameError:\n",
    "        raise NameError(\"Model classes not found. Please run the model definition cell first.\")\n",
    "    \n",
    "    # Remove potential compilation prefix\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    print(f\"✓ Model loaded from {checkpoint_path}\")\n",
    "    print(f\"✓ Model has {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "    print(f\"✓ Using device: {device}\")\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "def test_model_completion(model, tokenizer, device, prompt: str, max_new_tokens: int = 15, temperature: float = 0.1):\n",
    "    \"\"\"Test model completion given a prompt\"\"\"\n",
    "    \n",
    "    # Encode prompt\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    \n",
    "    # Generate completion\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "    \n",
    "    # Decode result\n",
    "    generated_tokens = generated[0].tolist()\n",
    "    full_response = tokenizer.decode(generated_tokens)\n",
    "    \n",
    "    return full_response, generated_tokens\n",
    "\n",
    "def calculate_expected_result(input_str: str) -> str:\n",
    "    \"\"\"Calculate what the expected output should be\"\"\"\n",
    "    \n",
    "    # Parse the input\n",
    "    parts = input_str.strip().split()\n",
    "    if len(parts) < 3 or parts[0] != '<sos>':\n",
    "        return \"Invalid format\"\n",
    "    \n",
    "    numbers_str = parts[1]\n",
    "    command = parts[2]\n",
    "    \n",
    "    # Convert string to list of digits\n",
    "    try:\n",
    "        digits = [int(d) for d in numbers_str]\n",
    "    except ValueError:\n",
    "        return \"Invalid numbers\"\n",
    "    \n",
    "    if command == '<sum>':\n",
    "        result = str(sum(digits))\n",
    "    elif command == '<reverse>':\n",
    "        result = numbers_str[::-1]\n",
    "    elif command == '<ascending>':\n",
    "        result = ''.join(map(str, sorted(digits)))\n",
    "    elif command == '<descending>':\n",
    "        result = ''.join(map(str, sorted(digits, reverse=True)))\n",
    "    elif command == '<even>':\n",
    "        # For <even>, we need to find a subsequence and repeat it\n",
    "        # This is harder to predict exactly, so we'll just note it\n",
    "        result = \"SUBSEQUENCE_REPEATED\"\n",
    "    else:\n",
    "        return \"Unknown command\"\n",
    "    \n",
    "    return f\"<sos> {numbers_str} {command} {result} <eos>\"\n",
    "\n",
    "def run_comprehensive_test(model, tokenizer, device):\n",
    "    \"\"\"Run comprehensive tests on the model\"\"\"\n",
    "    \n",
    "    test_cases = [\n",
    "        # Sum tests\n",
    "        \"<sos> 123 <sum>\",\n",
    "        \"<sos> 1232 <sum>\",\n",
    "        \"<sos> 92999 <sum>\",\n",
    "        \n",
    "        # Reverse tests\n",
    "        \"<sos> 1765 <reverse>\",\n",
    "        \"<sos> 9235 <reverse>\",\n",
    "        \"<sos> 505 <reverse>\",\n",
    "        \n",
    "        # Ascending sort tests\n",
    "        \"<sos> 5132 <ascending>\",\n",
    "        \"<sos> 19283 <ascending>\",\n",
    "        \"<sos> 5231 <ascending>\",\n",
    "        \n",
    "        # Descending sort tests\n",
    "        \"<sos> 1562 <descending>\",\n",
    "        \"<sos> 91735 <descending>\",\n",
    "        \"<sos> 482 <descending>\",\n",
    "    ]\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE MODEL TESTING\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\nTest {i+1}: {test_case}\")\n",
    "        \n",
    "        # Get model prediction\n",
    "        prediction, tokens = test_model_completion(model, tokenizer, device, test_case)\n",
    "        print(f\"Model output: {prediction}\")\n",
    "        \n",
    "        # Calculate expected result\n",
    "        expected = calculate_expected_result(test_case)\n",
    "        print(f\"Expected:     {expected}\")\n",
    "        \n",
    "        # Check if correct (for non-even commands)\n",
    "        if not '<even>' in test_case:\n",
    "            is_correct = prediction.strip() == expected.strip()\n",
    "            print(f\"Correct: {'✓' if is_correct else '✗'}\")\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "        else:\n",
    "            print(\"Correct: (even command - manual check needed)\")\n",
    "        \n",
    "        print(\"-\" * 40)\n",
    "    \n",
    "    if total > 0:\n",
    "        accuracy = correct / total * 100\n",
    "        print(f\"\\nOverall Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
    "    \n",
    "    return correct, total\n",
    "\n",
    "def interactive_test(model, tokenizer, device):\n",
    "    \"\"\"Interactive testing interface\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERACTIVE TESTING\")\n",
    "    print(\"Enter prompts like: <sos> 1234 <sum>\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"\\nEnter prompt: \").strip()\n",
    "            if prompt.lower() == 'quit':\n",
    "                break\n",
    "            \n",
    "            if not prompt:\n",
    "                continue\n",
    "            \n",
    "            # Test the prompt\n",
    "            prediction, tokens = test_model_completion(model, tokenizer, device, prompt)\n",
    "            print(f\"Model output: {prediction}\")\n",
    "            \n",
    "            # Show expected if possible\n",
    "            try:\n",
    "                expected = calculate_expected_result(prompt)\n",
    "                if \"SUBSEQUENCE\" not in expected and \"Invalid\" not in expected and \"Unknown\" not in expected:\n",
    "                    print(f\"Expected:     {expected}\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab3006ce-2af5-41fb-b346-d352a80b34bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Using fallback model configuration\n",
      "number of parameters: 0.00M\n",
      "✓ Model loaded from out_number_commands/ckpt.pt\n",
      "✓ Model has 0.00M parameters\n",
      "✓ Using device: cpu\n",
      "================================================================================\n",
      "COMPREHENSIVE MODEL TESTING\n",
      "================================================================================\n",
      "\n",
      "Test 1: <sos> 123 <sum>\n",
      "Error loading model: can't assign a NoneType to a torch.LongTensor\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Load the trained model\n",
    "    model, tokenizer, device = load_trained_model()\n",
    "    \n",
    "    # Run comprehensive tests\n",
    "    correct, total = run_comprehensive_test(model, tokenizer, device)\n",
    "    \n",
    "    # Start interactive testing\n",
    "    interactive_test(model, tokenizer, device)\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find model files. Make sure you've trained the model first.\")\n",
    "    print(\"Expected files:\")\n",
    "    print(\"  - out_number_commands/ckpt.pt\")\n",
    "    print(\"  - data/number_commands/meta.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
