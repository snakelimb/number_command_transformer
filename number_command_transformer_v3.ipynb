{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "oQ4wfYr24d-x",
   "metadata": {
    "id": "oQ4wfYr24d-x"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb41ff5d-9d85-4c94-9437-83b674b9425b",
   "metadata": {
    "id": "cb41ff5d-9d85-4c94-9437-83b674b9425b"
   },
   "source": [
    "\n",
    "# Exploration of a \"number command\" transformer.\n",
    "\n",
    "### Depending on which command token is after a sequence of digit's, the transformer is asked to perform different tasks.\n",
    "\n",
    "\n",
    "\n",
    "The idea is to test if initially the transformer can perform its task given near full demonstations. If given examples of these multiple tasks, can the varied task tokens properly signal the transformer to change it's behaviour?\n",
    "I hope to also explore by how many parameter can the model be reduce before its performance completely degrades.\n",
    "\n",
    "I tried to vary the sequence length of the inputs on some functions. This results in some functions having shorter sequences. This was intentional to test what will happen when we prompt it with sequences that are 7 numbers long.\n",
    "To challenge the Transformer I might try to se what it does if the training data contains gaps in sequence length. For example, if one task only has examples of sequences of length 1,3,5 and another has 2, 4,7, etc\n",
    "\n",
    "\n",
    "What happens when two commands are entered in succession? Will it do both?\n",
    "\n",
    "Which commands conflict with each other?\n",
    "\n",
    "## example sequences:\n",
    "\n",
    "```\n",
    "<sos> 55,50,14,9,10,36,56,53,47,21 <sum> 351 <eos>\n",
    "<sos> 44,30,61 <even_repeat> 44,30,44,30,44,30,44,30,44,30 <eos>\n",
    "<sos> 50,82,41 <odd_repeat> 41,41,41,41,41,41,41,41,41,41 <eos>\n",
    "<sos> 82,77,9,57,84,5,42,86 <reverse> 86,42,5,84,57,9,77,82 <eos>\n",
    "<sos> 425,616,162,221,244 <descending> 616,425,244,221,162 <eos>\n",
    "<sos> 190,676,440 <ascending> 190,440,676 <eos>\n",
    "```\n",
    "\n",
    "### Functions are bounded to produce combinations that do not exceed 999 in the training data.\n",
    "\n",
    "\n",
    "The transformer model used is based on Adrej Karpathy's NanoGPT\n",
    "https://github.com/karpathy/nanoGPT/tree/master\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Notebook by Justin Thomas August 2025\n",
    "\n",
    "[Current Status] it trains but I'm having an issue with testing/inference. This is 2.0 version of this notebook. The 1.0 version trained properly-ish, using a number range of 1-9.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7uq5gACf9xGx",
   "metadata": {
    "id": "7uq5gACf9xGx"
   },
   "source": [
    "## GOALS:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdf8134-f304-4ee2-928f-b41681a9a55c",
   "metadata": {
    "id": "ecdf8134-f304-4ee2-928f-b41681a9a55c"
   },
   "source": [
    "Ideas for improvements:\n",
    "\n",
    "done- increase the range of numbers to double and triple digit numbers.\n",
    "\n",
    "done - add commas to seperate the values.\n",
    "\n",
    "done - wider range of sequence lengths\n",
    "\n",
    "done- ensure there are no duplicate examples, and that the test set should include examples that do not occur in the training data.\n",
    "\n",
    "## extra task ideas\n",
    "\n",
    "Might need to systematically create examples rather than randomly. such as iterating through each possibility. Ehh but once I add in the apropriate complexity, adding every combination should be infeasible\n",
    "\n",
    "Could have tasks were the answer should be seperated by commas.\n",
    "\n",
    "\n",
    "\n",
    "operate on floating point numbers, using a special token for how many decimal places to include.\n",
    "\n",
    "## Tricky ideas for testing generalization\n",
    "\n",
    "have input sequence of 1-5, then try a 6th input digit.\n",
    "try have input sequence of 1-5, and 7. test the output on a six digit sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f601d3-983b-4a90-a1bc-420436555afc",
   "metadata": {
    "id": "71f601d3-983b-4a90-a1bc-420436555afc"
   },
   "source": [
    "# vocab expansion - up to 999 for outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b215341-6940-4b6a-9436-208cd2159528",
   "metadata": {
    "id": "5b215341-6940-4b6a-9436-208cd2159528"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enA4phy20MyQ",
   "metadata": {
    "id": "enA4phy20MyQ"
   },
   "source": [
    "## Todo\n",
    "\n",
    "Fix the testing script.\n",
    "\n",
    "Ensure the EOS token is handled correctly by the tokenizer\n",
    "\n",
    "Add table of\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T6HQT3t00svI",
   "metadata": {
    "id": "T6HQT3t00svI"
   },
   "source": [
    "GPT suggestion:\n",
    "\n",
    "Got it üëç Thanks for pasting everything ‚Äî I see exactly what‚Äôs happening.\n",
    "\n",
    "Your error:\n",
    "\n",
    "```\n",
    "Error during testing: can't assign a NoneType to a torch.cuda.LongTensor\n",
    "```\n",
    "\n",
    "comes from this line in your `generate` method:\n",
    "\n",
    "```python\n",
    "idx_next[finished.unsqueeze(1)] = eos_token_id\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Why it happens\n",
    "\n",
    "* You only pass `eos_token_id=None` into `generate` (since you didn‚Äôt specify it when calling).\n",
    "* Then in the code, it still tries to do\n",
    "\n",
    "  ```python\n",
    "  idx_next[finished.unsqueeze(1)] = eos_token_id\n",
    "  ```\n",
    "\n",
    "  even when `eos_token_id` is `None`.\n",
    "* PyTorch won‚Äôt let you assign `None` into a tensor, hence the crash.\n",
    "\n",
    "---\n",
    "\n",
    "### Fix\n",
    "\n",
    "We just need to make sure we **only assign EOS tokens when one is actually provided**. Modify your `generate` method like this:\n",
    "\n",
    "```python\n",
    "@torch.no_grad()\n",
    "def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
    "    batch_size = idx.size(0)\n",
    "    finished = torch.zeros(batch_size, dtype=torch.bool, device=idx.device)\n",
    "\n",
    "    for _ in range(max_new_tokens):\n",
    "        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "        logits, _ = self(idx_cond)\n",
    "        logits = logits[:, -1, :] / temperature\n",
    "\n",
    "        if top_k is not None:\n",
    "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        idx_next = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "        # Append token\n",
    "        idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        if eos_token_id is not None:\n",
    "            # Update finished mask\n",
    "            finished = finished | (idx_next.squeeze(1) == eos_token_id)\n",
    "            # If all sequences finished, break early\n",
    "            if finished.all():\n",
    "                break\n",
    "            # Replace tokens in finished sequences with eos_token_id\n",
    "            idx_next[finished.unsqueeze(1)] = eos_token_id\n",
    "\n",
    "    return idx\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Extra note\n",
    "\n",
    "Since your dataset clearly has `<eos>`, you should probably **tell the model what token ID that is**.\n",
    "Right now you‚Äôre not giving `eos_token_id` when calling:\n",
    "\n",
    "```python\n",
    "generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "```\n",
    "\n",
    "Change it to:\n",
    "\n",
    "```python\n",
    "eos_id = tokenizer.token2id.get(\"<eos>\", None)\n",
    "generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, temperature=temperature, eos_token_id=eos_id)\n",
    "```\n",
    "\n",
    "That way the model stops at `<eos>` properly.\n",
    "\n",
    "---\n",
    "\n",
    "‚úÖ So the **two-part fix** is:\n",
    "\n",
    "1. Patch `generate` to only assign `eos_token_id` if it‚Äôs not `None`.\n",
    "2. Pass the actual `<eos>` ID from your tokenizer when you call `generate`.\n",
    "\n",
    "---\n",
    "\n",
    "Do you want me to paste your **entire corrected code file** with those fixes in place (so you can drop it in), or just the minimal diffs?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ux1-1mxe0rzg",
   "metadata": {
    "id": "ux1-1mxe0rzg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83e3fbb6-ba20-423e-8709-4522a37a4d65",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83e3fbb6-ba20-423e-8709-4522a37a4d65",
    "outputId": "1c23ec23-c3df-46c7-95f6-b568b2be9222"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 30,65,70,79,10,29,3,67,37 <sum> 390 <eos>\n",
      "<sos> 75,94,21,11,31,53,43,96,34 <sum> 458 <eos>\n",
      "<sos> 22,20,28,91,56,96,52,14 <sum> 379 <eos>\n",
      "<sos> 47,70,73 <sum> 190 <eos>\n",
      "<sos> 59,37,65 <sum> 161 <eos>\n",
      "<sos> 35,75,61,37,16,85,67,22,65 <sum> 463 <eos>\n",
      "<sos> 26,13,1,10,56,49,21,0 <sum> 176 <eos>\n",
      "<sos> 58,15,76,73 <sum> 222 <eos>\n",
      "<sos> 64,88,59,82,37,10,52 <sum> 392 <eos>\n",
      "<sos> 31,33,86,76,98,58,53,71 <sum> 506 <eos>\n",
      "<sos> 7,38,4,67,33,33,46,49,39 <sum> 316 <eos>\n",
      "<sos> 78,80,96,16,30 <sum> 300 <eos>\n",
      "<sos> 9,21,45 <sum> 75 <eos>\n",
      "<sos> 66,14,88,63,63 <sum> 294 <eos>\n",
      "<sos> 27,43,43,49,43,70,51,22,83,24 <sum> 455 <eos>\n",
      "<sos> 86,5,72,57,68,78,98 <sum> 464 <eos>\n",
      "<sos> 55,8,60,72,68,75,7,36,32,63 <sum> 476 <eos>\n",
      "<sos> 22,55,61 <sum> 138 <eos>\n",
      "<sos> 35,49,11,88,79,55,68,94,80 <sum> 559 <eos>\n",
      "<sos> 44,42,90,63,43,72,54,97,26 <sum> 531 <eos>\n",
      "<sos> 77,93,32,55,87,27 <sum> 371 <eos>\n",
      "<sos> 16,62,38,14 <sum> 130 <eos>\n",
      "<sos> 0,80,7,66,40,83 <sum> 276 <eos>\n",
      "<sos> 67,76,26,1,73,54 <sum> 297 <eos>\n",
      "<sos> 47,71,84,42,70,83,27,86 <sum> 510 <eos>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Revised Version\n",
    "\n",
    "def generate_random_numbers_and_sum():\n",
    "    \"\"\"\n",
    "    Generate 3‚Äì10 random digits, each between 0 and 99, and output:\n",
    "    <sos> (digits separated by commas) <sum> total <eos>\n",
    "    \"\"\"\n",
    "    # Pick a random length between 3 and 10\n",
    "    length = random.randint(3, 10)\n",
    "\n",
    "    # Generate that many random digits, each between 0 and 99\n",
    "    numbers = [random.randint(0, 99) for _ in range(length)]\n",
    "\n",
    "    # Calculate the sum of the numbers\n",
    "    total_sum = sum(numbers)\n",
    "\n",
    "    # Join the numbers into a single string separated by commas\n",
    "    numbers_str = ','.join(map(str, numbers))\n",
    "\n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <sum> {total_sum} <eos>\"\n",
    "    return output\n",
    "\n",
    "for _ in range(25):\n",
    "    print(generate_random_numbers_and_sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64e2ce4-6214-440f-ac6c-c4bd7f4d2ee8",
   "metadata": {
    "id": "c64e2ce4-6214-440f-ac6c-c4bd7f4d2ee8"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e21ebe7-b0ca-4b5a-9583-ee38857d4201",
   "metadata": {
    "id": "6e21ebe7-b0ca-4b5a-9583-ee38857d4201"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wRwr4qt0u144",
   "metadata": {
    "id": "wRwr4qt0u144"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0b637-dcaf-416a-aa97-96b8baeb91e4",
   "metadata": {
    "id": "4af0b637-dcaf-416a-aa97-96b8baeb91e4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7xZjbK8Yu5c4",
   "metadata": {
    "id": "7xZjbK8Yu5c4"
   },
   "source": [
    "#Funtions defined\n",
    "\n",
    "generate_random_numbers_and_sum()\n",
    "\n",
    "generate_even_repeat_sequence()\n",
    "\n",
    "generate_odd_repeat_sequence()\n",
    "\n",
    "generate_random_numbers_and_reverse()\n",
    "\n",
    "generate_ascending_sort()\n",
    "\n",
    "generate_descending_sort()\n",
    "\n",
    "\n",
    "#Functions are bounded to produce combinations that do not exceed 999.\n",
    "\n",
    "# examples:\n",
    "\n",
    "```\n",
    "\n",
    "<sos> 55,50,14,9,10,36,56,53,47,21 <sum> 351 <eos>\n",
    "<sos> 44,30,61 <even_repeat> 44,30,44,30,44,30,44,30,44,30 <eos>\n",
    "<sos> 50,82,41 <odd_repeat> 41,41,41,41,41,41,41,41,41,41 <eos>\n",
    "<sos> 82,77,9,57,84,5,42,86 <reverse> 86,42,5,84,57,9,77,82 <eos>\n",
    "<sos> 425,616,162,221,244 <descending> 616,425,244,221,162 <eos>\n",
    "<sos> 190,676,440 <ascending> 190,440,676 <eos>\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5d3dfb0-165a-4ac9-8c61-1b6cbfdb80c7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5d3dfb0-165a-4ac9-8c61-1b6cbfdb80c7",
    "outputId": "61e266d6-51a8-499d-c926-244ee606af32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even repeat examples:\n",
      "<sos> 35,55,35 <even_repeat>  <eos>\n",
      "<sos> 78,76,42,93,55 <even_repeat> 78,76,42,78,76,42,78,76,42,78 <eos>\n",
      "<sos> 6,35,42 <even_repeat> 6,42,6,42,6,42,6,42,6,42 <eos>\n",
      "<sos> 23,77,76,75,99 <even_repeat> 76,76,76,76,76,76,76,76,76,76 <eos>\n",
      "<sos> 1,3,4,1 <even_repeat> 4,4,4,4,4,4,4,4,4,4 <eos>\n",
      "<sos> 27,94,47 <even_repeat> 94,94,94,94,94,94,94,94,94,94 <eos>\n",
      "<sos> 62,74,20 <even_repeat> 62,74,20,62,74,20,62,74,20,62 <eos>\n",
      "<sos> 15,25,40,3 <even_repeat> 40,40,40,40,40,40,40,40,40,40 <eos>\n",
      "<sos> 73,93,93,24,15 <even_repeat> 24,24,24,24,24,24,24,24,24,24 <eos>\n",
      "<sos> 50,40,3 <even_repeat> 50,40,50,40,50,40,50,40,50,40 <eos>\n",
      "<sos> 61,26,7 <even_repeat> 26,26,26,26,26,26,26,26,26,26 <eos>\n",
      "<sos> 33,66,2,26,84 <even_repeat> 66,2,26,84,66,2,26,84,66,2 <eos>\n",
      "<sos> 75,89,92,69,85 <even_repeat> 92,92,92,92,92,92,92,92,92,92 <eos>\n",
      "<sos> 69,85,4,81,51 <even_repeat> 4,4,4,4,4,4,4,4,4,4 <eos>\n",
      "<sos> 69,78,63,49,91 <even_repeat> 78,78,78,78,78,78,78,78,78,78 <eos>\n",
      "<sos> 63,73,5,85 <even_repeat>  <eos>\n",
      "<sos> 89,65,33,52 <even_repeat> 52,52,52,52,52,52,52,52,52,52 <eos>\n",
      "<sos> 73,61,14,84,87 <even_repeat> 14,84,14,84,14,84,14,84,14,84 <eos>\n",
      "<sos> 91,88,94,28,16 <even_repeat> 88,94,28,16,88,94,28,16,88,94 <eos>\n",
      "<sos> 15,64,27 <even_repeat> 64,64,64,64,64,64,64,64,64,64 <eos>\n",
      "<sos> 94,68,83,61 <even_repeat> 94,68,94,68,94,68,94,68,94,68 <eos>\n",
      "<sos> 4,56,29,39,67 <even_repeat> 4,56,4,56,4,56,4,56,4,56 <eos>\n",
      "<sos> 23,35,59 <even_repeat>  <eos>\n",
      "<sos> 31,64,97,32,63 <even_repeat> 64,32,64,32,64,32,64,32,64,32 <eos>\n",
      "<sos> 64,32,78,60 <even_repeat> 64,32,78,60,64,32,78,60,64,32 <eos>\n",
      "\n",
      "Odd repeat examples:\n",
      "<sos> 68,21,97,67,56 <odd_repeat> 21,97,67,21,97,67,21,97,67,21 <eos>\n",
      "<sos> 71,77,5 <odd_repeat> 71,77,5,71,77,5,71,77,5,71 <eos>\n",
      "<sos> 55,96,6,12 <odd_repeat> 55,55,55,55,55,55,55,55,55,55 <eos>\n",
      "<sos> 74,36,44 <odd_repeat>  <eos>\n",
      "<sos> 19,4,96 <odd_repeat> 19,19,19,19,19,19,19,19,19,19 <eos>\n",
      "<sos> 9,59,51,24 <odd_repeat> 9,59,51,9,59,51,9,59,51,9 <eos>\n",
      "<sos> 18,5,2 <odd_repeat> 5,5,5,5,5,5,5,5,5,5 <eos>\n",
      "<sos> 18,79,22 <odd_repeat> 79,79,79,79,79,79,79,79,79,79 <eos>\n",
      "<sos> 46,72,59,57,30 <odd_repeat> 59,57,59,57,59,57,59,57,59,57 <eos>\n",
      "<sos> 66,90,43,17,32 <odd_repeat> 43,17,43,17,43,17,43,17,43,17 <eos>\n",
      "<sos> 48,23,27 <odd_repeat> 23,27,23,27,23,27,23,27,23,27 <eos>\n",
      "<sos> 1,58,3 <odd_repeat> 1,3,1,3,1,3,1,3,1,3 <eos>\n",
      "<sos> 57,37,64,46,30 <odd_repeat> 57,37,57,37,57,37,57,37,57,37 <eos>\n",
      "<sos> 36,25,76,87,96 <odd_repeat> 25,87,25,87,25,87,25,87,25,87 <eos>\n",
      "<sos> 70,40,31 <odd_repeat> 31,31,31,31,31,31,31,31,31,31 <eos>\n",
      "<sos> 84,83,58,15,98 <odd_repeat> 83,15,83,15,83,15,83,15,83,15 <eos>\n",
      "<sos> 0,8,30 <odd_repeat>  <eos>\n",
      "<sos> 87,32,43,87 <odd_repeat> 87,43,87,87,43,87,87,43,87,87 <eos>\n",
      "<sos> 22,58,81,62,48 <odd_repeat> 81,81,81,81,81,81,81,81,81,81 <eos>\n",
      "<sos> 77,36,74 <odd_repeat> 77,77,77,77,77,77,77,77,77,77 <eos>\n",
      "<sos> 27,49,27 <odd_repeat> 27,49,27,27,49,27,27,49,27,27 <eos>\n",
      "<sos> 48,69,36 <odd_repeat> 69,69,69,69,69,69,69,69,69,69 <eos>\n",
      "<sos> 5,27,78,9,16 <odd_repeat> 5,27,9,5,27,9,5,27,9,5 <eos>\n",
      "<sos> 37,16,91 <odd_repeat> 37,91,37,91,37,91,37,91,37,91 <eos>\n",
      "<sos> 81,1,94 <odd_repeat> 81,1,81,1,81,1,81,1,81,1 <eos>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_even_repeat_sequence():\n",
    "    \"\"\"\n",
    "    Generate a sequence like:\n",
    "    <sos> 12,37,84,96 <even_repeat> 12,84,96,12,84,96,12,84,96,12 <eos>\n",
    "    - First part is random digits 0-99 with commas (length 3-5)\n",
    "    - Second part repeats only even numbers up to 10 total numbers\n",
    "    \"\"\"\n",
    "    # Generate 3-5 random numbers from 0-99\n",
    "    numbers = [random.randint(0, 99) for _ in range(random.randint(3, 5))]\n",
    "    numbers_str = ','.join(map(str, numbers))\n",
    "\n",
    "    # Filter for even numbers\n",
    "    even_numbers = [num for num in numbers if num % 2 == 0]\n",
    "\n",
    "    # If no even numbers, return sequence with empty repeat section\n",
    "    if not even_numbers:\n",
    "        output = f\"<sos> {numbers_str} <even_repeat>  <eos>\"\n",
    "        return output\n",
    "\n",
    "    # Repeat even numbers to fill up to 10 positions\n",
    "    repeated_evens = []\n",
    "    for i in range(10):\n",
    "        repeated_evens.append(even_numbers[i % len(even_numbers)])\n",
    "\n",
    "    repeated_str = ','.join(map(str, repeated_evens))\n",
    "\n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <even_repeat> {repeated_str} <eos>\"\n",
    "    return output\n",
    "\n",
    "def generate_odd_repeat_sequence():\n",
    "    \"\"\"\n",
    "    Generate a sequence like:\n",
    "    <sos> 12,37,84,96 <odd_repeat> 37,37,37,37,37,37,37,37,37,37 <eos>\n",
    "    - First part is random digits 0-99 with commas (length 3-5)\n",
    "    - Second part repeats only odd numbers up to 10 total numbers\n",
    "    \"\"\"\n",
    "    # Generate 3-5 random numbers from 0-99\n",
    "    numbers = [random.randint(0, 99) for _ in range(random.randint(3, 5))]\n",
    "    numbers_str = ','.join(map(str, numbers))\n",
    "\n",
    "    # Filter for odd numbers\n",
    "    odd_numbers = [num for num in numbers if num % 2 == 1]\n",
    "\n",
    "    # If no odd numbers, return sequence with empty repeat section\n",
    "    if not odd_numbers:\n",
    "        output = f\"<sos> {numbers_str} <odd_repeat>  <eos>\"\n",
    "        return output\n",
    "\n",
    "    # Repeat odd numbers to fill up to 10 positions\n",
    "    repeated_odds = []\n",
    "    for i in range(10):\n",
    "        repeated_odds.append(odd_numbers[i % len(odd_numbers)])\n",
    "\n",
    "    repeated_str = ','.join(map(str, repeated_odds))\n",
    "\n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <odd_repeat> {repeated_str} <eos>\"\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "print(\"Even repeat examples:\")\n",
    "for _ in range(25):\n",
    "    print(generate_even_repeat_sequence())\n",
    "\n",
    "print(\"\\nOdd repeat examples:\")\n",
    "for _ in range(25):\n",
    "    print(generate_odd_repeat_sequence())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a3ed4b-a3ea-412b-af6a-2e0a9932602c",
   "metadata": {
    "id": "e6a3ed4b-a3ea-412b-af6a-2e0a9932602c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4671f1f-2957-45f1-98c0-8b5789609801",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4671f1f-2957-45f1-98c0-8b5789609801",
    "outputId": "8f10ba4e-224f-4797-c2c7-fca9d58a35e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 87,96,29,89,58 <reverse> 58,89,29,96,87 <eos>\n",
      "<sos> 73,65,20,55,45,39,20 <reverse> 20,39,45,55,20,65,73 <eos>\n",
      "<sos> 76,52,87,11 <reverse> 11,87,52,76 <eos>\n",
      "<sos> 96,23,36,10,93 <reverse> 93,10,36,23,96 <eos>\n",
      "<sos> 60,67,94,5,16,35,55,3 <reverse> 3,55,35,16,5,94,67,60 <eos>\n",
      "<sos> 68,65,65,13,35,46,78,94 <reverse> 94,78,46,35,13,65,65,68 <eos>\n",
      "<sos> 49,44,14 <reverse> 14,44,49 <eos>\n",
      "<sos> 80,77,4 <reverse> 4,77,80 <eos>\n",
      "<sos> 57,4,3,17,10,0,33 <reverse> 33,0,10,17,3,4,57 <eos>\n",
      "<sos> 14,59,45 <reverse> 45,59,14 <eos>\n",
      "<sos> 22,19,81 <reverse> 81,19,22 <eos>\n",
      "<sos> 47,49,10,76,21,94 <reverse> 94,21,76,10,49,47 <eos>\n",
      "<sos> 58,51,88,2,32,6,25,28 <reverse> 28,25,6,32,2,88,51,58 <eos>\n",
      "<sos> 76,44,27 <reverse> 27,44,76 <eos>\n",
      "<sos> 35,34,8,25,60 <reverse> 60,25,8,34,35 <eos>\n",
      "<sos> 40,44,33,54,38,83 <reverse> 83,38,54,33,44,40 <eos>\n",
      "<sos> 63,13,71,59,51,50 <reverse> 50,51,59,71,13,63 <eos>\n",
      "<sos> 35,36,9,35,85,9,74 <reverse> 74,9,85,35,9,36,35 <eos>\n",
      "<sos> 21,48,12,48 <reverse> 48,12,48,21 <eos>\n",
      "<sos> 44,61,21,22 <reverse> 22,21,61,44 <eos>\n",
      "<sos> 11,74,90,56,35 <reverse> 35,56,90,74,11 <eos>\n",
      "<sos> 32,87,54,88,22,53,20 <reverse> 20,53,22,88,54,87,32 <eos>\n",
      "<sos> 52,10,10,62,85,65 <reverse> 65,85,62,10,10,52 <eos>\n",
      "<sos> 84,12,33 <reverse> 33,12,84 <eos>\n",
      "<sos> 61,67,97,36,84 <reverse> 84,36,97,67,61 <eos>\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "#Revised\n",
    "\n",
    "def generate_random_numbers_and_reverse():\n",
    "    \"\"\"\n",
    "    Generate a sequence like:\n",
    "    <sos> 12,37,84,96 <reverse> 96,84,37,12 <eos>\n",
    "    - First part is random numbers 0-99 with commas (length 3-5)\n",
    "    - Second part reverses the order of the numbers\n",
    "    \"\"\"\n",
    "    # Pick a random length between 3 and 8\n",
    "    length = random.randint(3, 8)\n",
    "\n",
    "    # Generate that many random numbers from 0-99\n",
    "    numbers = [random.randint(0, 99) for _ in range(length)]\n",
    "\n",
    "    # Join the numbers into a comma-separated string\n",
    "    numbers_str = ','.join(map(str, numbers))\n",
    "\n",
    "    # Reverse the list and join\n",
    "    reversed_str = ','.join(map(str, numbers[::-1]))\n",
    "\n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <reverse> {reversed_str} <eos>\"\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "for _ in range(25):\n",
    "    print(generate_random_numbers_and_reverse())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "751eb4b9-aa9a-4574-86bc-e65e35d6664c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "751eb4b9-aa9a-4574-86bc-e65e35d6664c",
    "outputId": "29117e35-b588-4edd-9b05-b9a5253b5109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sos> 134,315,544 <ascending> 134,315,544 <eos>\n",
      "<sos> 530,280,513,57,846 <ascending> 57,280,513,530,846 <eos>\n",
      "<sos> 4,86,828,836 <ascending> 4,86,828,836 <eos>\n",
      "<sos> 117,824,416 <ascending> 117,416,824 <eos>\n",
      "<sos> 330,236,666 <ascending> 236,330,666 <eos>\n",
      "<sos> 560,166,450,169 <descending> 560,450,169,166 <eos>\n",
      "<sos> 404,63,447,397,774 <descending> 774,447,404,397,63 <eos>\n",
      "<sos> 678,756,404,624 <descending> 756,678,624,404 <eos>\n",
      "<sos> 517,548,392 <descending> 548,517,392 <eos>\n",
      "<sos> 642,468,211,753 <descending> 753,642,468,211 <eos>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "def generate_ascending_sort():\n",
    "    \"\"\"\n",
    "    Generate 3‚Äì5 random numbers (0-999) and output:\n",
    "    <sos> numbers <ascending> sorted_numbers <eos>\n",
    "    \"\"\"\n",
    "    # Pick a random length between 3 and 5\n",
    "    length = random.randint(3, 5)\n",
    "\n",
    "    # Generate that many random numbers from 0-999\n",
    "    numbers = [random.randint(0, 999) for _ in range(length)]\n",
    "    numbers_str = ','.join(map(str, numbers))\n",
    "\n",
    "    # Sort ascending\n",
    "    sorted_numbers = sorted(numbers)\n",
    "    sorted_str = ','.join(map(str, sorted_numbers))\n",
    "\n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <ascending> {sorted_str} <eos>\"\n",
    "    return output\n",
    "\n",
    "def generate_descending_sort():\n",
    "    \"\"\"\n",
    "    Generate 3‚Äì5 random numbers (0-999) and output:\n",
    "    <sos> numbers <descending> sorted_numbers <eos>\n",
    "    \"\"\"\n",
    "    # Pick a random length between 3 and 5\n",
    "    length = random.randint(3, 5)\n",
    "\n",
    "    # Generate that many random numbers from 0-999\n",
    "    numbers = [random.randint(0, 999) for _ in range(length)]\n",
    "    numbers_str = ','.join(map(str, numbers))\n",
    "\n",
    "    # Sort descending\n",
    "    sorted_numbers = sorted(numbers, reverse=True)\n",
    "    sorted_str = ','.join(map(str, sorted_numbers))\n",
    "\n",
    "    # Format the output\n",
    "    output = f\"<sos> {numbers_str} <descending> {sorted_str} <eos>\"\n",
    "    return output\n",
    "\n",
    "# Example usage\n",
    "for _ in range(5):\n",
    "    print(generate_ascending_sort())\n",
    "\n",
    "for _ in range(5):\n",
    "    print(generate_descending_sort())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5b2dec-1adf-4c2f-b1a5-e0ad56b10feb",
   "metadata": {
    "id": "cf5b2dec-1adf-4c2f-b1a5-e0ad56b10feb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b78c083-84a7-426b-967f-952a9672d880",
   "metadata": {
    "id": "1b78c083-84a7-426b-967f-952a9672d880"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "from typing import List, Tuple, Set\n",
    "import os\n",
    "\n",
    "class CustomTokenizer:\n",
    "    def __init__(self):\n",
    "        # Define vocabulary\n",
    "        self.tokens = [\n",
    "            \"<sos>\", \"<eos>\", \"<sum>\", \"<reverse>\", \"<ascending>\", \"<descending>\",\n",
    "            \"<even>\", \"<odd>\", \"<even_repeat>\", \"<odd_repeat>\", \",\"\n",
    "        ] + [str(i) for i in range(1000)]  # 0-999\n",
    "\n",
    "        # Token ‚Üî ID mappings\n",
    "        self.token2id = {tok: idx for idx, tok in enumerate(self.tokens)}\n",
    "        self.id2token = {idx: tok for tok, idx in self.token2id.items()}\n",
    "        self.vocab_size = len(self.tokens)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        \"\"\"\n",
    "        Convert string to list of token IDs.\n",
    "        - Matches tags, numbers (0-999), and commas\n",
    "        - Ignores spaces\n",
    "        \"\"\"\n",
    "        tokens = []\n",
    "        i = 0\n",
    "        while i < len(text):\n",
    "            if text[i] == \" \":\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if text[i] == \"<\":  # special token\n",
    "                j = text.find(\">\", i)\n",
    "                if j != -1:\n",
    "                    tok = text[i:j+1]\n",
    "                    if tok in self.token2id:\n",
    "                        tokens.append(self.token2id[tok])\n",
    "                        i = j + 1\n",
    "                        continue\n",
    "\n",
    "            if text[i] == \",\":  # comma\n",
    "                tokens.append(self.token2id[\",\"])\n",
    "                i += 1\n",
    "                continue\n",
    "\n",
    "            if text[i].isdigit():  # start of number\n",
    "                # Extract full number (could be 1-3 digits)\n",
    "                j = i\n",
    "                while j < len(text) and text[j].isdigit():\n",
    "                    j += 1\n",
    "                number_str = text[i:j]\n",
    "                number = int(number_str)\n",
    "\n",
    "                if number <= 999:\n",
    "                    tokens.append(self.token2id[str(number)])\n",
    "                    i = j\n",
    "                else:\n",
    "                    raise ValueError(f\"Number out of range: {number}\")\n",
    "            else:\n",
    "                raise ValueError(f\"Unexpected character: {text[i]}\")\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        \"\"\"Convert list of IDs back to string with proper comma formatting.\"\"\"\n",
    "        tokens = [self.id2token[i] for i in token_ids]\n",
    "        result = \"\"\n",
    "\n",
    "        for i, token in enumerate(tokens):\n",
    "            if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "                # Add special token with spaces\n",
    "                if result and not result.endswith(\" \"):\n",
    "                    result += \" \"\n",
    "                result += token\n",
    "                if i < len(tokens) - 1:  # not last token\n",
    "                    result += \" \"\n",
    "            elif token == \",\":\n",
    "                result += \",\"\n",
    "            else:\n",
    "                # Number token\n",
    "                result += token\n",
    "\n",
    "        return result\n",
    "\n",
    "# Note: These functions should be imported from your existing modules\n",
    "# generate_random_numbers_and_sum()\n",
    "# generate_even_repeat_sequence()\n",
    "# generate_odd_repeat_sequence()\n",
    "# generate_random_numbers_and_reverse()\n",
    "# generate_ascending_sort()\n",
    "# generate_descending_sort()\n",
    "\n",
    "def generate_dataset(num_examples: int = 50000) -> List[str]:\n",
    "    \"\"\"Generate a balanced dataset with all command types, ensuring no duplicates\"\"\"\n",
    "    examples = set()  # Use set to automatically handle duplicates\n",
    "    generators = [\n",
    "        generate_random_numbers_and_sum, generate_random_numbers_and_reverse,\n",
    "        generate_ascending_sort, generate_descending_sort,\n",
    "        generate_even_repeat_sequence, generate_odd_repeat_sequence\n",
    "    ]\n",
    "\n",
    "    examples_per_type = num_examples // len(generators)\n",
    "\n",
    "    # Generate examples for each type\n",
    "    for generator in generators:\n",
    "        attempts = 0\n",
    "        generated_for_this_type = 0\n",
    "\n",
    "        while generated_for_this_type < examples_per_type and attempts < examples_per_type * 3:\n",
    "            example = generator()\n",
    "            if example not in examples:\n",
    "                examples.add(example)\n",
    "                generated_for_this_type += 1\n",
    "            attempts += 1\n",
    "\n",
    "    # Fill remaining examples\n",
    "    while len(examples) < num_examples:\n",
    "        generator = random.choice(generators)\n",
    "        example = generator()\n",
    "        examples.add(example)\n",
    "\n",
    "        # Prevent infinite loop if we can't generate enough unique examples\n",
    "        if len(examples) >= num_examples * 0.95:  # Accept 95% of target if struggling\n",
    "            break\n",
    "\n",
    "    # Convert back to list and shuffle\n",
    "    examples_list = list(examples)\n",
    "    random.shuffle(examples_list)\n",
    "\n",
    "    print(f\"Generated {len(examples_list)} unique examples (requested {num_examples})\")\n",
    "    return examples_list\n",
    "\n",
    "def create_training_data(examples: List[str], tokenizer: CustomTokenizer) -> Tuple[np.ndarray, dict]:\n",
    "    \"\"\"Convert examples to tokenized training data\"\"\"\n",
    "    tokenized_examples = []\n",
    "    max_length = 0\n",
    "\n",
    "    # Tokenize all examples and find max length\n",
    "    for example in examples:\n",
    "        tokens = tokenizer.encode(example)\n",
    "        tokenized_examples.append(tokens)\n",
    "        max_length = max(max_length, len(tokens))\n",
    "\n",
    "    print(f\"Maximum sequence length: {max_length}\")\n",
    "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
    "\n",
    "    # Convert to numpy array (pad sequences to max length)\n",
    "    data = np.full((len(examples), max_length), tokenizer.token2id[\"<eos>\"], dtype=np.int64)\n",
    "\n",
    "    for i, tokens in enumerate(tokenized_examples):\n",
    "        data[i, :len(tokens)] = tokens\n",
    "\n",
    "    # Create metadata\n",
    "    meta = {\n",
    "        'vocab_size': tokenizer.vocab_size,\n",
    "        'max_length': max_length,\n",
    "        'num_examples': len(examples),\n",
    "        'token2id': tokenizer.token2id,\n",
    "        'id2token': tokenizer.id2token\n",
    "    }\n",
    "\n",
    "    return data, meta\n",
    "\n",
    "def test_tokenizer():\n",
    "    \"\"\"Test the tokenizer with sample examples\"\"\"\n",
    "    print(\"Testing Tokenizer...\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    tokenizer = CustomTokenizer()\n",
    "\n",
    "    # Test with each type of example\n",
    "    test_examples = [\n",
    "        generate_random_numbers_and_sum(),\n",
    "        generate_random_numbers_and_reverse(),\n",
    "        generate_ascending_sort(),\n",
    "        generate_descending_sort(),\n",
    "        generate_even_repeat_sequence(),\n",
    "        generate_odd_repeat_sequence()\n",
    "    ]\n",
    "\n",
    "    for i, example in enumerate(test_examples):\n",
    "        print(f\"\\nTest {i+1}:\")\n",
    "        print(f\"Original: {example}\")\n",
    "\n",
    "        # Encode\n",
    "        encoded = tokenizer.encode(example)\n",
    "        print(f\"Encoded:  {encoded}\")\n",
    "\n",
    "        # Decode\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        print(f\"Decoded:  {decoded}\")\n",
    "\n",
    "        # Check if round-trip works\n",
    "        success = example.replace(\" \", \"\") == decoded.replace(\" \", \"\")\n",
    "        print(f\"Round-trip successful: {success}\")\n",
    "\n",
    "def save_dataset(train_data: np.ndarray, val_data: np.ndarray, meta: dict, data_dir: str = \"data/number_commands\"):\n",
    "    \"\"\"Save dataset to files\"\"\"\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Save binary data\n",
    "    train_data.astype(np.uint16).tofile(os.path.join(data_dir, 'train.bin'))\n",
    "    val_data.astype(np.uint16).tofile(os.path.join(data_dir, 'val.bin'))\n",
    "\n",
    "    # Save metadata\n",
    "    with open(os.path.join(data_dir, 'meta.pkl'), 'wb') as f:\n",
    "        pickle.dump(meta, f)\n",
    "\n",
    "    print(f\"Dataset saved to {data_dir}\")\n",
    "    print(f\"Train examples: {len(train_data)}\")\n",
    "    print(f\"Val examples: {len(val_data)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11f74c8-dd01-40fc-b275-39cf93c28af2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e11f74c8-dd01-40fc-b275-39cf93c28af2",
    "outputId": "08d9d2ed-ab58-4e06-dc71-17a7d8975c4e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tokenizer...\n",
      "==================================================\n",
      "\n",
      "Test 1:\n",
      "Original: <sos> 88,30,71,58,30,2,91 <sum> 370 <eos>\n",
      "Encoded:  [0, 99, 10, 41, 10, 82, 10, 69, 10, 41, 10, 13, 10, 102, 2, 381, 1]\n",
      "Decoded:  <sos> 88,30,71,58,30,2,91 <sum> 370 <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Test 2:\n",
      "Original: <sos> 31,46,61,15 <reverse> 15,61,46,31 <eos>\n",
      "Encoded:  [0, 42, 10, 57, 10, 72, 10, 26, 3, 26, 10, 72, 10, 57, 10, 42, 1]\n",
      "Decoded:  <sos> 31,46,61,15 <reverse> 15,61,46,31 <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Test 3:\n",
      "Original: <sos> 249,982,420 <ascending> 249,420,982 <eos>\n",
      "Encoded:  [0, 260, 10, 993, 10, 431, 4, 260, 10, 431, 10, 993, 1]\n",
      "Decoded:  <sos> 249,982,420 <ascending> 249,420,982 <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Test 4:\n",
      "Original: <sos> 806,553,285,497,136 <descending> 806,553,497,285,136 <eos>\n",
      "Encoded:  [0, 817, 10, 564, 10, 296, 10, 508, 10, 147, 5, 817, 10, 564, 10, 508, 10, 296, 10, 147, 1]\n",
      "Decoded:  <sos> 806,553,285,497,136 <descending> 806,553,497,285,136 <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Test 5:\n",
      "Original: <sos> 29,85,83 <even_repeat>  <eos>\n",
      "Encoded:  [0, 40, 10, 96, 10, 94, 8, 1]\n",
      "Decoded:  <sos> 29,85,83 <even_repeat> <eos>\n",
      "Round-trip successful: True\n",
      "\n",
      "Test 6:\n",
      "Original: <sos> 2,73,62 <odd_repeat> 73,73,73,73,73,73,73,73,73,73 <eos>\n",
      "Encoded:  [0, 13, 10, 84, 10, 73, 9, 84, 10, 84, 10, 84, 10, 84, 10, 84, 10, 84, 10, 84, 10, 84, 10, 84, 10, 84, 1]\n",
      "Decoded:  <sos> 2,73,62 <odd_repeat> 73,73,73,73,73,73,73,73,73,73 <eos>\n",
      "Round-trip successful: True\n"
     ]
    }
   ],
   "source": [
    "# Test the tokenizer\n",
    "test_tokenizer()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a13074a-717e-4ebc-af78-2816b43eb55b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2a13074a-717e-4ebc-af78-2816b43eb55b",
    "outputId": "844e6129-3090-40d2-a584-29751fae28c3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Generating Dataset...\n",
      "Generated 600000 unique examples (requested 600000)\n",
      "Maximum sequence length: 33\n",
      "Vocabulary size: 1011\n",
      "Maximum sequence length: 33\n",
      "Vocabulary size: 1011\n",
      "Dataset saved to data/number_commands\n",
      "Train examples: 540000\n",
      "Val examples: 60000\n"
     ]
    }
   ],
   "source": [
    "# Generate and process dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Generating Dataset...\")\n",
    "\n",
    "examples = generate_dataset(600000)\n",
    "tokenizer = CustomTokenizer()\n",
    "\n",
    "# Split into train/val\n",
    "split_idx = int(0.9 * len(examples))\n",
    "train_examples = examples[:split_idx]\n",
    "val_examples = examples[split_idx:]\n",
    "\n",
    "# Create training data\n",
    "train_data, meta = create_training_data(train_examples, tokenizer)\n",
    "val_data, _ = create_training_data(val_examples, tokenizer)\n",
    "\n",
    "# Save dataset\n",
    "save_dataset(train_data, val_data, meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f872868-8648-4af3-9486-ae0ea0c70e46",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2f872868-8648-4af3-9486-ae0ea0c70e46",
    "outputId": "b69e8848-2c4b-4f5d-c8b1-f4812c2daa2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Vocabulary size: 1011\n",
      "Max sequence length: 33\n",
      "Train data shape: (540000, 33)\n",
      "Val data shape: (60000, 33)\n",
      "\n",
      "Sample training examples:\n",
      "1: <sos> 89,76,32 <odd_repeat> 89,89,89,89,89,89,89,89,89,89 <eos>\n",
      "2: <sos> 103,271,900,565,876 <ascending> 103,271,565,876,900 <eos>\n",
      "3: <sos> 819,455,196,254 <ascending> 196,254,455,819 <eos>\n",
      "4: <sos> 771,320,964,435,932 <descending> 964,932,771,435,320 <eos>\n",
      "5: <sos> 21,138,911,845,783 <descending> 911,845,783,138,21 <eos>\n"
     ]
    }
   ],
   "source": [
    "# Show some statistics\n",
    "print(\"\\nDataset Statistics:\")\n",
    "print(f\"Vocabulary size: {meta['vocab_size']}\")\n",
    "print(f\"Max sequence length: {meta['max_length']}\")\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Val data shape: {val_data.shape}\")\n",
    "\n",
    "# Show sample training examples\n",
    "print(\"\\nSample training examples:\")\n",
    "for i in range(5):\n",
    "    tokens = train_data[i]\n",
    "    # Remove padding\n",
    "    tokens = tokens[tokens != tokenizer.token2id[\"<eos>\"]]\n",
    "    tokens = np.append(tokens, tokenizer.token2id[\"<eos>\"])  # Add back one EOS\n",
    "    decoded = tokenizer.decode(tokens.tolist())\n",
    "    print(f\"{i+1}: {decoded}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff174b9-6bfa-40c6-9358-0cb3920a5a2c",
   "metadata": {
    "id": "8ff174b9-6bfa-40c6-9358-0cb3920a5a2c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5292ce41-fdec-40dc-a2c9-8fdccbf8f53a",
   "metadata": {
    "id": "5292ce41-fdec-40dc-a2c9-8fdccbf8f53a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98b4f33c-dc03-483a-8dcf-14d567017eae",
   "metadata": {
    "id": "98b4f33c-dc03-483a-8dcf-14d567017eae"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class NumberCommandConfig:\n",
    "    block_size: int = 32  \n",
    "    vocab_size: int = 17  # Will be set from dataset\n",
    "    n_layer: int = 4      \n",
    "    n_head: int = 4      \n",
    "    n_embd: int = 128    \n",
    "    dropout: float = 0.1  # Some dropout for regularization\n",
    "    bias: bool = True     # Keep bias terms\n",
    "\n",
    "class NumberCommandTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "\n",
    "        # Weight tying - share embeddings with output layer\n",
    "        self.transformer.wte.weight = self.lm_head.weight\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\"Estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS\"\"\"\n",
    "        # First estimate the number of flops we do per iteration.\n",
    "        # See PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # Express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt)  # per second\n",
    "        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
    "        batch_size = idx.size(0)\n",
    "        finished = torch.zeros(batch_size, dtype=torch.bool, device=idx.device)\n",
    "    \n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            logits, _ = self(idx_cond)\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "    \n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "    \n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "    \n",
    "            # Handle EOS logic BEFORE appending to sequence\n",
    "            if eos_token_id is not None:\n",
    "                # For sequences that were already finished, force them to generate EOS tokens\n",
    "                idx_next[finished.unsqueeze(1)] = eos_token_id\n",
    "    \n",
    "            # Now append the (possibly modified) tokens\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "    \n",
    "            # Update finished mask and check for early termination\n",
    "            if eos_token_id is not None:\n",
    "                # Update finished mask based on what we just added\n",
    "                finished = finished | (idx_next.squeeze(1) == eos_token_id)\n",
    "                # If all sequences finished, break early\n",
    "                if finished.all():\n",
    "                    break\n",
    "    \n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87916a28-60f7-4da9-8eb1-fe6d313f75e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87916a28-60f7-4da9-8eb1-fe6d313f75e3",
    "outputId": "45ed15b7-e8e5-43ef-e200-73679aad9ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc790478-7d33-444e-a1d0-28319aa2d2e9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dc790478-7d33-444e-a1d0-28319aa2d2e9",
    "outputId": "7551c3a8-2b98-4d06-c8dc-e12b587a4413"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu118\n",
      "CUDA available: True\n",
      "CUDA device: NVIDIA GeForce RTX 3060\n",
      "‚úì Training data found\n",
      "‚úì Vocab size: 1011\n",
      "‚úì Max sequence length: 33\n",
      "‚úì Using device: cuda\n",
      "‚úì Model compilation: False\n",
      "‚úì Using dtype: bfloat16\n",
      "‚úì Testing data loading...\n",
      "‚úì Batch shape: torch.Size([2, 32]), torch.Size([2, 32])\n",
      "number of parameters: 0.92M\n",
      "‚úì Model created with 0.92M parameters\n",
      "num decayed parameter tensors: 18, with 919,936 parameters\n",
      "num non-decayed parameter tensors: 34, with 6,912 parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abrax\\AppData\\Local\\Temp\\ipykernel_18572\\3753233571.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using fused AdamW: True\n",
      "\n",
      "============================================================\n",
      "STARTING TRAINING\n",
      "============================================================\n",
      "Initial: train loss 6.7178, val loss 6.7198\n",
      "step 0: train loss 6.7162, val loss 6.7191, lr 1.49e-06\n",
      "‚úì Saved checkpoint (val_loss: 6.7191)\n",
      "iter 0: loss 6.7268, time 11528.5ms\n",
      "iter 50: loss 5.3462, time 2125.5ms\n",
      "iter 100: loss 4.4233, time 2123.1ms\n",
      "iter 150: loss 3.1653, time 2109.8ms\n",
      "iter 200: loss 2.3094, time 1889.8ms\n",
      "iter 250: loss 2.0802, time 1953.2ms\n",
      "iter 300: loss 1.9363, time 1855.9ms\n",
      "iter 350: loss 1.8345, time 1912.5ms\n",
      "iter 400: loss 1.9217, time 1918.9ms\n",
      "iter 450: loss 1.8650, time 1910.8ms\n",
      "step 500: train loss 1.8212, val loss 1.8244, lr 3.00e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.8244)\n",
      "iter 500: loss 1.7950, time 7557.7ms\n",
      "iter 550: loss 1.8098, time 2017.1ms\n",
      "iter 600: loss 1.7800, time 1772.0ms\n",
      "iter 650: loss 1.7448, time 1819.2ms\n",
      "iter 700: loss 1.7306, time 1953.4ms\n",
      "iter 750: loss 1.6593, time 1953.8ms\n",
      "iter 800: loss 1.6521, time 1976.7ms\n",
      "iter 850: loss 1.6071, time 1863.8ms\n",
      "iter 900: loss 1.6501, time 1999.6ms\n",
      "iter 950: loss 1.5975, time 2033.2ms\n",
      "step 1000: train loss 1.5382, val loss 1.5384, lr 3.00e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.5384)\n",
      "iter 1000: loss 1.6296, time 7486.4ms\n",
      "iter 1050: loss 1.5675, time 2097.0ms\n",
      "iter 1100: loss 1.5399, time 2010.5ms\n",
      "iter 1150: loss 1.5531, time 2015.2ms\n",
      "iter 1200: loss 1.5082, time 1908.7ms\n",
      "iter 1250: loss 1.4677, time 1994.4ms\n",
      "iter 1300: loss 1.5087, time 1936.8ms\n",
      "iter 1350: loss 1.4561, time 1953.1ms\n",
      "iter 1400: loss 1.4647, time 1954.7ms\n",
      "iter 1450: loss 1.4368, time 2065.0ms\n",
      "step 1500: train loss 1.4601, val loss 1.4513, lr 3.00e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.4513)\n",
      "iter 1500: loss 1.5015, time 7494.1ms\n",
      "iter 1550: loss 1.4629, time 2148.1ms\n",
      "iter 1600: loss 1.5341, time 1922.8ms\n",
      "iter 1650: loss 1.4133, time 1903.4ms\n",
      "iter 1700: loss 1.4288, time 1829.2ms\n",
      "iter 1750: loss 1.4846, time 1896.0ms\n",
      "iter 1800: loss 1.4691, time 1811.2ms\n",
      "iter 1850: loss 1.4323, time 1965.6ms\n",
      "iter 1900: loss 1.4691, time 1863.8ms\n",
      "iter 1950: loss 1.4076, time 1898.4ms\n",
      "step 2000: train loss 1.3887, val loss 1.3897, lr 2.99e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.3897)\n",
      "iter 2000: loss 1.4478, time 7372.6ms\n",
      "iter 2050: loss 1.4010, time 2067.2ms\n",
      "iter 2100: loss 1.4030, time 1913.2ms\n",
      "iter 2150: loss 1.4341, time 1916.3ms\n",
      "iter 2200: loss 1.4433, time 1880.0ms\n",
      "iter 2250: loss 1.4316, time 1861.8ms\n",
      "iter 2300: loss 1.4156, time 1934.5ms\n",
      "iter 2350: loss 1.4009, time 1868.1ms\n",
      "iter 2400: loss 1.4435, time 1748.8ms\n",
      "iter 2450: loss 1.4071, time 2073.7ms\n",
      "step 2500: train loss 1.3189, val loss 1.3211, lr 2.99e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.3211)\n",
      "iter 2500: loss 1.3838, time 7533.2ms\n",
      "iter 2550: loss 1.3714, time 2170.8ms\n",
      "iter 2600: loss 1.3555, time 1969.7ms\n",
      "iter 2650: loss 1.3544, time 2004.1ms\n",
      "iter 2700: loss 1.3680, time 2192.7ms\n",
      "iter 2750: loss 1.3386, time 1881.9ms\n",
      "iter 2800: loss 1.2933, time 1986.2ms\n",
      "iter 2850: loss 1.3469, time 1833.4ms\n",
      "iter 2900: loss 1.2490, time 1866.2ms\n",
      "iter 2950: loss 1.3136, time 1968.9ms\n",
      "step 3000: train loss 1.2318, val loss 1.2311, lr 2.98e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.2311)\n",
      "iter 3000: loss 1.2918, time 7499.0ms\n",
      "iter 3050: loss 1.2885, time 2150.9ms\n",
      "iter 3100: loss 1.3030, time 1810.2ms\n",
      "iter 3150: loss 1.2487, time 1944.4ms\n",
      "iter 3200: loss 1.2914, time 1901.4ms\n",
      "iter 3250: loss 1.2247, time 1841.4ms\n",
      "iter 3300: loss 1.2221, time 1774.8ms\n",
      "iter 3350: loss 1.2839, time 1742.5ms\n",
      "iter 3400: loss 1.2681, time 1821.2ms\n",
      "iter 3450: loss 1.2946, time 1867.3ms\n",
      "step 3500: train loss 1.1974, val loss 1.1948, lr 2.97e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1948)\n",
      "iter 3500: loss 1.2253, time 7683.9ms\n",
      "iter 3550: loss 1.2436, time 2175.0ms\n",
      "iter 3600: loss 1.2246, time 2050.2ms\n",
      "iter 3650: loss 1.2636, time 2102.6ms\n",
      "iter 3700: loss 1.2910, time 1862.2ms\n",
      "iter 3750: loss 1.2645, time 2022.0ms\n",
      "iter 3800: loss 1.2445, time 1909.3ms\n",
      "iter 3850: loss 1.2344, time 1858.8ms\n",
      "iter 3900: loss 1.2749, time 1897.1ms\n",
      "iter 3950: loss 1.2575, time 1780.5ms\n",
      "step 4000: train loss 1.1802, val loss 1.1851, lr 2.96e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1851)\n",
      "iter 4000: loss 1.2664, time 7852.2ms\n",
      "iter 4050: loss 1.2682, time 2004.0ms\n",
      "iter 4100: loss 1.2058, time 1827.7ms\n",
      "iter 4150: loss 1.2460, time 1759.3ms\n",
      "iter 4200: loss 1.2395, time 1766.6ms\n",
      "iter 4250: loss 1.2259, time 1825.4ms\n",
      "iter 4300: loss 1.2343, time 1806.1ms\n",
      "iter 4350: loss 1.2247, time 1804.8ms\n",
      "iter 4400: loss 1.2515, time 1751.7ms\n",
      "iter 4450: loss 1.2230, time 1739.3ms\n",
      "step 4500: train loss 1.1869, val loss 1.1863, lr 2.95e-04\n",
      "iter 4500: loss 1.2276, time 7092.0ms\n",
      "iter 4550: loss 1.2780, time 2043.2ms\n",
      "iter 4600: loss 1.2557, time 1855.7ms\n",
      "iter 4650: loss 1.2111, time 1783.9ms\n",
      "iter 4700: loss 1.2284, time 1781.8ms\n",
      "iter 4750: loss 1.1959, time 1804.7ms\n",
      "iter 4800: loss 1.2238, time 1748.8ms\n",
      "iter 4850: loss 1.2199, time 1778.0ms\n",
      "iter 4900: loss 1.2305, time 1735.9ms\n",
      "iter 4950: loss 1.1756, time 1763.3ms\n",
      "step 5000: train loss 1.1855, val loss 1.1795, lr 2.94e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1795)\n",
      "iter 5000: loss 1.2800, time 7355.8ms\n",
      "iter 5050: loss 1.2252, time 1911.5ms\n",
      "iter 5100: loss 1.1795, time 1881.2ms\n",
      "iter 5150: loss 1.2359, time 1810.7ms\n",
      "iter 5200: loss 1.2278, time 1632.8ms\n",
      "iter 5250: loss 1.2052, time 1731.3ms\n",
      "iter 5300: loss 1.2252, time 1830.4ms\n",
      "iter 5350: loss 1.2142, time 1793.1ms\n",
      "iter 5400: loss 1.1899, time 1878.3ms\n",
      "iter 5450: loss 1.1861, time 1987.5ms\n",
      "step 5500: train loss 1.1719, val loss 1.1745, lr 2.93e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1745)\n",
      "iter 5500: loss 1.2513, time 7559.4ms\n",
      "iter 5550: loss 1.1973, time 2177.3ms\n",
      "iter 5600: loss 1.1984, time 1914.1ms\n",
      "iter 5650: loss 1.2468, time 1931.7ms\n",
      "iter 5700: loss 1.1697, time 2077.5ms\n",
      "iter 5750: loss 1.1787, time 2082.5ms\n",
      "iter 5800: loss 1.2189, time 2010.3ms\n",
      "iter 5850: loss 1.1950, time 1904.9ms\n",
      "iter 5900: loss 1.2248, time 1980.5ms\n",
      "iter 5950: loss 1.2239, time 2063.5ms\n",
      "step 6000: train loss 1.1667, val loss 1.1664, lr 2.91e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1664)\n",
      "iter 6000: loss 1.2169, time 7929.6ms\n",
      "iter 6050: loss 1.1911, time 2193.6ms\n",
      "iter 6100: loss 1.2246, time 1919.9ms\n",
      "iter 6150: loss 1.2117, time 1945.0ms\n",
      "iter 6200: loss 1.2382, time 1891.9ms\n",
      "iter 6250: loss 1.2197, time 2192.3ms\n",
      "iter 6300: loss 1.2155, time 2052.4ms\n",
      "iter 6350: loss 1.2083, time 1910.0ms\n",
      "iter 6400: loss 1.2342, time 1877.0ms\n",
      "iter 6450: loss 1.2324, time 1999.9ms\n",
      "step 6500: train loss 1.1663, val loss 1.1687, lr 2.89e-04\n",
      "iter 6500: loss 1.1920, time 7529.8ms\n",
      "iter 6550: loss 1.1756, time 2002.8ms\n",
      "iter 6600: loss 1.2128, time 1831.0ms\n",
      "iter 6650: loss 1.2072, time 1924.6ms\n",
      "iter 6700: loss 1.2272, time 1772.8ms\n",
      "iter 6750: loss 1.1728, time 1910.5ms\n",
      "iter 6800: loss 1.2051, time 1799.0ms\n",
      "iter 6850: loss 1.1745, time 1958.5ms\n",
      "iter 6900: loss 1.2281, time 1870.2ms\n",
      "iter 6950: loss 1.2052, time 1861.1ms\n",
      "step 7000: train loss 1.1659, val loss 1.1694, lr 2.88e-04\n",
      "iter 7000: loss 1.2158, time 7276.3ms\n",
      "iter 7050: loss 1.2146, time 1988.1ms\n",
      "iter 7100: loss 1.1831, time 1874.7ms\n",
      "iter 7150: loss 1.1763, time 1953.5ms\n",
      "iter 7200: loss 1.1686, time 1887.3ms\n",
      "iter 7250: loss 1.1899, time 1837.2ms\n",
      "iter 7300: loss 1.2101, time 2038.4ms\n",
      "iter 7350: loss 1.2612, time 1865.0ms\n",
      "iter 7400: loss 1.1554, time 1922.5ms\n",
      "iter 7450: loss 1.1614, time 1842.0ms\n",
      "step 7500: train loss 1.1580, val loss 1.1590, lr 2.86e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1590)\n",
      "iter 7500: loss 1.1478, time 7780.0ms\n",
      "iter 7550: loss 1.1638, time 1938.9ms\n",
      "iter 7600: loss 1.1870, time 1835.8ms\n",
      "iter 7650: loss 1.1805, time 1792.4ms\n",
      "iter 7700: loss 1.2164, time 1836.4ms\n",
      "iter 7750: loss 1.1916, time 1774.0ms\n",
      "iter 7800: loss 1.1926, time 1742.5ms\n",
      "iter 7850: loss 1.1444, time 1718.3ms\n",
      "iter 7900: loss 1.2089, time 1701.6ms\n",
      "iter 7950: loss 1.1870, time 1723.3ms\n",
      "step 8000: train loss 1.1499, val loss 1.1522, lr 2.84e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1522)\n",
      "iter 8000: loss 1.1709, time 7090.7ms\n",
      "iter 8050: loss 1.2600, time 1860.3ms\n",
      "iter 8100: loss 1.1670, time 1824.9ms\n",
      "iter 8150: loss 1.1783, time 1731.5ms\n",
      "iter 8200: loss 1.2212, time 1723.3ms\n",
      "iter 8250: loss 1.2063, time 2218.0ms\n",
      "iter 8300: loss 1.1756, time 1998.2ms\n",
      "iter 8350: loss 1.1534, time 2038.8ms\n",
      "iter 8400: loss 1.1795, time 2181.3ms\n",
      "iter 8450: loss 1.1729, time 2272.9ms\n",
      "step 8500: train loss 1.1485, val loss 1.1487, lr 2.82e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1487)\n",
      "iter 8500: loss 1.2111, time 8024.7ms\n",
      "iter 8550: loss 1.2016, time 1904.8ms\n",
      "iter 8600: loss 1.1889, time 1808.6ms\n",
      "iter 8650: loss 1.1839, time 1772.8ms\n",
      "iter 8700: loss 1.1899, time 1744.1ms\n",
      "iter 8750: loss 1.1957, time 1811.7ms\n",
      "iter 8800: loss 1.1875, time 1856.6ms\n",
      "iter 8850: loss 1.2178, time 2236.5ms\n",
      "iter 8900: loss 1.1788, time 2068.6ms\n",
      "iter 8950: loss 1.1896, time 2022.6ms\n",
      "step 9000: train loss 1.1437, val loss 1.1483, lr 2.80e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1483)\n",
      "iter 9000: loss 1.1869, time 8681.3ms\n",
      "iter 9050: loss 1.1822, time 2215.4ms\n",
      "iter 9100: loss 1.1545, time 2585.2ms\n",
      "iter 9150: loss 1.1364, time 1964.5ms\n",
      "iter 9200: loss 1.2122, time 1821.5ms\n",
      "iter 9250: loss 1.1920, time 1692.4ms\n",
      "iter 9300: loss 1.1615, time 1712.4ms\n",
      "iter 9350: loss 1.1836, time 1719.1ms\n",
      "iter 9400: loss 1.1538, time 1747.5ms\n",
      "iter 9450: loss 1.1611, time 1766.5ms\n",
      "step 9500: train loss 1.1425, val loss 1.1445, lr 2.77e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1445)\n",
      "iter 9500: loss 1.2104, time 6936.4ms\n",
      "iter 9550: loss 1.1724, time 1860.2ms\n",
      "iter 9600: loss 1.1788, time 1815.7ms\n",
      "iter 9650: loss 1.1426, time 1889.4ms\n",
      "iter 9700: loss 1.1998, time 1699.4ms\n",
      "iter 9750: loss 1.1798, time 1701.8ms\n",
      "iter 9800: loss 1.1545, time 1696.5ms\n",
      "iter 9850: loss 1.1873, time 1715.0ms\n",
      "iter 9900: loss 1.1713, time 1717.4ms\n",
      "iter 9950: loss 1.1776, time 1678.7ms\n",
      "step 10000: train loss 1.1463, val loss 1.1452, lr 2.75e-04\n",
      "iter 10000: loss 1.1870, time 6952.3ms\n",
      "iter 10050: loss 1.1797, time 1877.4ms\n",
      "iter 10100: loss 1.1748, time 1915.9ms\n",
      "iter 10150: loss 1.1510, time 1758.1ms\n",
      "iter 10200: loss 1.1719, time 1793.3ms\n",
      "iter 10250: loss 1.1597, time 1746.5ms\n",
      "iter 10300: loss 1.1858, time 1695.5ms\n",
      "iter 10350: loss 1.1765, time 1730.6ms\n",
      "iter 10400: loss 1.1455, time 1689.6ms\n",
      "iter 10450: loss 1.1930, time 1686.4ms\n",
      "step 10500: train loss 1.1372, val loss 1.1446, lr 2.72e-04\n",
      "iter 10500: loss 1.1512, time 6617.6ms\n",
      "iter 10550: loss 1.1781, time 1774.8ms\n",
      "iter 10600: loss 1.1279, time 1693.6ms\n",
      "iter 10650: loss 1.1749, time 1796.7ms\n",
      "iter 10700: loss 1.1911, time 1677.0ms\n",
      "iter 10750: loss 1.1470, time 1679.0ms\n",
      "iter 10800: loss 1.1312, time 1678.3ms\n",
      "iter 10850: loss 1.1547, time 1692.5ms\n",
      "iter 10900: loss 1.1415, time 1690.3ms\n",
      "iter 10950: loss 1.2188, time 1694.7ms\n",
      "step 11000: train loss 1.1408, val loss 1.1445, lr 2.70e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1445)\n",
      "iter 11000: loss 1.2226, time 6680.3ms\n",
      "iter 11050: loss 1.2288, time 1807.6ms\n",
      "iter 11100: loss 1.1828, time 1682.7ms\n",
      "iter 11150: loss 1.1464, time 1678.3ms\n",
      "iter 11200: loss 1.1693, time 1688.6ms\n",
      "iter 11250: loss 1.1772, time 1806.7ms\n",
      "iter 11300: loss 1.1871, time 1703.8ms\n",
      "iter 11350: loss 1.1397, time 1678.2ms\n",
      "iter 11400: loss 1.1554, time 1680.2ms\n",
      "iter 11450: loss 1.1814, time 1689.5ms\n",
      "step 11500: train loss 1.1443, val loss 1.1456, lr 2.67e-04\n",
      "iter 11500: loss 1.2048, time 7278.3ms\n",
      "iter 11550: loss 1.1986, time 1852.0ms\n",
      "iter 11600: loss 1.1360, time 1700.9ms\n",
      "iter 11650: loss 1.1988, time 1691.5ms\n",
      "iter 11700: loss 1.2075, time 1813.9ms\n",
      "iter 11750: loss 1.2067, time 1827.0ms\n",
      "iter 11800: loss 1.1720, time 1877.4ms\n",
      "iter 11850: loss 1.1760, time 1720.2ms\n",
      "iter 11900: loss 1.1464, time 1705.6ms\n",
      "iter 11950: loss 1.1735, time 1866.8ms\n",
      "step 12000: train loss 1.1455, val loss 1.1433, lr 2.64e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1433)\n",
      "iter 12000: loss 1.1767, time 6919.8ms\n",
      "iter 12050: loss 1.1786, time 1883.8ms\n",
      "iter 12100: loss 1.1364, time 1740.8ms\n",
      "iter 12150: loss 1.1604, time 1742.5ms\n",
      "iter 12200: loss 1.1984, time 1700.6ms\n",
      "iter 12250: loss 1.1408, time 1710.7ms\n",
      "iter 12300: loss 1.1784, time 1750.3ms\n",
      "iter 12350: loss 1.1586, time 1738.6ms\n",
      "iter 12400: loss 1.1443, time 1904.6ms\n",
      "iter 12450: loss 1.1815, time 1780.7ms\n",
      "step 12500: train loss 1.1400, val loss 1.1428, lr 2.61e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1428)\n",
      "iter 12500: loss 1.1663, time 6820.7ms\n",
      "iter 12550: loss 1.1767, time 1862.4ms\n",
      "iter 12600: loss 1.1676, time 1776.9ms\n",
      "iter 12650: loss 1.1811, time 1747.4ms\n",
      "iter 12700: loss 1.1716, time 1698.9ms\n",
      "iter 12750: loss 1.1703, time 1674.8ms\n",
      "iter 12800: loss 1.1703, time 1802.3ms\n",
      "iter 12850: loss 1.1743, time 1785.1ms\n",
      "iter 12900: loss 1.1680, time 1881.0ms\n",
      "iter 12950: loss 1.1672, time 2002.3ms\n",
      "step 13000: train loss 1.1390, val loss 1.1377, lr 2.58e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1377)\n",
      "iter 13000: loss 1.1637, time 7333.1ms\n",
      "iter 13050: loss 1.1306, time 2107.1ms\n",
      "iter 13100: loss 1.1488, time 1961.0ms\n",
      "iter 13150: loss 1.1693, time 1743.5ms\n",
      "iter 13200: loss 1.1775, time 1838.3ms\n",
      "iter 13250: loss 1.1781, time 1832.3ms\n",
      "iter 13300: loss 1.1300, time 1815.7ms\n",
      "iter 13350: loss 1.1768, time 1832.7ms\n",
      "iter 13400: loss 1.1146, time 1826.9ms\n",
      "iter 13450: loss 1.1514, time 1771.7ms\n",
      "step 13500: train loss 1.1396, val loss 1.1437, lr 2.55e-04\n",
      "iter 13500: loss 1.1644, time 7702.8ms\n",
      "iter 13550: loss 1.1684, time 2174.2ms\n",
      "iter 13600: loss 1.1629, time 1967.0ms\n",
      "iter 13650: loss 1.1859, time 1959.5ms\n",
      "iter 13700: loss 1.1871, time 1894.4ms\n",
      "iter 13750: loss 1.1566, time 1978.0ms\n",
      "iter 13800: loss 1.1677, time 1956.5ms\n",
      "iter 13850: loss 1.1636, time 1910.3ms\n",
      "iter 13900: loss 1.1539, time 2070.8ms\n",
      "iter 13950: loss 1.1613, time 1863.1ms\n",
      "step 14000: train loss 1.1418, val loss 1.1379, lr 2.52e-04\n",
      "iter 14000: loss 1.1800, time 7765.3ms\n",
      "iter 14050: loss 1.1539, time 1898.3ms\n",
      "iter 14100: loss 1.1662, time 1807.1ms\n",
      "iter 14150: loss 1.1802, time 1730.4ms\n",
      "iter 14200: loss 1.1526, time 1703.1ms\n",
      "iter 14250: loss 1.1870, time 1761.9ms\n",
      "iter 14300: loss 1.1627, time 1781.5ms\n",
      "iter 14350: loss 1.1718, time 1747.2ms\n",
      "iter 14400: loss 1.1472, time 1768.3ms\n",
      "iter 14450: loss 1.1887, time 1957.2ms\n",
      "step 14500: train loss 1.1416, val loss 1.1373, lr 2.49e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1373)\n",
      "iter 14500: loss 1.1358, time 7104.1ms\n",
      "iter 14550: loss 1.1384, time 1888.3ms\n",
      "iter 14600: loss 1.1602, time 1776.3ms\n",
      "iter 14650: loss 1.2151, time 1763.5ms\n",
      "iter 14700: loss 1.1408, time 1743.9ms\n",
      "iter 14750: loss 1.1700, time 1754.4ms\n",
      "iter 14800: loss 1.1509, time 1867.7ms\n",
      "iter 14850: loss 1.1920, time 1811.6ms\n",
      "iter 14900: loss 1.1155, time 1795.9ms\n",
      "iter 14950: loss 1.1749, time 1851.5ms\n",
      "step 15000: train loss 1.1384, val loss 1.1377, lr 2.45e-04\n",
      "iter 15000: loss 1.1753, time 7071.3ms\n",
      "iter 15050: loss 1.2047, time 2087.3ms\n",
      "iter 15100: loss 1.1436, time 1880.2ms\n",
      "iter 15150: loss 1.1389, time 1755.2ms\n",
      "iter 15200: loss 1.1761, time 1756.1ms\n",
      "iter 15250: loss 1.1638, time 1752.0ms\n",
      "iter 15300: loss 1.1858, time 1746.4ms\n",
      "iter 15350: loss 1.1449, time 1840.2ms\n",
      "iter 15400: loss 1.1408, time 1915.6ms\n",
      "iter 15450: loss 1.1414, time 1746.4ms\n",
      "step 15500: train loss 1.1387, val loss 1.1362, lr 2.42e-04\n",
      "‚úì Saved checkpoint (val_loss: 1.1362)\n",
      "iter 15500: loss 1.1404, time 7076.5ms\n",
      "iter 15550: loss 1.1532, time 1913.2ms\n",
      "iter 15600: loss 1.1834, time 1859.6ms\n",
      "iter 15650: loss 1.1803, time 1968.9ms\n",
      "iter 15700: loss 1.1345, time 1995.2ms\n",
      "iter 15750: loss 1.1292, time 1793.9ms\n",
      "iter 15800: loss 1.1548, time 1915.7ms\n",
      "iter 15850: loss 1.1125, time 1810.9ms\n",
      "iter 15900: loss 1.1089, time 1772.6ms\n",
      "iter 15950: loss 1.1250, time 1884.3ms\n",
      "step 16000: train loss 1.1353, val loss 1.1417, lr 2.38e-04\n",
      "iter 16000: loss 1.1629, time 7314.0ms\n",
      "iter 16050: loss 1.1684, time 2133.0ms\n",
      "iter 16100: loss 1.1031, time 2050.5ms\n",
      "iter 16150: loss 1.1859, time 1988.3ms\n",
      "iter 16200: loss 1.1746, time 2103.9ms\n",
      "iter 16250: loss 1.1720, time 2241.0ms\n"
     ]
    }
   ],
   "source": [
    "# ALL-IN-ONE TRAINING SCRIPT FOR NUMBER COMMAND TRANSFORMER\n",
    "#\n",
    "\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "import platform\n",
    "from contextlib import nullcontext\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA device:\", torch.cuda.get_device_name())\n",
    "\n",
    "# Check if we have the required data\n",
    "data_dir = 'data/number_commands'\n",
    "if not os.path.exists(os.path.join(data_dir, 'train.bin')):\n",
    "    raise FileNotFoundError(f\"Training data not found at {data_dir}. Please run dataset generation first.\")\n",
    "\n",
    "print(\"‚úì Training data found\")\n",
    "\n",
    "# Load metadata\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "with open(meta_path, 'rb') as f:\n",
    "    meta = pickle.load(f)\n",
    "\n",
    "vocab_size = meta['vocab_size']\n",
    "max_length = meta['max_length']\n",
    "\n",
    "print(f\"‚úì Vocab size: {vocab_size}\")\n",
    "print(f\"‚úì Max sequence length: {max_length}\")\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    'vocab_size': vocab_size,\n",
    "    'block_size': min(max_length, 32),  # Use actual max length or 32, whichever is smaller\n",
    "    'n_layer': 4,\n",
    "    'n_head': 4,\n",
    "    'n_embd': 128, #worked iwth 128, let's try smaller for our massive 1011 token vocab\n",
    "    'dropout': 0.1,\n",
    "    'bias': True,\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 3e-4,\n",
    "    'max_iters': 50000,\n",
    "    'eval_interval': 500,\n",
    "    'log_interval': 50,\n",
    "    'warmup_iters': 200,\n",
    "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    'compile': torch.cuda.is_available() and platform.system() != 'Windows',\n",
    "    'out_dir': 'out_number_commands'\n",
    "}\n",
    "\n",
    "print(f\"‚úì Using device: {config['device']}\")\n",
    "print(f\"‚úì Model compilation: {config['compile']}\")\n",
    "\n",
    "# Set up device and dtype\n",
    "device = config['device']\n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
    "\n",
    "print(f\"‚úì Using dtype: {dtype}\")\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(config['out_dir'], exist_ok=True)\n",
    "\n",
    "# Set seeds\n",
    "torch.manual_seed(1337)\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# Data loading function\n",
    "def get_batch(split, batch_size, block_size):\n",
    "    if split == 'train':\n",
    "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "    else:\n",
    "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "\n",
    "    if device == 'cuda':\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "# Test data loading\n",
    "print(\"‚úì Testing data loading...\")\n",
    "test_x, test_y = get_batch('train', 2, config['block_size'])\n",
    "print(f\"‚úì Batch shape: {test_x.shape}, {test_y.shape}\")\n",
    "\n",
    "# Create and initialize model (model classes should be defined already)\n",
    "try:\n",
    "    model_config = NumberCommandConfig(\n",
    "        vocab_size=config['vocab_size'],\n",
    "        block_size=config['block_size'],\n",
    "        n_layer=config['n_layer'],\n",
    "        n_head=config['n_head'],\n",
    "        n_embd=config['n_embd'],\n",
    "        dropout=config['dropout'],\n",
    "        bias=config['bias']\n",
    "    )\n",
    "\n",
    "    model = NumberCommandTransformer(model_config)\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"‚úì Model created with {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "\n",
    "except NameError:\n",
    "    print(\"‚ùå Model classes not found! Please run the model definition cell first.\")\n",
    "    raise\n",
    "\n",
    "# Initialize training components\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "optimizer = model.configure_optimizers(\n",
    "    weight_decay=1e-2,\n",
    "    learning_rate=config['learning_rate'],\n",
    "    betas=(0.9, 0.95),\n",
    "    device_type=device\n",
    ")\n",
    "\n",
    "# Compile model with error handling\n",
    "if config['compile']:\n",
    "    try:\n",
    "        print(\"Compiling model...\")\n",
    "        unoptimized_model = model\n",
    "        model = torch.compile(model)\n",
    "        print(\"‚úì Model compilation successful!\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† Model compilation failed: {e}\")\n",
    "        print(\"Continuing without compilation...\")\n",
    "\n",
    "# Loss estimation function\n",
    "@torch.no_grad()\n",
    "def estimate_loss(eval_iters=100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, config['batch_size'], config['block_size'])\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Learning rate scheduler\n",
    "def get_lr(it):\n",
    "    if it < config['warmup_iters']:\n",
    "        return config['learning_rate'] * (it + 1) / (config['warmup_iters'] + 1)\n",
    "    if it > config['max_iters']:\n",
    "        return config['learning_rate'] * 0.1\n",
    "    decay_ratio = (it - config['warmup_iters']) / (config['max_iters'] - config['warmup_iters'])\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return config['learning_rate'] * 0.1 + coeff * (config['learning_rate'] - config['learning_rate'] * 0.1)\n",
    "\n",
    "# Training loop\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"STARTING TRAINING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = float('inf')\n",
    "t0 = time.time()\n",
    "\n",
    "# Initial evaluation\n",
    "losses = estimate_loss()\n",
    "print(f\"Initial: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "\n",
    "while iter_num < config['max_iters']:\n",
    "    # Set learning rate\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # Evaluate and save checkpoints\n",
    "    if iter_num % config['eval_interval'] == 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.2e}\")\n",
    "\n",
    "        if losses['val'] < best_val_loss:\n",
    "            best_val_loss = losses['val']\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'model_args': config,\n",
    "                'iter_num': iter_num,\n",
    "                'best_val_loss': best_val_loss,\n",
    "            }\n",
    "            torch.save(checkpoint, os.path.join(config['out_dir'], 'ckpt.pt'))\n",
    "            print(f\"‚úì Saved checkpoint (val_loss: {best_val_loss:.4f})\")\n",
    "\n",
    "    # Training step\n",
    "    X, Y = get_batch('train', config['batch_size'], config['block_size'])\n",
    "\n",
    "    with ctx:\n",
    "        logits, loss = model(X, Y)\n",
    "\n",
    "    # Backward pass\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # Logging\n",
    "    if iter_num % config['log_interval'] == 0:\n",
    "        t1 = time.time()\n",
    "        dt = t1 - t0\n",
    "        t0 = t1\n",
    "        print(f\"iter {iter_num}: loss {loss.item():.4f}, time {dt*1000:.1f}ms\")\n",
    "\n",
    "    iter_num += 1\n",
    "\n",
    "# Final evaluation\n",
    "losses = estimate_loss()\n",
    "print(f\"\\nFinal: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "print(f\"Training completed! Best val loss: {best_val_loss:.4f}\")\n",
    "print(f\"Model saved to: {config['out_dir']}/ckpt.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0257dc83-bd1b-4a2a-b3ff-dfa851859f90",
   "metadata": {
    "id": "0257dc83-bd1b-4a2a-b3ff-dfa851859f90"
   },
   "source": [
    "# Now for Some Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b56399-0e6f-40f2-a295-cfb65f6f5adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_and_tokenizer():\n",
    "    \"\"\"Load the trained model and recreate tokenizer from metadata\"\"\"\n",
    "    \n",
    "    # Load metadata\n",
    "    meta_path = 'data/number_commands/meta.pkl'\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    \n",
    "    # Recreate tokenizer\n",
    "    class CustomTokenizer:\n",
    "        def __init__(self, token2id, id2token):\n",
    "            self.token2id = token2id\n",
    "            self.id2token = id2token\n",
    "            self.vocab_size = len(token2id)\n",
    "\n",
    "        def encode(self, text: str) -> List[int]:\n",
    "            tokens = []\n",
    "            i = 0\n",
    "            while i < len(text):\n",
    "                if text[i] == \" \":\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if text[i] == \"<\":\n",
    "                    j = text.find(\">\", i)\n",
    "                    if j != -1:\n",
    "                        tok = text[i:j+1]\n",
    "                        if tok in self.token2id:\n",
    "                            tokens.append(self.token2id[tok])\n",
    "                            i = j + 1\n",
    "                            continue\n",
    "\n",
    "                if text[i] == \",\":\n",
    "                    tokens.append(self.token2id[\",\"])\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if text[i].isdigit():\n",
    "                    j = i\n",
    "                    while j < len(text) and text[j].isdigit():\n",
    "                        j += 1\n",
    "                    number_str = text[i:j]\n",
    "                    number = int(number_str)\n",
    "\n",
    "                    if str(number) in self.token2id:\n",
    "                        tokens.append(self.token2id[str(number)])\n",
    "                        i = j\n",
    "                    else:\n",
    "                        raise ValueError(f\"Number out of range: {number}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected character: {text[i]}\")\n",
    "\n",
    "            return tokens\n",
    "\n",
    "        def decode(self, token_ids: List[int]) -> str:\n",
    "            tokens = [self.id2token[i] for i in token_ids]\n",
    "            result = \"\"\n",
    "\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "                    if result and not result.endswith(\" \"):\n",
    "                        result += \" \"\n",
    "                    result += token\n",
    "                    if i < len(tokens) - 1:\n",
    "                        result += \" \"\n",
    "                elif token == \",\":\n",
    "                    result += \",\"\n",
    "                else:\n",
    "                    result += token\n",
    "\n",
    "            return result\n",
    "\n",
    "    tokenizer = CustomTokenizer(meta['token2id'], meta['id2token'])\n",
    "    \n",
    "    # Load model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    checkpoint_path = 'out_number_commands/ckpt.pt'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    \n",
    "    # Create model config\n",
    "    model_args = checkpoint['model_args']\n",
    "    config_params = {\n",
    "        'block_size': model_args['block_size'],\n",
    "        'vocab_size': model_args['vocab_size'], \n",
    "        'n_layer': model_args['n_layer'],\n",
    "        'n_head': model_args['n_head'],\n",
    "        'n_embd': model_args['n_embd'],\n",
    "        'dropout': model_args['dropout'],\n",
    "        'bias': model_args['bias']\n",
    "    }\n",
    "    \n",
    "    config = NumberCommandConfig(**config_params)\n",
    "    model = NumberCommandTransformer(config)\n",
    "    \n",
    "    # Load state dict (remove compilation prefix if exists)\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k, v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "    \n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    return model, tokenizer, device\n",
    "\n",
    "def interactive_model_testing():\n",
    "    \"\"\"Interactive testing interface for the number command transformer\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Load model and tokenizer\n",
    "        model, tokenizer, device = load_model_and_tokenizer()\n",
    "        eos_id = tokenizer.token2id.get(\"<eos>\", None)\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"INTERACTIVE NUMBER COMMAND TRANSFORMER TESTING\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Enter prompts like:\")\n",
    "        print(\"  <sos> 55,50,14 <sum>\")\n",
    "        print(\"  <sos> 82,77,9,57 <reverse>\") \n",
    "        print(\"  <sos> 425,616,162 <ascending>\")\n",
    "        print(\"  <sos> 44,30,61 <even_repeat>\")\n",
    "        print(\"\\nType 'quit' to exit\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Make sure the model is trained and saved properly.\")\n",
    "        return\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"\\nEnter prompt: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"Exiting interactive testing...\")\n",
    "                break\n",
    "                \n",
    "            if not user_input:\n",
    "                print(\"Please enter a valid prompt.\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"Input: {user_input}\")\n",
    "            \n",
    "            # Encode the input\n",
    "            try:\n",
    "                input_tokens = tokenizer.encode(user_input)\n",
    "                input_tensor = torch.tensor(input_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "            except Exception as e:\n",
    "                print(f\"Error encoding input: {e}\")\n",
    "                print(\"Make sure your input follows the format: <sos> numbers <command>\")\n",
    "                continue\n",
    "            \n",
    "            # Generate completion\n",
    "            with torch.no_grad():\n",
    "                generated = model.generate(\n",
    "                    input_tensor, \n",
    "                    max_new_tokens=25, \n",
    "                    temperature=0.1,  # Low temperature for more deterministic output\n",
    "                    eos_token_id=eos_id\n",
    "                )\n",
    "            \n",
    "            # Decode and display result\n",
    "            generated_tokens = generated[0].tolist()\n",
    "            model_output = tokenizer.decode(generated_tokens)\n",
    "            \n",
    "            print(f\"Model output: {model_output}\")\n",
    "            \n",
    "            # Show some additional info\n",
    "            print(f\"Generated {len(generated_tokens) - len(input_tokens)} new tokens\")\n",
    "            \n",
    "            # Calculate expected result if possible\n",
    "            try:\n",
    "                expected = calculate_expected_completion(user_input)\n",
    "                if expected != \"Unknown command\" and expected != \"Invalid format\":\n",
    "                    print(f\"Expected: {expected}\")\n",
    "                    is_correct = model_output.strip() == expected.strip()\n",
    "                    print(f\"Matches expected: {'‚úÖ' if is_correct else '‚ùå'}\")\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "\n",
    "def calculate_expected_completion(prompt: str) -> str:\n",
    "    \"\"\"Calculate what the expected completion should be for a given prompt\"\"\"\n",
    "    \n",
    "    # Parse the prompt to extract numbers and command\n",
    "    parts = prompt.strip().split()\n",
    "    if len(parts) < 3 or parts[0] != '<sos>':\n",
    "        return \"Invalid format\"\n",
    "    \n",
    "    numbers_str = parts[1]\n",
    "    command = parts[2]\n",
    "    \n",
    "    # Convert comma-separated string to list of numbers\n",
    "    try:\n",
    "        numbers = [int(n) for n in numbers_str.split(',')]\n",
    "    except ValueError:\n",
    "        return \"Invalid format\"\n",
    "    \n",
    "    # Calculate result based on command\n",
    "    if command == '<sum>':\n",
    "        result = str(sum(numbers))\n",
    "    elif command == '<reverse>':\n",
    "        result = ','.join(map(str, numbers[::-1]))\n",
    "    elif command == '<ascending>':\n",
    "        result = ','.join(map(str, sorted(numbers)))\n",
    "    elif command == '<descending>':\n",
    "        result = ','.join(map(str, sorted(numbers, reverse=True)))\n",
    "    elif command == '<even_repeat>':\n",
    "        even_numbers = [num for num in numbers if num % 2 == 0]\n",
    "        if not even_numbers:\n",
    "            result = \"\"\n",
    "        else:\n",
    "            repeated_evens = []\n",
    "            for i in range(10):\n",
    "                repeated_evens.append(even_numbers[i % len(even_numbers)])\n",
    "            result = ','.join(map(str, repeated_evens))\n",
    "    elif command == '<odd_repeat>':\n",
    "        odd_numbers = [num for num in numbers if num % 2 == 1]\n",
    "        if not odd_numbers:\n",
    "            result = \"\"\n",
    "        else:\n",
    "            repeated_odds = []\n",
    "            for i in range(10):\n",
    "                repeated_odds.append(odd_numbers[i % len(odd_numbers)])\n",
    "            result = ','.join(map(str, repeated_odds))\n",
    "    else:\n",
    "        return \"Unknown command\"\n",
    "    \n",
    "    return f\"{prompt} {result} <eos>\"\n",
    "\n",
    "# Run interactive testing\n",
    "interactive_model_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d35e581-ed3a-4a63-8e5f-621ff7e228f0",
   "metadata": {
    "id": "8d35e581-ed3a-4a63-8e5f-621ff7e228f0"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_trained_model(checkpoint_path: str = 'out_number_commands/ckpt.pt'):\n",
    "    \"\"\"Load the trained model and tokenizer\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load metadata first\n",
    "        meta_path = 'data/number_commands/meta.pkl'\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Dataset metadata not found. Please run dataset generation first.\")\n",
    "\n",
    "    # Recreate tokenizer\n",
    "    class CustomTokenizer:\n",
    "        def __init__(self, token2id, id2token):\n",
    "            self.token2id = token2id\n",
    "            self.id2token = id2token\n",
    "            self.vocab_size = len(token2id)\n",
    "\n",
    "        def encode(self, text: str) -> List[int]:\n",
    "            tokens = []\n",
    "            i = 0\n",
    "            while i < len(text):\n",
    "                if text[i] == \" \":\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if text[i] == \"<\":\n",
    "                    j = text.find(\">\", i)\n",
    "                    if j != -1:\n",
    "                        tok = text[i:j+1]\n",
    "                        if tok in self.token2id:\n",
    "                            tokens.append(self.token2id[tok])\n",
    "                            i = j + 1\n",
    "                            continue\n",
    "\n",
    "                if text[i] == \",\":\n",
    "                    tokens.append(self.token2id[\",\"])\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if text[i].isdigit():\n",
    "                    j = i\n",
    "                    while j < len(text) and text[j].isdigit():\n",
    "                        j += 1\n",
    "                    number_str = text[i:j]\n",
    "                    number = int(number_str)\n",
    "\n",
    "                    if str(number) in self.token2id:\n",
    "                        tokens.append(self.token2id[str(number)])\n",
    "                        i = j\n",
    "                    else:\n",
    "                        raise ValueError(f\"Number out of range: {number}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected character: {text[i]}\")\n",
    "\n",
    "            return tokens\n",
    "\n",
    "        def decode(self, token_ids: List[int]) -> str:\n",
    "            tokens = [self.id2token[i] for i in token_ids]\n",
    "            result = \"\"\n",
    "\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "                    if result and not result.endswith(\" \"):\n",
    "                        result += \" \"\n",
    "                    result += token\n",
    "                    if i < len(tokens) - 1:\n",
    "                        result += \" \"\n",
    "                elif token == \",\":\n",
    "                    result += \",\"\n",
    "                else:\n",
    "                    result += token\n",
    "\n",
    "            return result\n",
    "\n",
    "    tokenizer = CustomTokenizer(meta['token2id'], meta['id2token'])\n",
    "\n",
    "    # Load model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}. Please train the model first.\")\n",
    "\n",
    "    model_args = checkpoint['model_args']\n",
    "\n",
    "    # Handle both old and new checkpoint formats\n",
    "    if isinstance(model_args, dict):\n",
    "        # New format - direct config dict\n",
    "        try:\n",
    "            config = NumberCommandConfig(**model_args)\n",
    "        except TypeError:\n",
    "            # Fallback for missing config class\n",
    "            print(\"Warning: Using fallback model configuration\")\n",
    "            config = type('Config', (), model_args)()\n",
    "    else:\n",
    "        # Old format - might be a config object\n",
    "        config = model_args\n",
    "\n",
    "    try:\n",
    "        model = NumberCommandTransformer(config)\n",
    "    except NameError:\n",
    "        raise NameError(\"Model classes not found. Please run the model definition cell first.\")\n",
    "\n",
    "    # Remove potential compilation prefix\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded from {checkpoint_path}\")\n",
    "    print(f\"Model has {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def test_model_completion(model, tokenizer, device, prompt: str, max_new_tokens: int = 15, temperature: float = 0.1):\n",
    "    # Encode prompt\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    # Get the actual EOS token ID\n",
    "    eos_id = tokenizer.token2id.get(\"<eos>\", None)\n",
    "\n",
    "    # Generate completion\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, \n",
    "                                 temperature=temperature, eos_token_id=eos_id)\n",
    "\n",
    "    # Decode result\n",
    "    generated_tokens = generated[0].tolist()\n",
    "    full_response = tokenizer.decode(generated_tokens)\n",
    "\n",
    "    return full_response, generated_tokens\n",
    "\n",
    "def calculate_expected_result(input_str: str) -> str:\n",
    "    \"\"\"Calculate what the expected output should be\"\"\"\n",
    "\n",
    "    # Parse the input\n",
    "    parts = input_str.strip().split()\n",
    "    if len(parts) < 3 or parts[0] != '<sos>':\n",
    "        return \"Invalid format\"\n",
    "\n",
    "    numbers_str = parts[1]\n",
    "    command = parts[2]\n",
    "\n",
    "    # Convert comma-separated string to list of numbers\n",
    "    try:\n",
    "        numbers = [int(n) for n in numbers_str.split(',')]\n",
    "    except ValueError:\n",
    "        return \"Invalid numbers\"\n",
    "\n",
    "    if command == '<sum>':\n",
    "        result = str(sum(numbers))\n",
    "    elif command == '<reverse>':\n",
    "        result = ','.join(map(str, numbers[::-1]))\n",
    "    elif command == '<ascending>':\n",
    "        result = ','.join(map(str, sorted(numbers)))\n",
    "    elif command == '<descending>':\n",
    "        result = ','.join(map(str, sorted(numbers, reverse=True)))\n",
    "    elif command == '<even_repeat>':\n",
    "        even_numbers = [num for num in numbers if num % 2 == 0]\n",
    "        if not even_numbers:\n",
    "            result = \"\"\n",
    "        else:\n",
    "            repeated_evens = []\n",
    "            for i in range(10):\n",
    "                repeated_evens.append(even_numbers[i % len(even_numbers)])\n",
    "            result = ','.join(map(str, repeated_evens))\n",
    "    elif command == '<odd_repeat>':\n",
    "        odd_numbers = [num for num in numbers if num % 2 == 1]\n",
    "        if not odd_numbers:\n",
    "            result = \"\"\n",
    "        else:\n",
    "            repeated_odds = []\n",
    "            for i in range(10):\n",
    "                repeated_odds.append(odd_numbers[i % len(odd_numbers)])\n",
    "            result = ','.join(map(str, repeated_odds))\n",
    "    else:\n",
    "        return \"Unknown command\"\n",
    "\n",
    "    return f\"<sos> {numbers_str} {command} {result} <eos>\"\n",
    "\n",
    "def run_comprehensive_test(model, tokenizer, device):\n",
    "    \"\"\"Run comprehensive tests on the model\"\"\"\n",
    "\n",
    "    test_cases = [\n",
    "        # Sum tests\n",
    "        \"<sos> 55,50,14 <sum>\",\n",
    "        \"<sos> 26,55,99 <sum>\",\n",
    "        \"<sos> 92,99,1 <sum>\",\n",
    "\n",
    "        # Reverse tests\n",
    "        \"<sos> 82,77,9,57 <reverse>\",\n",
    "        \"<sos> 190,676,440 <reverse>\",\n",
    "        \"<sos> 505,100 <reverse>\",\n",
    "\n",
    "        # Ascending sort tests\n",
    "        \"<sos> 425,616,162 <ascending>\",\n",
    "        \"<sos> 190,676,440 <ascending>\",\n",
    "        \"<sos> 5,231,44 <ascending>\",\n",
    "\n",
    "        # Descending sort tests\n",
    "        \"<sos> 425,616,162,221,244 <descending>\",\n",
    "        \"<sos> 917,35,88 <descending>\",\n",
    "        \"<sos> 48,2,99 <descending>\",\n",
    "\n",
    "        # Even repeat tests\n",
    "        \"<sos> 44,30,61 <even_repeat>\",\n",
    "        \"<sos> 12,37,84 <even_repeat>\",\n",
    "\n",
    "        # Odd repeat tests\n",
    "        \"<sos> 50,82,41 <odd_repeat>\",\n",
    "        \"<sos> 13,28,55,78 <odd_repeat>\",\n",
    "    ]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE MODEL TESTING\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\nTest {i+1}: {test_case}\")\n",
    "\n",
    "        # Get model prediction\n",
    "        prediction, tokens = test_model_completion(model, tokenizer, device, test_case)\n",
    "        print(f\"Model output: {prediction}\")\n",
    "\n",
    "        # Calculate expected result\n",
    "        expected = calculate_expected_result(test_case)\n",
    "        print(f\"Expected:     {expected}\")\n",
    "\n",
    "        # Check if correct\n",
    "        is_correct = prediction.strip() == expected.strip()\n",
    "        print(f\"Correct: {'YES' if is_correct else 'NO'}\")\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    if total > 0:\n",
    "        accuracy = correct / total * 100\n",
    "        print(f\"\\nOverall Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
    "\n",
    "    return correct, total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3006ce-2af5-41fb-b346-d352a80b34bc",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ab3006ce-2af5-41fb-b346-d352a80b34bc",
    "outputId": "75a5435e-434c-4f18-d0c3-1f19cdba71b8"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Load the trained model\n",
    "    model, tokenizer, device = load_trained_model()\n",
    "\n",
    "    # Run comprehensive tests\n",
    "    correct, total = run_comprehensive_test(model, tokenizer, device)\n",
    "\n",
    "    # Start interactive testing\n",
    "    #interactive_test(model, tokenizer, device)\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Could not find model files. Make sure you've trained the model first.\")\n",
    "    print(\"Expected files:\")\n",
    "    print(\"  - out_number_commands/ckpt.pt\")\n",
    "    print(\"  - data/number_commands/meta.pkl\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "AYhyre4kyM2I",
   "metadata": {
    "id": "AYhyre4kyM2I"
   },
   "source": [
    "# trying again to load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yUPZpZXKxYtA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yUPZpZXKxYtA",
    "outputId": "79804c43-877c-4916-a745-01f5d2c95bf5"
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "# Load model and tokenizer\n",
    "def load_trained_model(checkpoint_path: str = 'out_number_commands/ckpt.pt'):\n",
    "    \"\"\"Load the trained model and tokenizer\"\"\"\n",
    "\n",
    "    try:\n",
    "        # Load metadata first\n",
    "        meta_path = 'data/number_commands/meta.pkl'\n",
    "        with open(meta_path, 'rb') as f:\n",
    "            meta = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"Dataset metadata not found. Please run dataset generation first.\")\n",
    "\n",
    "    # Recreate tokenizer\n",
    "    class CustomTokenizer:\n",
    "        def __init__(self, token2id, id2token):\n",
    "            self.token2id = token2id\n",
    "            self.id2token = id2token\n",
    "            self.vocab_size = len(token2id)\n",
    "\n",
    "        def encode(self, text: str) -> List[int]:\n",
    "            tokens = []\n",
    "            i = 0\n",
    "            while i < len(text):\n",
    "                if text[i] == \" \":\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if text[i] == \"<\":\n",
    "                    j = text.find(\">\", i)\n",
    "                    if j != -1:\n",
    "                        tok = text[i:j+1]\n",
    "                        if tok in self.token2id:\n",
    "                            tokens.append(self.token2id[tok])\n",
    "                            i = j + 1\n",
    "                            continue\n",
    "\n",
    "                if text[i] == \",\":\n",
    "                    tokens.append(self.token2id[\",\"])\n",
    "                    i += 1\n",
    "                    continue\n",
    "\n",
    "                if text[i].isdigit():\n",
    "                    # Extract full number (could be 1-3 digits)\n",
    "                    j = i\n",
    "                    while j < len(text) and text[j].isdigit():\n",
    "                        j += 1\n",
    "                    number_str = text[i:j]\n",
    "                    number = int(number_str)\n",
    "\n",
    "                    if str(number) in self.token2id:\n",
    "                        tokens.append(self.token2id[str(number)])\n",
    "                        i = j\n",
    "                    else:\n",
    "                        raise ValueError(f\"Number out of range: {number}\")\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected character: {text[i]}\")\n",
    "\n",
    "            return tokens\n",
    "\n",
    "        def decode(self, token_ids: List[int]) -> str:\n",
    "            tokens = [self.id2token[i] for i in token_ids]\n",
    "            result = \"\"\n",
    "\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token.startswith(\"<\") and token.endswith(\">\"):\n",
    "                    # Add special token with spaces\n",
    "                    if result and not result.endswith(\" \"):\n",
    "                        result += \" \"\n",
    "                    result += token\n",
    "                    if i < len(tokens) - 1:  # not last token\n",
    "                        result += \" \"\n",
    "                elif token == \",\":\n",
    "                    result += \",\"\n",
    "                else:\n",
    "                    # Number token\n",
    "                    result += token\n",
    "\n",
    "            return result\n",
    "\n",
    "    tokenizer = CustomTokenizer(meta['token2id'], meta['id2token'])\n",
    "\n",
    "    # Load model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    try:\n",
    "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}. Please train the model first.\")\n",
    "\n",
    "    model_args = checkpoint['model_args']\n",
    "\n",
    "    # Handle both old and new checkpoint formats\n",
    "    if isinstance(model_args, dict):\n",
    "        # New format - direct config dict\n",
    "        try:\n",
    "            config = NumberCommandConfig(**model_args)\n",
    "        except TypeError:\n",
    "            # Fallback for missing config class\n",
    "            print(\"Warning: Using fallback model configuration\")\n",
    "            config = type('Config', (), model_args)()\n",
    "    else:\n",
    "        # Old format - might be a config object\n",
    "        config = model_args\n",
    "\n",
    "    try:\n",
    "        model = NumberCommandTransformer(config)\n",
    "    except NameError:\n",
    "        raise NameError(\"Model classes not found. Please run the model definition cell first.\")\n",
    "\n",
    "    # Remove potential compilation prefix\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"‚úì Model loaded from {checkpoint_path}\")\n",
    "    print(f\"‚úì Model has {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "    print(f\"‚úì Using device: {device}\")\n",
    "\n",
    "    return model, tokenizer, device\n",
    "\n",
    "def test_model_completion(model, tokenizer, device, prompt: str, max_new_tokens: int = 15, temperature: float = 0.1):\n",
    "    \"\"\"Test model completion given a prompt\"\"\"\n",
    "\n",
    "    # Encode prompt\n",
    "    prompt_tokens = tokenizer.encode(prompt)\n",
    "    prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
    "\n",
    "    # Generate completion\n",
    "    with torch.no_grad():\n",
    "        generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, temperature=temperature)\n",
    "\n",
    "    # Decode result\n",
    "    generated_tokens = generated[0].tolist()\n",
    "    full_response = tokenizer.decode(generated_tokens)\n",
    "\n",
    "    return full_response, generated_tokens\n",
    "\n",
    "def calculate_expected_result(input_str: str) -> str:\n",
    "    \"\"\"Calculate what the expected output should be\"\"\"\n",
    "\n",
    "    # Parse the input\n",
    "    parts = input_str.strip().split()\n",
    "    if len(parts) < 3 or parts[0] != '<sos>':\n",
    "        return \"Invalid format\"\n",
    "\n",
    "    numbers_str = parts[1]\n",
    "    command = parts[2]\n",
    "\n",
    "    # Convert comma-separated string to list of numbers\n",
    "    try:\n",
    "        numbers = [int(n) for n in numbers_str.split(',')]\n",
    "    except ValueError:\n",
    "        return \"Invalid numbers\"\n",
    "\n",
    "    if command == '<sum>':\n",
    "        result = str(sum(numbers))\n",
    "    elif command == '<reverse>':\n",
    "        result = ','.join(map(str, numbers[::-1]))\n",
    "    elif command == '<ascending>':\n",
    "        result = ','.join(map(str, sorted(numbers)))\n",
    "    elif command == '<descending>':\n",
    "        result = ','.join(map(str, sorted(numbers, reverse=True)))\n",
    "    elif command == '<even_repeat>':\n",
    "        even_numbers = [num for num in numbers if num % 2 == 0]\n",
    "        if not even_numbers:\n",
    "            result = \"\"\n",
    "        else:\n",
    "            repeated_evens = []\n",
    "            for i in range(10):\n",
    "                repeated_evens.append(even_numbers[i % len(even_numbers)])\n",
    "            result = ','.join(map(str, repeated_evens))\n",
    "    elif command == '<odd_repeat>':\n",
    "        odd_numbers = [num for num in numbers if num % 2 == 1]\n",
    "        if not odd_numbers:\n",
    "            result = \"\"\n",
    "        else:\n",
    "            repeated_odds = []\n",
    "            for i in range(10):\n",
    "                repeated_odds.append(odd_numbers[i % len(odd_numbers)])\n",
    "            result = ','.join(map(str, repeated_odds))\n",
    "    else:\n",
    "        return \"Unknown command\"\n",
    "\n",
    "    return f\"<sos> {numbers_str} {command} {result} <eos>\"\n",
    "\n",
    "def run_comprehensive_test(model, tokenizer, device):\n",
    "    \"\"\"Run comprehensive tests on the model\"\"\"\n",
    "\n",
    "    test_cases = [\n",
    "        # Sum tests\n",
    "        \"<sos> 55,50,14 <sum>\",\n",
    "        \"<sos> 123,456,789 <sum>\",\n",
    "        \"<sos> 92,99,1 <sum>\",\n",
    "\n",
    "        # Reverse tests\n",
    "        \"<sos> 82,77,9,57 <reverse>\",\n",
    "        \"<sos> 190,676,440 <reverse>\",\n",
    "        \"<sos> 505,100 <reverse>\",\n",
    "\n",
    "        # Ascending sort tests\n",
    "        \"<sos> 425,616,162 <ascending>\",\n",
    "        \"<sos> 190,676,440 <ascending>\",\n",
    "        \"<sos> 5,231,44 <ascending>\",\n",
    "\n",
    "        # Descending sort tests\n",
    "        \"<sos> 425,616,162,221,244 <descending>\",\n",
    "        \"<sos> 917,35,88 <descending>\",\n",
    "        \"<sos> 48,2,99 <descending>\",\n",
    "\n",
    "        # Even repeat tests\n",
    "        \"<sos> 44,30,61 <even_repeat>\",\n",
    "        \"<sos> 12,37,84 <even_repeat>\",\n",
    "\n",
    "        # Odd repeat tests\n",
    "        \"<sos> 50,82,41 <odd_repeat>\",\n",
    "        \"<sos> 13,28,55 <odd_repeat>\",\n",
    "    ]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE MODEL TESTING\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, test_case in enumerate(test_cases):\n",
    "        print(f\"\\nTest {i+1}: {test_case}\")\n",
    "\n",
    "        # Get model prediction\n",
    "        prediction, tokens = test_model_completion(model, tokenizer, device, test_case)\n",
    "        print(f\"Model output: {prediction}\")\n",
    "\n",
    "        # Calculate expected result\n",
    "        expected = calculate_expected_result(test_case)\n",
    "        print(f\"Expected:     {expected}\")\n",
    "\n",
    "        # Check if correct\n",
    "        is_correct = prediction.strip() == expected.strip()\n",
    "        print(f\"Correct: {'‚úì' if is_correct else '‚úó'}\")\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"\\nOverall Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
    "\n",
    "    return correct, total\n",
    "\n",
    "def test_with_generated_examples(model, tokenizer, device, num_tests=20):\n",
    "    \"\"\"Test model using your existing generator functions\"\"\"\n",
    "\n",
    "    # Import/call your existing functions\n",
    "    from your_module import (\n",
    "        generate_random_numbers_and_sum,\n",
    "        generate_even_repeat_sequence,\n",
    "        generate_odd_repeat_sequence,\n",
    "        generate_random_numbers_and_reverse,\n",
    "        generate_ascending_sort,\n",
    "        generate_descending_sort\n",
    "    )\n",
    "\n",
    "    generators = [\n",
    "        generate_random_numbers_and_sum,\n",
    "        generate_even_repeat_sequence,\n",
    "        generate_odd_repeat_sequence,\n",
    "        generate_random_numbers_and_reverse,\n",
    "        generate_ascending_sort,\n",
    "        generate_descending_sort\n",
    "    ]\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TESTING WITH GENERATED EXAMPLES\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i in range(num_tests):\n",
    "        # Pick random generator\n",
    "        generator = generators[i % len(generators)]\n",
    "\n",
    "        # Generate full example\n",
    "        full_example = generator()\n",
    "\n",
    "        # Split into prompt and expected\n",
    "        parts = full_example.split(' <eos>')\n",
    "        prompt_part = parts[0]  # Everything before <eos>\n",
    "\n",
    "        # Find where the command ends to create prompt\n",
    "        command_tokens = ['<sum>', '<reverse>', '<ascending>', '<descending>', '<even_repeat>', '<odd_repeat>']\n",
    "        prompt = None\n",
    "        expected = full_example\n",
    "\n",
    "        for cmd in command_tokens:\n",
    "            if cmd in prompt_part:\n",
    "                cmd_index = prompt_part.find(cmd)\n",
    "                prompt = prompt_part[:cmd_index + len(cmd)]\n",
    "                break\n",
    "\n",
    "        if prompt is None:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nTest {i+1}:\")\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "\n",
    "        # Get model prediction\n",
    "        prediction, tokens = test_model_completion(model, tokenizer, device, prompt, max_new_tokens=20)\n",
    "        print(f\"Model output: {prediction}\")\n",
    "        print(f\"Expected:     {expected}\")\n",
    "\n",
    "        # Check if correct\n",
    "        is_correct = prediction.strip() == expected.strip()\n",
    "        print(f\"Correct: {'‚úì' if is_correct else '‚úó'}\")\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "        total += 1\n",
    "\n",
    "    accuracy = correct / total * 100\n",
    "    print(f\"\\nGenerated Examples Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
    "\n",
    "    return correct, total\n",
    "\n",
    "def interactive_test(model, tokenizer, device):\n",
    "    \"\"\"Interactive testing interface\"\"\"\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERACTIVE TESTING\")\n",
    "    print(\"Enter prompts like: <sos> 55,50,14 <sum>\")\n",
    "    print(\"Type 'quit' to exit\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            prompt = input(\"\\nEnter prompt: \").strip()\n",
    "            if prompt.lower() == 'quit':\n",
    "                break\n",
    "\n",
    "            if not prompt:\n",
    "                continue\n",
    "\n",
    "            # Test the prompt\n",
    "            prediction, tokens = test_model_completion(model, tokenizer, device, prompt)\n",
    "            print(f\"Model output: {prediction}\")\n",
    "\n",
    "            # Show expected if possible\n",
    "            try:\n",
    "                expected = calculate_expected_result(prompt)\n",
    "                if \"Invalid\" not in expected and \"Unknown\" not in expected:\n",
    "                    print(f\"Expected:     {expected}\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nExiting...\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Main testing function\n",
    "def main():\n",
    "    \"\"\"Run all tests\"\"\"\n",
    "    try:\n",
    "        # Load model\n",
    "        model, tokenizer, device = load_trained_model()\n",
    "\n",
    "        # Run comprehensive tests\n",
    "        run_comprehensive_test(model, tokenizer, device)\n",
    "\n",
    "        # Uncomment to test with generated examples (requires importing your functions)\n",
    "        # test_with_generated_examples(model, tokenizer, device)\n",
    "\n",
    "        # Run interactive test\n",
    "        interactive_test(model, tokenizer, device)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during testing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cBC8I5plzu24",
   "metadata": {
    "id": "cBC8I5plzu24"
   },
   "outputs": [],
   "source": [
    "\n",
    "#minimal testing\n",
    "\n",
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_on_validation_set():\n",
    "    \"\"\"Evaluate model on validation set\"\"\"\n",
    "\n",
    "    # Load metadata\n",
    "    data_dir = 'data/number_commands'\n",
    "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "\n",
    "    # Load model\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    checkpoint_path = 'out_number_commands/ckpt.pt'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "\n",
    "    model_args = checkpoint['model_args']\n",
    "\n",
    "    # Filter to only NumberCommandConfig parameters\n",
    "    config_params = {\n",
    "        'block_size': model_args['block_size'],\n",
    "        'vocab_size': model_args['vocab_size'],\n",
    "        'n_layer': model_args['n_layer'],\n",
    "        'n_head': model_args['n_head'],\n",
    "        'n_embd': model_args['n_embd'],\n",
    "        'dropout': model_args['dropout'],\n",
    "        'bias': model_args['bias']\n",
    "    }\n",
    "\n",
    "    config = NumberCommandConfig(**config_params)\n",
    "    model = NumberCommandTransformer(config)\n",
    "\n",
    "    # Remove compilation prefix if exists\n",
    "    state_dict = checkpoint['model']\n",
    "    unwanted_prefix = '_orig_mod.'\n",
    "    for k,v in list(state_dict.items()):\n",
    "        if k.startswith(unwanted_prefix):\n",
    "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Load validation data\n",
    "    val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "    block_size = config.block_size\n",
    "\n",
    "    print(f\"Model loaded: {model.get_num_params()/1e6:.2f}M parameters\")\n",
    "    print(f\"Validation data size: {len(val_data)} tokens\")\n",
    "\n",
    "    # Evaluate loss on validation set\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    batch_size = 32\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(val_data) - block_size, batch_size * block_size):\n",
    "            # Get batch\n",
    "            batch_end = min(i + batch_size * block_size, len(val_data) - block_size)\n",
    "            batch_indices = range(i, batch_end, block_size)\n",
    "\n",
    "            if len(batch_indices) == 0:\n",
    "                break\n",
    "\n",
    "            x = torch.stack([\n",
    "                torch.from_numpy(val_data[j:j+block_size].astype(np.int64))\n",
    "                for j in batch_indices\n",
    "            ])\n",
    "            y = torch.stack([\n",
    "                torch.from_numpy(val_data[j+1:j+1+block_size].astype(np.int64))\n",
    "                for j in batch_indices\n",
    "            ])\n",
    "\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits, loss = model(x, y)\n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "\n",
    "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Perplexity: {perplexity:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MIAEmAmxzxqQ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MIAEmAmxzxqQ",
    "outputId": "8175469a-b201-4f2b-be25-47cbf3e61c4d"
   },
   "outputs": [],
   "source": [
    "\n",
    "evaluate_on_validation_set()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
