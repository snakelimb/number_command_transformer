{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oQ4wfYr24d-x"
      },
      "id": "oQ4wfYr24d-x"
    },
    {
      "cell_type": "markdown",
      "id": "cb41ff5d-9d85-4c94-9437-83b674b9425b",
      "metadata": {
        "id": "cb41ff5d-9d85-4c94-9437-83b674b9425b"
      },
      "source": [
        "\n",
        "# Exploration of a \"number command\" transformer.\n",
        "\n",
        "### Depending on which command token is after a sequence of digit's, the transformer is asked to perform different tasks.\n",
        "\n",
        "\n",
        "\n",
        "The idea is to test if initially the transformer can perform its task given near full demonstations. If given examples of these multiple tasks, can the varied task tokens properly signal the transformer to change it's behaviour?\n",
        "I hope to also explore by how many parameter can the model be reduce before its performance completely degrades.\n",
        "\n",
        "I tried to vary the sequence length of the inputs on some functions. This results in some functions having shorter sequences. This was intentional to test what will happen when we prompt it with sequences that are 7 numbers long.\n",
        "To challenge the Transformer I might try to se what it does if the training data contains gaps in sequence length. For example, if one task only has examples of sequences of length 1,3,5 and another has 2, 4,7, etc\n",
        "\n",
        "\n",
        "What happens when two commands are entered in succession? Will it do both?\n",
        "\n",
        "Which commands conflict with each other?\n",
        "\n",
        "example sequences:\n",
        "\n",
        "```\n",
        "<sos> 55,50,14,9,10,36,56,53,47,21 <sum> 351 <eos>\n",
        "<sos> 44,30,61 <even_repeat> 44,30,44,30,44,30,44,30,44,30 <eos>\n",
        "<sos> 50,82,41 <odd_repeat> 41,41,41,41,41,41,41,41,41,41 <eos>\n",
        "<sos> 82,77,9,57,84,5,42,86 <reverse> 86,42,5,84,57,9,77,82 <eos>\n",
        "<sos> 425,616,162,221,244 <descending> 616,425,244,221,162 <eos>\n",
        "<sos> 190,676,440 <ascending> 190,440,676 <eos>\n",
        "```\n",
        "\n",
        "## Functions are bounded to produce combinations that do not exceed 999 in the training data.\n",
        "\n",
        "\n",
        "The transformer model used is based on Adrej Karpathy's NanoGPT\n",
        "https://github.com/karpathy/nanoGPT/tree/master\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Notebook by Justin Thomas August 2025\n",
        "\n",
        "[Current Status] it trains but I'm having an issue with testing/inference. This is 2.0 version of this notebook. The 1.0 version trained properly-ish, using a number range of 1-9.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GOALS:\n",
        "\n"
      ],
      "metadata": {
        "id": "7uq5gACf9xGx"
      },
      "id": "7uq5gACf9xGx"
    },
    {
      "cell_type": "markdown",
      "id": "ecdf8134-f304-4ee2-928f-b41681a9a55c",
      "metadata": {
        "id": "ecdf8134-f304-4ee2-928f-b41681a9a55c"
      },
      "source": [
        "Ideas for improvements:\n",
        "\n",
        "done- increase the range of numbers to double and triple digit numbers.\n",
        "\n",
        "done - add commas to seperate the values.\n",
        "\n",
        "done - wider range of sequence lengths\n",
        "\n",
        "done- ensure there are no duplicate examples, and that the test set should include examples that do not occur in the training data.\n",
        "\n",
        "## extra task ideas\n",
        "\n",
        "Might need to systematically create examples rather than randomly. such as iterating through each possibility. Ehh but once I add in the apropriate complexity, adding every combination should be infeasible\n",
        "\n",
        "Could have tasks were the answer should be seperated by commas.\n",
        "\n",
        "\n",
        "\n",
        "operate on floating point numbers, using a special token for how many decimal places to include.\n",
        "\n",
        "## Tricky ideas for testing generalization\n",
        "\n",
        "have input sequence of 1-5, then try a 6th input digit.\n",
        "try have input sequence of 1-5, and 7. test the output on a six digit sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71f601d3-983b-4a90-a1bc-420436555afc",
      "metadata": {
        "id": "71f601d3-983b-4a90-a1bc-420436555afc"
      },
      "source": [
        "# vocab expansion - up to 999 for outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b215341-6940-4b6a-9436-208cd2159528",
      "metadata": {
        "id": "5b215341-6940-4b6a-9436-208cd2159528"
      },
      "source": [
        "\n",
        "I would like you to refactor this function into one that can take parameters. I want to be able to generate a dataset with\n",
        "\n",
        "\n",
        "generate a sequence of 3-10 digits, which can vary between 0 and 99 in value, and output in this format:\n",
        "\n",
        "<sos> (digits seperated by commas) <sum> total <eos>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Todo\n",
        "\n",
        "Fix the testing script.\n",
        "\n",
        "Ensure the EOS token is handled correctly by the tokenizer\n",
        "\n",
        "Add table of\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "enA4phy20MyQ"
      },
      "id": "enA4phy20MyQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPT suggestion:\n",
        "\n",
        "Got it üëç Thanks for pasting everything ‚Äî I see exactly what‚Äôs happening.\n",
        "\n",
        "Your error:\n",
        "\n",
        "```\n",
        "Error during testing: can't assign a NoneType to a torch.cuda.LongTensor\n",
        "```\n",
        "\n",
        "comes from this line in your `generate` method:\n",
        "\n",
        "```python\n",
        "idx_next[finished.unsqueeze(1)] = eos_token_id\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Why it happens\n",
        "\n",
        "* You only pass `eos_token_id=None` into `generate` (since you didn‚Äôt specify it when calling).\n",
        "* Then in the code, it still tries to do\n",
        "\n",
        "  ```python\n",
        "  idx_next[finished.unsqueeze(1)] = eos_token_id\n",
        "  ```\n",
        "\n",
        "  even when `eos_token_id` is `None`.\n",
        "* PyTorch won‚Äôt let you assign `None` into a tensor, hence the crash.\n",
        "\n",
        "---\n",
        "\n",
        "### Fix\n",
        "\n",
        "We just need to make sure we **only assign EOS tokens when one is actually provided**. Modify your `generate` method like this:\n",
        "\n",
        "```python\n",
        "@torch.no_grad()\n",
        "def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
        "    batch_size = idx.size(0)\n",
        "    finished = torch.zeros(batch_size, dtype=torch.bool, device=idx.device)\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "        logits, _ = self(idx_cond)\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        # Append token\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        if eos_token_id is not None:\n",
        "            # Update finished mask\n",
        "            finished = finished | (idx_next.squeeze(1) == eos_token_id)\n",
        "            # If all sequences finished, break early\n",
        "            if finished.all():\n",
        "                break\n",
        "            # Replace tokens in finished sequences with eos_token_id\n",
        "            idx_next[finished.unsqueeze(1)] = eos_token_id\n",
        "\n",
        "    return idx\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### Extra note\n",
        "\n",
        "Since your dataset clearly has `<eos>`, you should probably **tell the model what token ID that is**.\n",
        "Right now you‚Äôre not giving `eos_token_id` when calling:\n",
        "\n",
        "```python\n",
        "generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "```\n",
        "\n",
        "Change it to:\n",
        "\n",
        "```python\n",
        "eos_id = tokenizer.token2id.get(\"<eos>\", None)\n",
        "generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, temperature=temperature, eos_token_id=eos_id)\n",
        "```\n",
        "\n",
        "That way the model stops at `<eos>` properly.\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ So the **two-part fix** is:\n",
        "\n",
        "1. Patch `generate` to only assign `eos_token_id` if it‚Äôs not `None`.\n",
        "2. Pass the actual `<eos>` ID from your tokenizer when you call `generate`.\n",
        "\n",
        "---\n",
        "\n",
        "Do you want me to paste your **entire corrected code file** with those fixes in place (so you can drop it in), or just the minimal diffs?\n"
      ],
      "metadata": {
        "id": "T6HQT3t00svI"
      },
      "id": "T6HQT3t00svI"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ux1-1mxe0rzg"
      },
      "id": "ux1-1mxe0rzg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83e3fbb6-ba20-423e-8709-4522a37a4d65",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83e3fbb6-ba20-423e-8709-4522a37a4d65",
        "outputId": "1c23ec23-c3df-46c7-95f6-b568b2be9222"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> 7,4,7,45,68,51 <sum> 182 <eos>\n",
            "<sos> 24,32,3,48,20,75 <sum> 202 <eos>\n",
            "<sos> 80,14,59,50,21,96,22,55 <sum> 397 <eos>\n",
            "<sos> 47,42,5,43,94,5,64 <sum> 300 <eos>\n",
            "<sos> 21,8,98,5,66,33,28,54,1 <sum> 314 <eos>\n",
            "<sos> 50,20,74,89,81,74,14,56,61,34 <sum> 553 <eos>\n",
            "<sos> 64,17,10 <sum> 91 <eos>\n",
            "<sos> 6,33,0 <sum> 39 <eos>\n",
            "<sos> 0,2,35,22,88 <sum> 147 <eos>\n",
            "<sos> 4,59,3,53,12,17 <sum> 148 <eos>\n",
            "<sos> 63,74,15 <sum> 152 <eos>\n",
            "<sos> 32,55,14,43 <sum> 144 <eos>\n",
            "<sos> 56,63,34,69,32 <sum> 254 <eos>\n",
            "<sos> 81,48,36,87,18 <sum> 270 <eos>\n",
            "<sos> 8,0,1,66,13,64,0,54,46,46 <sum> 298 <eos>\n",
            "<sos> 61,53,73,99,21,45,12,95 <sum> 459 <eos>\n",
            "<sos> 39,16,35,38,26 <sum> 154 <eos>\n",
            "<sos> 92,21,93,54,67,69 <sum> 396 <eos>\n",
            "<sos> 77,30,69,57,37,65,30,49 <sum> 414 <eos>\n",
            "<sos> 12,91,52,59,85,52,61,95,15 <sum> 522 <eos>\n",
            "<sos> 7,69,13,63,6,54,21,46 <sum> 279 <eos>\n",
            "<sos> 22,35,33,32 <sum> 122 <eos>\n",
            "<sos> 49,77,17,87,22,67,33,97,78,13 <sum> 540 <eos>\n",
            "<sos> 1,13,71,41 <sum> 126 <eos>\n",
            "<sos> 27,19,21,65,92,11 <sum> 235 <eos>\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "#Revised Version\n",
        "\n",
        "def generate_random_numbers_and_sum():\n",
        "    \"\"\"\n",
        "    Generate 3‚Äì10 random digits, each between 0 and 99, and output:\n",
        "    <sos> (digits separated by commas) <sum> total <eos>\n",
        "    \"\"\"\n",
        "    # Pick a random length between 3 and 10\n",
        "    length = random.randint(3, 10)\n",
        "\n",
        "    # Generate that many random digits, each between 0 and 99\n",
        "    numbers = [random.randint(0, 99) for _ in range(length)]\n",
        "\n",
        "    # Calculate the sum of the numbers\n",
        "    total_sum = sum(numbers)\n",
        "\n",
        "    # Join the numbers into a single string separated by commas\n",
        "    numbers_str = ','.join(map(str, numbers))\n",
        "\n",
        "    # Format the output\n",
        "    output = f\"<sos> {numbers_str} <sum> {total_sum} <eos>\"\n",
        "    return output\n",
        "\n",
        "for _ in range(25):\n",
        "    print(generate_random_numbers_and_sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c64e2ce4-6214-440f-ac6c-c4bd7f4d2ee8",
      "metadata": {
        "id": "c64e2ce4-6214-440f-ac6c-c4bd7f4d2ee8"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e21ebe7-b0ca-4b5a-9583-ee38857d4201",
      "metadata": {
        "id": "6e21ebe7-b0ca-4b5a-9583-ee38857d4201"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "wRwr4qt0u144"
      },
      "id": "wRwr4qt0u144"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4af0b637-dcaf-416a-aa97-96b8baeb91e4",
      "metadata": {
        "id": "4af0b637-dcaf-416a-aa97-96b8baeb91e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Funtions defined\n",
        "\n",
        "generate_random_numbers_and_sum()\n",
        "\n",
        "generate_even_repeat_sequence()\n",
        "\n",
        "generate_odd_repeat_sequence()\n",
        "\n",
        "generate_random_numbers_and_reverse()\n",
        "\n",
        "generate_ascending_sort()\n",
        "\n",
        "generate_descending_sort()\n",
        "\n",
        "\n",
        "#Functions are bounded to produce combinations that do not exceed 999.\n",
        "\n",
        "# examples:\n",
        "\n",
        "```\n",
        "\n",
        "<sos> 55,50,14,9,10,36,56,53,47,21 <sum> 351 <eos>\n",
        "<sos> 44,30,61 <even_repeat> 44,30,44,30,44,30,44,30,44,30 <eos>\n",
        "<sos> 50,82,41 <odd_repeat> 41,41,41,41,41,41,41,41,41,41 <eos>\n",
        "<sos> 82,77,9,57,84,5,42,86 <reverse> 86,42,5,84,57,9,77,82 <eos>\n",
        "<sos> 425,616,162,221,244 <descending> 616,425,244,221,162 <eos>\n",
        "<sos> 190,676,440 <ascending> 190,440,676 <eos>\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "7xZjbK8Yu5c4"
      },
      "id": "7xZjbK8Yu5c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e5d3dfb0-165a-4ac9-8c61-1b6cbfdb80c7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5d3dfb0-165a-4ac9-8c61-1b6cbfdb80c7",
        "outputId": "61e266d6-51a8-499d-c926-244ee606af32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Even repeat examples:\n",
            "<sos> 10,36,3 <even_repeat> 10,36,10,36,10,36,10,36,10,36 <eos>\n",
            "<sos> 1,47,48 <even_repeat> 48,48,48,48,48,48,48,48,48,48 <eos>\n",
            "<sos> 71,98,72 <even_repeat> 98,72,98,72,98,72,98,72,98,72 <eos>\n",
            "<sos> 40,61,13 <even_repeat> 40,40,40,40,40,40,40,40,40,40 <eos>\n",
            "<sos> 68,60,0 <even_repeat> 68,60,0,68,60,0,68,60,0,68 <eos>\n",
            "<sos> 59,4,77 <even_repeat> 4,4,4,4,4,4,4,4,4,4 <eos>\n",
            "<sos> 5,73,91,0,71 <even_repeat> 0,0,0,0,0,0,0,0,0,0 <eos>\n",
            "<sos> 12,48,34,29 <even_repeat> 12,48,34,12,48,34,12,48,34,12 <eos>\n",
            "<sos> 25,12,20 <even_repeat> 12,20,12,20,12,20,12,20,12,20 <eos>\n",
            "<sos> 33,57,1,36 <even_repeat> 36,36,36,36,36,36,36,36,36,36 <eos>\n",
            "<sos> 66,34,7,82,9 <even_repeat> 66,34,82,66,34,82,66,34,82,66 <eos>\n",
            "<sos> 58,2,42,66 <even_repeat> 58,2,42,66,58,2,42,66,58,2 <eos>\n",
            "<sos> 59,23,47 <even_repeat>  <eos>\n",
            "<sos> 51,77,43 <even_repeat>  <eos>\n",
            "<sos> 62,76,11,7,92 <even_repeat> 62,76,92,62,76,92,62,76,92,62 <eos>\n",
            "<sos> 9,11,96,24 <even_repeat> 96,24,96,24,96,24,96,24,96,24 <eos>\n",
            "<sos> 99,16,10,78,77 <even_repeat> 16,10,78,16,10,78,16,10,78,16 <eos>\n",
            "<sos> 35,33,98 <even_repeat> 98,98,98,98,98,98,98,98,98,98 <eos>\n",
            "<sos> 35,88,38 <even_repeat> 88,38,88,38,88,38,88,38,88,38 <eos>\n",
            "<sos> 93,0,39,36,9 <even_repeat> 0,36,0,36,0,36,0,36,0,36 <eos>\n",
            "<sos> 79,68,88 <even_repeat> 68,88,68,88,68,88,68,88,68,88 <eos>\n",
            "<sos> 24,81,99,88 <even_repeat> 24,88,24,88,24,88,24,88,24,88 <eos>\n",
            "<sos> 3,63,24 <even_repeat> 24,24,24,24,24,24,24,24,24,24 <eos>\n",
            "<sos> 86,75,35 <even_repeat> 86,86,86,86,86,86,86,86,86,86 <eos>\n",
            "<sos> 83,31,84,72,70 <even_repeat> 84,72,70,84,72,70,84,72,70,84 <eos>\n",
            "\n",
            "Odd repeat examples:\n",
            "<sos> 92,3,49,85 <odd_repeat> 3,49,85,3,49,85,3,49,85,3 <eos>\n",
            "<sos> 87,34,48 <odd_repeat> 87,87,87,87,87,87,87,87,87,87 <eos>\n",
            "<sos> 64,90,21 <odd_repeat> 21,21,21,21,21,21,21,21,21,21 <eos>\n",
            "<sos> 7,98,74 <odd_repeat> 7,7,7,7,7,7,7,7,7,7 <eos>\n",
            "<sos> 0,72,7,34 <odd_repeat> 7,7,7,7,7,7,7,7,7,7 <eos>\n",
            "<sos> 1,4,19,22 <odd_repeat> 1,19,1,19,1,19,1,19,1,19 <eos>\n",
            "<sos> 62,35,97,47 <odd_repeat> 35,97,47,35,97,47,35,97,47,35 <eos>\n",
            "<sos> 74,66,21 <odd_repeat> 21,21,21,21,21,21,21,21,21,21 <eos>\n",
            "<sos> 36,5,50,88,45 <odd_repeat> 5,45,5,45,5,45,5,45,5,45 <eos>\n",
            "<sos> 37,8,50,88,34 <odd_repeat> 37,37,37,37,37,37,37,37,37,37 <eos>\n",
            "<sos> 56,70,74,67 <odd_repeat> 67,67,67,67,67,67,67,67,67,67 <eos>\n",
            "<sos> 74,19,10 <odd_repeat> 19,19,19,19,19,19,19,19,19,19 <eos>\n",
            "<sos> 5,12,21 <odd_repeat> 5,21,5,21,5,21,5,21,5,21 <eos>\n",
            "<sos> 72,59,38,28,2 <odd_repeat> 59,59,59,59,59,59,59,59,59,59 <eos>\n",
            "<sos> 7,49,54,24,60 <odd_repeat> 7,49,7,49,7,49,7,49,7,49 <eos>\n",
            "<sos> 9,66,59,87,26 <odd_repeat> 9,59,87,9,59,87,9,59,87,9 <eos>\n",
            "<sos> 79,20,6,71,40 <odd_repeat> 79,71,79,71,79,71,79,71,79,71 <eos>\n",
            "<sos> 95,99,75,68 <odd_repeat> 95,99,75,95,99,75,95,99,75,95 <eos>\n",
            "<sos> 11,3,46,56 <odd_repeat> 11,3,11,3,11,3,11,3,11,3 <eos>\n",
            "<sos> 33,73,6,2,12 <odd_repeat> 33,73,33,73,33,73,33,73,33,73 <eos>\n",
            "<sos> 91,5,98,91 <odd_repeat> 91,5,91,91,5,91,91,5,91,91 <eos>\n",
            "<sos> 7,89,28 <odd_repeat> 7,89,7,89,7,89,7,89,7,89 <eos>\n",
            "<sos> 29,71,74,34,47 <odd_repeat> 29,71,47,29,71,47,29,71,47,29 <eos>\n",
            "<sos> 13,34,64 <odd_repeat> 13,13,13,13,13,13,13,13,13,13 <eos>\n",
            "<sos> 20,86,15,83,98 <odd_repeat> 15,83,15,83,15,83,15,83,15,83 <eos>\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "def generate_even_repeat_sequence():\n",
        "    \"\"\"\n",
        "    Generate a sequence like:\n",
        "    <sos> 12,37,84,96 <even_repeat> 12,84,96,12,84,96,12,84,96,12 <eos>\n",
        "    - First part is random digits 0-99 with commas (length 3-5)\n",
        "    - Second part repeats only even numbers up to 10 total numbers\n",
        "    \"\"\"\n",
        "    # Generate 3-5 random numbers from 0-99\n",
        "    numbers = [random.randint(0, 99) for _ in range(random.randint(3, 5))]\n",
        "    numbers_str = ','.join(map(str, numbers))\n",
        "\n",
        "    # Filter for even numbers\n",
        "    even_numbers = [num for num in numbers if num % 2 == 0]\n",
        "\n",
        "    # If no even numbers, return sequence with empty repeat section\n",
        "    if not even_numbers:\n",
        "        output = f\"<sos> {numbers_str} <even_repeat>  <eos>\"\n",
        "        return output\n",
        "\n",
        "    # Repeat even numbers to fill up to 10 positions\n",
        "    repeated_evens = []\n",
        "    for i in range(10):\n",
        "        repeated_evens.append(even_numbers[i % len(even_numbers)])\n",
        "\n",
        "    repeated_str = ','.join(map(str, repeated_evens))\n",
        "\n",
        "    # Format the output\n",
        "    output = f\"<sos> {numbers_str} <even_repeat> {repeated_str} <eos>\"\n",
        "    return output\n",
        "\n",
        "def generate_odd_repeat_sequence():\n",
        "    \"\"\"\n",
        "    Generate a sequence like:\n",
        "    <sos> 12,37,84,96 <odd_repeat> 37,37,37,37,37,37,37,37,37,37 <eos>\n",
        "    - First part is random digits 0-99 with commas (length 3-5)\n",
        "    - Second part repeats only odd numbers up to 10 total numbers\n",
        "    \"\"\"\n",
        "    # Generate 3-5 random numbers from 0-99\n",
        "    numbers = [random.randint(0, 99) for _ in range(random.randint(3, 5))]\n",
        "    numbers_str = ','.join(map(str, numbers))\n",
        "\n",
        "    # Filter for odd numbers\n",
        "    odd_numbers = [num for num in numbers if num % 2 == 1]\n",
        "\n",
        "    # If no odd numbers, return sequence with empty repeat section\n",
        "    if not odd_numbers:\n",
        "        output = f\"<sos> {numbers_str} <odd_repeat>  <eos>\"\n",
        "        return output\n",
        "\n",
        "    # Repeat odd numbers to fill up to 10 positions\n",
        "    repeated_odds = []\n",
        "    for i in range(10):\n",
        "        repeated_odds.append(odd_numbers[i % len(odd_numbers)])\n",
        "\n",
        "    repeated_str = ','.join(map(str, repeated_odds))\n",
        "\n",
        "    # Format the output\n",
        "    output = f\"<sos> {numbers_str} <odd_repeat> {repeated_str} <eos>\"\n",
        "    return output\n",
        "\n",
        "# Example usage\n",
        "print(\"Even repeat examples:\")\n",
        "for _ in range(25):\n",
        "    print(generate_even_repeat_sequence())\n",
        "\n",
        "print(\"\\nOdd repeat examples:\")\n",
        "for _ in range(25):\n",
        "    print(generate_odd_repeat_sequence())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a3ed4b-a3ea-412b-af6a-2e0a9932602c",
      "metadata": {
        "id": "e6a3ed4b-a3ea-412b-af6a-2e0a9932602c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4671f1f-2957-45f1-98c0-8b5789609801",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4671f1f-2957-45f1-98c0-8b5789609801",
        "outputId": "8f10ba4e-224f-4797-c2c7-fca9d58a35e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> 28,44,84,3,77,84,26 <reverse> 26,84,77,3,84,44,28 <eos>\n",
            "<sos> 87,21,0 <reverse> 0,21,87 <eos>\n",
            "<sos> 27,17,8,54,20,84 <reverse> 84,20,54,8,17,27 <eos>\n",
            "<sos> 94,95,63,93,48,33,25,94 <reverse> 94,25,33,48,93,63,95,94 <eos>\n",
            "<sos> 88,12,87,39,44,15,78,72 <reverse> 72,78,15,44,39,87,12,88 <eos>\n",
            "<sos> 96,36,11 <reverse> 11,36,96 <eos>\n",
            "<sos> 96,53,42,25,57,99 <reverse> 99,57,25,42,53,96 <eos>\n",
            "<sos> 50,33,23,2,35,28,57 <reverse> 57,28,35,2,23,33,50 <eos>\n",
            "<sos> 58,55,18,48,30 <reverse> 30,48,18,55,58 <eos>\n",
            "<sos> 65,95,36,65,97,83,7 <reverse> 7,83,97,65,36,95,65 <eos>\n",
            "<sos> 4,71,95,33,23,56,62,78 <reverse> 78,62,56,23,33,95,71,4 <eos>\n",
            "<sos> 3,92,70,41,45 <reverse> 45,41,70,92,3 <eos>\n",
            "<sos> 72,95,25,31,10,65 <reverse> 65,10,31,25,95,72 <eos>\n",
            "<sos> 72,85,86,52,81,56,62 <reverse> 62,56,81,52,86,85,72 <eos>\n",
            "<sos> 9,22,90 <reverse> 90,22,9 <eos>\n",
            "<sos> 14,23,90,41,96,79,28 <reverse> 28,79,96,41,90,23,14 <eos>\n",
            "<sos> 85,77,30,45,54 <reverse> 54,45,30,77,85 <eos>\n",
            "<sos> 59,83,2,49,11 <reverse> 11,49,2,83,59 <eos>\n",
            "<sos> 23,23,8 <reverse> 8,23,23 <eos>\n",
            "<sos> 64,76,47 <reverse> 47,76,64 <eos>\n",
            "<sos> 47,36,42,86,53 <reverse> 53,86,42,36,47 <eos>\n",
            "<sos> 73,0,5,65,93,48,41,98 <reverse> 98,41,48,93,65,5,0,73 <eos>\n",
            "<sos> 53,46,88,89,4,53,85 <reverse> 85,53,4,89,88,46,53 <eos>\n",
            "<sos> 71,22,66,49,13,74 <reverse> 74,13,49,66,22,71 <eos>\n",
            "<sos> 82,71,97,48 <reverse> 48,97,71,82 <eos>\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "#Revised\n",
        "\n",
        "def generate_random_numbers_and_reverse():\n",
        "    \"\"\"\n",
        "    Generate a sequence like:\n",
        "    <sos> 12,37,84,96 <reverse> 96,84,37,12 <eos>\n",
        "    - First part is random numbers 0-99 with commas (length 3-5)\n",
        "    - Second part reverses the order of the numbers\n",
        "    \"\"\"\n",
        "    # Pick a random length between 3 and 8\n",
        "    length = random.randint(3, 8)\n",
        "\n",
        "    # Generate that many random numbers from 0-99\n",
        "    numbers = [random.randint(0, 99) for _ in range(length)]\n",
        "\n",
        "    # Join the numbers into a comma-separated string\n",
        "    numbers_str = ','.join(map(str, numbers))\n",
        "\n",
        "    # Reverse the list and join\n",
        "    reversed_str = ','.join(map(str, numbers[::-1]))\n",
        "\n",
        "    # Format the output\n",
        "    output = f\"<sos> {numbers_str} <reverse> {reversed_str} <eos>\"\n",
        "    return output\n",
        "\n",
        "# Example usage\n",
        "for _ in range(25):\n",
        "    print(generate_random_numbers_and_reverse())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "751eb4b9-aa9a-4574-86bc-e65e35d6664c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "751eb4b9-aa9a-4574-86bc-e65e35d6664c",
        "outputId": "29117e35-b588-4edd-9b05-b9a5253b5109"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<sos> 326,838,913 <ascending> 326,838,913 <eos>\n",
            "<sos> 814,568,557,768 <ascending> 557,568,768,814 <eos>\n",
            "<sos> 256,96,612,229 <ascending> 96,229,256,612 <eos>\n",
            "<sos> 421,138,432,367 <ascending> 138,367,421,432 <eos>\n",
            "<sos> 386,507,370 <ascending> 370,386,507 <eos>\n",
            "<sos> 764,321,598 <descending> 764,598,321 <eos>\n",
            "<sos> 406,442,282 <descending> 442,406,282 <eos>\n",
            "<sos> 125,319,224 <descending> 319,224,125 <eos>\n",
            "<sos> 845,96,629,128 <descending> 845,629,128,96 <eos>\n",
            "<sos> 608,736,898,198,357 <descending> 898,736,608,357,198 <eos>\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import random\n",
        "\n",
        "def generate_ascending_sort():\n",
        "    \"\"\"\n",
        "    Generate 3‚Äì5 random numbers (0-999) and output:\n",
        "    <sos> numbers <ascending> sorted_numbers <eos>\n",
        "    \"\"\"\n",
        "    # Pick a random length between 3 and 5\n",
        "    length = random.randint(3, 5)\n",
        "\n",
        "    # Generate that many random numbers from 0-999\n",
        "    numbers = [random.randint(0, 999) for _ in range(length)]\n",
        "    numbers_str = ','.join(map(str, numbers))\n",
        "\n",
        "    # Sort ascending\n",
        "    sorted_numbers = sorted(numbers)\n",
        "    sorted_str = ','.join(map(str, sorted_numbers))\n",
        "\n",
        "    # Format the output\n",
        "    output = f\"<sos> {numbers_str} <ascending> {sorted_str} <eos>\"\n",
        "    return output\n",
        "\n",
        "def generate_descending_sort():\n",
        "    \"\"\"\n",
        "    Generate 3‚Äì5 random numbers (0-999) and output:\n",
        "    <sos> numbers <descending> sorted_numbers <eos>\n",
        "    \"\"\"\n",
        "    # Pick a random length between 3 and 5\n",
        "    length = random.randint(3, 5)\n",
        "\n",
        "    # Generate that many random numbers from 0-999\n",
        "    numbers = [random.randint(0, 999) for _ in range(length)]\n",
        "    numbers_str = ','.join(map(str, numbers))\n",
        "\n",
        "    # Sort descending\n",
        "    sorted_numbers = sorted(numbers, reverse=True)\n",
        "    sorted_str = ','.join(map(str, sorted_numbers))\n",
        "\n",
        "    # Format the output\n",
        "    output = f\"<sos> {numbers_str} <descending> {sorted_str} <eos>\"\n",
        "    return output\n",
        "\n",
        "# Example usage\n",
        "for _ in range(5):\n",
        "    print(generate_ascending_sort())\n",
        "\n",
        "for _ in range(5):\n",
        "    print(generate_descending_sort())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf5b2dec-1adf-4c2f-b1a5-e0ad56b10feb",
      "metadata": {
        "id": "cf5b2dec-1adf-4c2f-b1a5-e0ad56b10feb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b78c083-84a7-426b-967f-952a9672d880",
      "metadata": {
        "id": "1b78c083-84a7-426b-967f-952a9672d880"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "from typing import List, Tuple, Set\n",
        "import os\n",
        "\n",
        "class CustomTokenizer:\n",
        "    def __init__(self):\n",
        "        # Define vocabulary\n",
        "        self.tokens = [\n",
        "            \"<sos>\", \"<eos>\", \"<sum>\", \"<reverse>\", \"<ascending>\", \"<descending>\",\n",
        "            \"<even>\", \"<odd>\", \"<even_repeat>\", \"<odd_repeat>\", \",\"\n",
        "        ] + [str(i) for i in range(1000)]  # 0-999\n",
        "\n",
        "        # Token ‚Üî ID mappings\n",
        "        self.token2id = {tok: idx for idx, tok in enumerate(self.tokens)}\n",
        "        self.id2token = {idx: tok for tok, idx in self.token2id.items()}\n",
        "        self.vocab_size = len(self.tokens)\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        \"\"\"\n",
        "        Convert string to list of token IDs.\n",
        "        - Matches tags, numbers (0-999), and commas\n",
        "        - Ignores spaces\n",
        "        \"\"\"\n",
        "        tokens = []\n",
        "        i = 0\n",
        "        while i < len(text):\n",
        "            if text[i] == \" \":\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if text[i] == \"<\":  # special token\n",
        "                j = text.find(\">\", i)\n",
        "                if j != -1:\n",
        "                    tok = text[i:j+1]\n",
        "                    if tok in self.token2id:\n",
        "                        tokens.append(self.token2id[tok])\n",
        "                        i = j + 1\n",
        "                        continue\n",
        "\n",
        "            if text[i] == \",\":  # comma\n",
        "                tokens.append(self.token2id[\",\"])\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            if text[i].isdigit():  # start of number\n",
        "                # Extract full number (could be 1-3 digits)\n",
        "                j = i\n",
        "                while j < len(text) and text[j].isdigit():\n",
        "                    j += 1\n",
        "                number_str = text[i:j]\n",
        "                number = int(number_str)\n",
        "\n",
        "                if number <= 999:\n",
        "                    tokens.append(self.token2id[str(number)])\n",
        "                    i = j\n",
        "                else:\n",
        "                    raise ValueError(f\"Number out of range: {number}\")\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected character: {text[i]}\")\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    def decode(self, token_ids: List[int]) -> str:\n",
        "        \"\"\"Convert list of IDs back to string with proper comma formatting.\"\"\"\n",
        "        tokens = [self.id2token[i] for i in token_ids]\n",
        "        result = \"\"\n",
        "\n",
        "        for i, token in enumerate(tokens):\n",
        "            if token.startswith(\"<\") and token.endswith(\">\"):\n",
        "                # Add special token with spaces\n",
        "                if result and not result.endswith(\" \"):\n",
        "                    result += \" \"\n",
        "                result += token\n",
        "                if i < len(tokens) - 1:  # not last token\n",
        "                    result += \" \"\n",
        "            elif token == \",\":\n",
        "                result += \",\"\n",
        "            else:\n",
        "                # Number token\n",
        "                result += token\n",
        "\n",
        "        return result\n",
        "\n",
        "# Note: These functions should be imported from your existing modules\n",
        "# generate_random_numbers_and_sum()\n",
        "# generate_even_repeat_sequence()\n",
        "# generate_odd_repeat_sequence()\n",
        "# generate_random_numbers_and_reverse()\n",
        "# generate_ascending_sort()\n",
        "# generate_descending_sort()\n",
        "\n",
        "def generate_dataset(num_examples: int = 50000) -> List[str]:\n",
        "    \"\"\"Generate a balanced dataset with all command types, ensuring no duplicates\"\"\"\n",
        "    examples = set()  # Use set to automatically handle duplicates\n",
        "    generators = [\n",
        "        generate_random_numbers_and_sum, generate_random_numbers_and_reverse,\n",
        "        generate_ascending_sort, generate_descending_sort,\n",
        "        generate_even_repeat_sequence, generate_odd_repeat_sequence\n",
        "    ]\n",
        "\n",
        "    examples_per_type = num_examples // len(generators)\n",
        "\n",
        "    # Generate examples for each type\n",
        "    for generator in generators:\n",
        "        attempts = 0\n",
        "        generated_for_this_type = 0\n",
        "\n",
        "        while generated_for_this_type < examples_per_type and attempts < examples_per_type * 3:\n",
        "            example = generator()\n",
        "            if example not in examples:\n",
        "                examples.add(example)\n",
        "                generated_for_this_type += 1\n",
        "            attempts += 1\n",
        "\n",
        "    # Fill remaining examples\n",
        "    while len(examples) < num_examples:\n",
        "        generator = random.choice(generators)\n",
        "        example = generator()\n",
        "        examples.add(example)\n",
        "\n",
        "        # Prevent infinite loop if we can't generate enough unique examples\n",
        "        if len(examples) >= num_examples * 0.95:  # Accept 95% of target if struggling\n",
        "            break\n",
        "\n",
        "    # Convert back to list and shuffle\n",
        "    examples_list = list(examples)\n",
        "    random.shuffle(examples_list)\n",
        "\n",
        "    print(f\"Generated {len(examples_list)} unique examples (requested {num_examples})\")\n",
        "    return examples_list\n",
        "\n",
        "def create_training_data(examples: List[str], tokenizer: CustomTokenizer) -> Tuple[np.ndarray, dict]:\n",
        "    \"\"\"Convert examples to tokenized training data\"\"\"\n",
        "    tokenized_examples = []\n",
        "    max_length = 0\n",
        "\n",
        "    # Tokenize all examples and find max length\n",
        "    for example in examples:\n",
        "        tokens = tokenizer.encode(example)\n",
        "        tokenized_examples.append(tokens)\n",
        "        max_length = max(max_length, len(tokens))\n",
        "\n",
        "    print(f\"Maximum sequence length: {max_length}\")\n",
        "    print(f\"Vocabulary size: {tokenizer.vocab_size}\")\n",
        "\n",
        "    # Convert to numpy array (pad sequences to max length)\n",
        "    data = np.full((len(examples), max_length), tokenizer.token2id[\"<eos>\"], dtype=np.int64)\n",
        "\n",
        "    for i, tokens in enumerate(tokenized_examples):\n",
        "        data[i, :len(tokens)] = tokens\n",
        "\n",
        "    # Create metadata\n",
        "    meta = {\n",
        "        'vocab_size': tokenizer.vocab_size,\n",
        "        'max_length': max_length,\n",
        "        'num_examples': len(examples),\n",
        "        'token2id': tokenizer.token2id,\n",
        "        'id2token': tokenizer.id2token\n",
        "    }\n",
        "\n",
        "    return data, meta\n",
        "\n",
        "def test_tokenizer():\n",
        "    \"\"\"Test the tokenizer with sample examples\"\"\"\n",
        "    print(\"Testing Tokenizer...\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    tokenizer = CustomTokenizer()\n",
        "\n",
        "    # Test with each type of example\n",
        "    test_examples = [\n",
        "        generate_random_numbers_and_sum(),\n",
        "        generate_random_numbers_and_reverse(),\n",
        "        generate_ascending_sort(),\n",
        "        generate_descending_sort(),\n",
        "        generate_even_repeat_sequence(),\n",
        "        generate_odd_repeat_sequence()\n",
        "    ]\n",
        "\n",
        "    for i, example in enumerate(test_examples):\n",
        "        print(f\"\\nTest {i+1}:\")\n",
        "        print(f\"Original: {example}\")\n",
        "\n",
        "        # Encode\n",
        "        encoded = tokenizer.encode(example)\n",
        "        print(f\"Encoded:  {encoded}\")\n",
        "\n",
        "        # Decode\n",
        "        decoded = tokenizer.decode(encoded)\n",
        "        print(f\"Decoded:  {decoded}\")\n",
        "\n",
        "        # Check if round-trip works\n",
        "        success = example.replace(\" \", \"\") == decoded.replace(\" \", \"\")\n",
        "        print(f\"Round-trip successful: {success}\")\n",
        "\n",
        "def save_dataset(train_data: np.ndarray, val_data: np.ndarray, meta: dict, data_dir: str = \"data/number_commands\"):\n",
        "    \"\"\"Save dataset to files\"\"\"\n",
        "    os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "    # Save binary data\n",
        "    train_data.astype(np.uint16).tofile(os.path.join(data_dir, 'train.bin'))\n",
        "    val_data.astype(np.uint16).tofile(os.path.join(data_dir, 'val.bin'))\n",
        "\n",
        "    # Save metadata\n",
        "    with open(os.path.join(data_dir, 'meta.pkl'), 'wb') as f:\n",
        "        pickle.dump(meta, f)\n",
        "\n",
        "    print(f\"Dataset saved to {data_dir}\")\n",
        "    print(f\"Train examples: {len(train_data)}\")\n",
        "    print(f\"Val examples: {len(val_data)}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11f74c8-dd01-40fc-b275-39cf93c28af2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e11f74c8-dd01-40fc-b275-39cf93c28af2",
        "outputId": "08d9d2ed-ab58-4e06-dc71-17a7d8975c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Tokenizer...\n",
            "==================================================\n",
            "\n",
            "Test 1:\n",
            "Original: <sos> 78,84,56,32,81,85,10,20,22,47 <sum> 515 <eos>\n",
            "Encoded:  [0, 89, 10, 95, 10, 67, 10, 43, 10, 92, 10, 96, 10, 21, 10, 31, 10, 33, 10, 58, 2, 526, 1]\n",
            "Decoded:  <sos> 78,84,56,32,81,85,10,20,22,47 <sum> 515 <eos>\n",
            "Round-trip successful: True\n",
            "\n",
            "Test 2:\n",
            "Original: <sos> 60,95,9,51,97,92,94 <reverse> 94,92,97,51,9,95,60 <eos>\n",
            "Encoded:  [0, 71, 10, 106, 10, 20, 10, 62, 10, 108, 10, 103, 10, 105, 3, 105, 10, 103, 10, 108, 10, 62, 10, 20, 10, 106, 10, 71, 1]\n",
            "Decoded:  <sos> 60,95,9,51,97,92,94 <reverse> 94,92,97,51,9,95,60 <eos>\n",
            "Round-trip successful: True\n",
            "\n",
            "Test 3:\n",
            "Original: <sos> 510,808,39,185 <ascending> 39,185,510,808 <eos>\n",
            "Encoded:  [0, 521, 10, 819, 10, 50, 10, 196, 4, 50, 10, 196, 10, 521, 10, 819, 1]\n",
            "Decoded:  <sos> 510,808,39,185 <ascending> 39,185,510,808 <eos>\n",
            "Round-trip successful: True\n",
            "\n",
            "Test 4:\n",
            "Original: <sos> 658,865,329,671,103 <descending> 865,671,658,329,103 <eos>\n",
            "Encoded:  [0, 669, 10, 876, 10, 340, 10, 682, 10, 114, 5, 876, 10, 682, 10, 669, 10, 340, 10, 114, 1]\n",
            "Decoded:  <sos> 658,865,329,671,103 <descending> 865,671,658,329,103 <eos>\n",
            "Round-trip successful: True\n",
            "\n",
            "Test 5:\n",
            "Original: <sos> 18,19,51,6 <even_repeat> 18,6,18,6,18,6,18,6,18,6 <eos>\n",
            "Encoded:  [0, 29, 10, 30, 10, 62, 10, 17, 8, 29, 10, 17, 10, 29, 10, 17, 10, 29, 10, 17, 10, 29, 10, 17, 10, 29, 10, 17, 1]\n",
            "Decoded:  <sos> 18,19,51,6 <even_repeat> 18,6,18,6,18,6,18,6,18,6 <eos>\n",
            "Round-trip successful: True\n",
            "\n",
            "Test 6:\n",
            "Original: <sos> 91,87,49,63 <odd_repeat> 91,87,49,63,91,87,49,63,91,87 <eos>\n",
            "Encoded:  [0, 102, 10, 98, 10, 60, 10, 74, 9, 102, 10, 98, 10, 60, 10, 74, 10, 102, 10, 98, 10, 60, 10, 74, 10, 102, 10, 98, 1]\n",
            "Decoded:  <sos> 91,87,49,63 <odd_repeat> 91,87,49,63,91,87,49,63,91,87 <eos>\n",
            "Round-trip successful: True\n"
          ]
        }
      ],
      "source": [
        "# Test the tokenizer\n",
        "test_tokenizer()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a13074a-717e-4ebc-af78-2816b43eb55b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2a13074a-717e-4ebc-af78-2816b43eb55b",
        "outputId": "844e6129-3090-40d2-a584-29751fae28c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "Generating Dataset...\n",
            "Generated 60000 unique examples (requested 60000)\n",
            "Maximum sequence length: 33\n",
            "Vocabulary size: 1011\n",
            "Maximum sequence length: 33\n",
            "Vocabulary size: 1011\n",
            "Dataset saved to data/number_commands\n",
            "Train examples: 54000\n",
            "Val examples: 6000\n"
          ]
        }
      ],
      "source": [
        "# Generate and process dataset\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Generating Dataset...\")\n",
        "\n",
        "examples = generate_dataset(60000)\n",
        "tokenizer = CustomTokenizer()\n",
        "\n",
        "# Split into train/val\n",
        "split_idx = int(0.9 * len(examples))\n",
        "train_examples = examples[:split_idx]\n",
        "val_examples = examples[split_idx:]\n",
        "\n",
        "# Create training data\n",
        "train_data, meta = create_training_data(train_examples, tokenizer)\n",
        "val_data, _ = create_training_data(val_examples, tokenizer)\n",
        "\n",
        "# Save dataset\n",
        "save_dataset(train_data, val_data, meta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f872868-8648-4af3-9486-ae0ea0c70e46",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f872868-8648-4af3-9486-ae0ea0c70e46",
        "outputId": "b69e8848-2c4b-4f5d-c8b1-f4812c2daa2d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset Statistics:\n",
            "Vocabulary size: 1011\n",
            "Max sequence length: 33\n",
            "Train data shape: (54000, 33)\n",
            "Val data shape: (6000, 33)\n",
            "\n",
            "Sample training examples:\n",
            "1: <sos> 64,23,50,10,39 <reverse> 39,10,50,23,64 <eos>\n",
            "2: <sos> 94,53,63,45,46,79,65 <reverse> 65,79,46,45,63,53,94 <eos>\n",
            "3: <sos> 502,272,75,32 <descending> 502,272,75,32 <eos>\n",
            "4: <sos> 20,97,62,94,29 <reverse> 29,94,62,97,20 <eos>\n",
            "5: <sos> 84,20,1 <odd_repeat> 1,1,1,1,1,1,1,1,1,1 <eos>\n"
          ]
        }
      ],
      "source": [
        "# Show some statistics\n",
        "print(\"\\nDataset Statistics:\")\n",
        "print(f\"Vocabulary size: {meta['vocab_size']}\")\n",
        "print(f\"Max sequence length: {meta['max_length']}\")\n",
        "print(f\"Train data shape: {train_data.shape}\")\n",
        "print(f\"Val data shape: {val_data.shape}\")\n",
        "\n",
        "# Show sample training examples\n",
        "print(\"\\nSample training examples:\")\n",
        "for i in range(5):\n",
        "    tokens = train_data[i]\n",
        "    # Remove padding\n",
        "    tokens = tokens[tokens != tokenizer.token2id[\"<eos>\"]]\n",
        "    tokens = np.append(tokens, tokenizer.token2id[\"<eos>\"])  # Add back one EOS\n",
        "    decoded = tokenizer.decode(tokens.tolist())\n",
        "    print(f\"{i+1}: {decoded}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ff174b9-6bfa-40c6-9358-0cb3920a5a2c",
      "metadata": {
        "id": "8ff174b9-6bfa-40c6-9358-0cb3920a5a2c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5292ce41-fdec-40dc-a2c9-8fdccbf8f53a",
      "metadata": {
        "id": "5292ce41-fdec-40dc-a2c9-8fdccbf8f53a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98b4f33c-dc03-483a-8dcf-14d567017eae",
      "metadata": {
        "id": "98b4f33c-dc03-483a-8dcf-14d567017eae"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
        "        if self.flash:\n",
        "            # efficient attention using Flash Attention CUDA kernels\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            # manual implementation of attention\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "@dataclass\n",
        "class NumberCommandConfig:\n",
        "    block_size: int = 32  # Small sequences for number commands\n",
        "    vocab_size: int = 17  # Will be set from dataset\n",
        "    n_layer: int = 4      # Smaller model for this simple task\n",
        "    n_head: int = 4       # Fewer attention heads\n",
        "    n_embd: int = 128     # Smaller embedding dimension\n",
        "    dropout: float = 0.1  # Some dropout for regularization\n",
        "    bias: bool = True     # Keep bias terms\n",
        "\n",
        "class NumberCommandTransformer(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # Weight tying - share embeddings with output layer\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        # report number of parameters\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        \"\"\"\n",
        "        Return the number of parameters in the model.\n",
        "        For non-embedding count (default), the position embeddings get subtracted.\n",
        "        The token embeddings would too, except due to the parameter sharing these\n",
        "        params are actually used as weights in the final layer, so we include them.\n",
        "        \"\"\"\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.wpe.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        # start with all of the candidate parameters\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        # filter out those that do not require grad\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
        "        \"\"\"Estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS\"\"\"\n",
        "        # First estimate the number of flops we do per iteration.\n",
        "        # See PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
        "        N = self.get_num_params()\n",
        "        cfg = self.config\n",
        "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
        "        flops_per_token = 6*N + 12*L*H*Q*T\n",
        "        flops_per_fwdbwd = flops_per_token * T\n",
        "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
        "        # Express our flops throughput as ratio of A100 bfloat16 peak flops\n",
        "        flops_achieved = flops_per_iter * (1.0/dt)  # per second\n",
        "        flops_promised = 312e12  # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
        "        mfu = flops_achieved / flops_promised\n",
        "        return mfu\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None, eos_token_id=None):\n",
        "        batch_size = idx.size(0)\n",
        "        finished = torch.zeros(batch_size, dtype=torch.bool, device=idx.device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            # Append only for unfinished sequences\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "            if eos_token_id is not None:\n",
        "                # Update finished mask\n",
        "                finished = finished | (idx_next.squeeze(1) == eos_token_id)\n",
        "                # If all sequences finished, we can break early\n",
        "                if finished.all():\n",
        "                    break\n",
        "\n",
        "            # Optionally, you could replace tokens in finished sequences with eos_token_id to pad\n",
        "            idx_next[finished.unsqueeze(1)] = eos_token_id\n",
        "\n",
        "        return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87916a28-60f7-4da9-8eb1-fe6d313f75e3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "87916a28-60f7-4da9-8eb1-fe6d313f75e3",
        "outputId": "45ed15b7-e8e5-43ef-e200-73679aad9ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.8.0+cu126\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc790478-7d33-444e-a1d0-28319aa2d2e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dc790478-7d33-444e-a1d0-28319aa2d2e9",
        "outputId": "7551c3a8-2b98-4d06-c8dc-e12b587a4413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "CUDA device: Tesla T4\n",
            "‚úì Training data found\n",
            "‚úì Vocab size: 1011\n",
            "‚úì Max sequence length: 33\n",
            "‚úì Using device: cuda\n",
            "‚úì Model compilation: True\n",
            "‚úì Using dtype: bfloat16\n",
            "‚úì Testing data loading...\n",
            "‚úì Batch shape: torch.Size([2, 32]), torch.Size([2, 32])\n",
            "number of parameters: 0.01M\n",
            "‚úì Model created with 0.01M parameters\n",
            "num decayed parameter tensors: 18, with 11,416 parameters\n",
            "num non-decayed parameter tensors: 34, with 432 parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3318903178.py:122: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using fused AdamW: True\n",
            "Compiling model...\n",
            "‚úì Model compilation successful!\n",
            "\n",
            "============================================================\n",
            "STARTING TRAINING\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "W0826 05:00:40.266000 266 torch/_inductor/utils.py:1436] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial: train loss 6.9013, val loss 6.9010\n",
            "step 0: train loss 6.9015, val loss 6.9011, lr 1.49e-06\n",
            "‚úì Saved checkpoint (val_loss: 6.9011)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_inductor/compile_fx.py:2509: UserWarning: Tesla T4 does not support bfloat16 compilation natively, skipping\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "iter 0: loss 6.9041, time 21095.7ms\n",
            "iter 50: loss 6.8567, time 982.0ms\n",
            "iter 100: loss 6.7708, time 877.4ms\n",
            "iter 150: loss 6.6585, time 874.1ms\n",
            "iter 200: loss 6.4991, time 874.3ms\n",
            "iter 250: loss 6.3190, time 881.1ms\n",
            "iter 300: loss 6.1180, time 875.4ms\n",
            "iter 350: loss 5.9414, time 890.9ms\n",
            "iter 400: loss 5.7563, time 890.3ms\n",
            "iter 450: loss 5.5737, time 992.4ms\n",
            "step 500: train loss 5.3480, val loss 5.3426, lr 2.97e-04\n",
            "‚úì Saved checkpoint (val_loss: 5.3426)\n",
            "iter 500: loss 5.3616, time 2849.9ms\n",
            "iter 550: loss 5.1536, time 887.3ms\n",
            "iter 600: loss 4.9176, time 885.2ms\n",
            "iter 650: loss 4.7385, time 891.1ms\n",
            "iter 700: loss 4.5448, time 894.6ms\n",
            "iter 750: loss 4.3489, time 886.1ms\n",
            "iter 800: loss 4.1536, time 889.5ms\n",
            "iter 850: loss 3.9233, time 887.5ms\n",
            "iter 900: loss 3.7530, time 867.4ms\n",
            "iter 950: loss 3.5629, time 892.5ms\n",
            "step 1000: train loss 3.3359, val loss 3.3265, lr 2.82e-04\n",
            "‚úì Saved checkpoint (val_loss: 3.3265)\n",
            "iter 1000: loss 3.3285, time 2941.1ms\n",
            "iter 1050: loss 3.1350, time 932.2ms\n",
            "iter 1100: loss 3.0977, time 881.6ms\n",
            "iter 1150: loss 2.9265, time 895.9ms\n",
            "iter 1200: loss 2.8144, time 899.4ms\n",
            "iter 1250: loss 2.6529, time 874.5ms\n",
            "iter 1300: loss 2.5659, time 885.8ms\n",
            "iter 1350: loss 2.5815, time 890.7ms\n",
            "iter 1400: loss 2.4298, time 900.4ms\n",
            "iter 1450: loss 2.4461, time 897.4ms\n",
            "step 1500: train loss 2.2965, val loss 2.2790, lr 2.54e-04\n",
            "‚úì Saved checkpoint (val_loss: 2.2790)\n",
            "iter 1500: loss 2.3471, time 3095.2ms\n",
            "iter 1550: loss 2.2775, time 1200.0ms\n",
            "iter 1600: loss 2.1750, time 887.7ms\n",
            "iter 1650: loss 2.1740, time 871.3ms\n",
            "iter 1700: loss 2.2775, time 891.0ms\n",
            "iter 1750: loss 2.1990, time 868.8ms\n",
            "iter 1800: loss 2.0916, time 882.3ms\n",
            "iter 1850: loss 2.1575, time 894.7ms\n",
            "iter 1900: loss 2.1614, time 902.0ms\n",
            "iter 1950: loss 2.1643, time 878.9ms\n",
            "step 2000: train loss 2.1149, val loss 2.1163, lr 2.17e-04\n",
            "‚úì Saved checkpoint (val_loss: 2.1163)\n",
            "iter 2000: loss 2.2196, time 2371.5ms\n",
            "iter 2050: loss 2.2314, time 1065.9ms\n",
            "iter 2100: loss 2.1842, time 1182.6ms\n",
            "iter 2150: loss 1.9971, time 950.5ms\n",
            "iter 2200: loss 2.1557, time 886.5ms\n",
            "iter 2250: loss 2.0790, time 1224.8ms\n",
            "iter 2300: loss 2.1456, time 1588.5ms\n",
            "iter 2350: loss 2.0636, time 1509.2ms\n",
            "iter 2400: loss 2.0091, time 884.0ms\n",
            "iter 2450: loss 2.1123, time 882.9ms\n",
            "step 2500: train loss 2.0634, val loss 2.0463, lr 1.74e-04\n",
            "‚úì Saved checkpoint (val_loss: 2.0463)\n",
            "iter 2500: loss 2.0241, time 2444.1ms\n",
            "iter 2550: loss 2.1496, time 1148.4ms\n",
            "iter 2600: loss 2.0904, time 1115.2ms\n",
            "iter 2650: loss 2.0949, time 888.7ms\n",
            "iter 2700: loss 2.0341, time 892.8ms\n",
            "iter 2750: loss 2.0422, time 911.7ms\n",
            "iter 2800: loss 1.9802, time 886.8ms\n",
            "iter 2850: loss 2.0500, time 886.8ms\n",
            "iter 2900: loss 1.9794, time 890.9ms\n",
            "iter 2950: loss 2.0436, time 914.3ms\n",
            "step 3000: train loss 2.0229, val loss 2.0112, lr 1.30e-04\n",
            "‚úì Saved checkpoint (val_loss: 2.0112)\n",
            "iter 3000: loss 2.1061, time 2452.9ms\n",
            "iter 3050: loss 2.0157, time 929.0ms\n",
            "iter 3100: loss 2.0250, time 1154.1ms\n",
            "iter 3150: loss 2.0631, time 1161.7ms\n",
            "iter 3200: loss 2.0304, time 898.2ms\n",
            "iter 3250: loss 2.0928, time 891.4ms\n",
            "iter 3300: loss 2.0232, time 875.0ms\n",
            "iter 3350: loss 2.0670, time 884.6ms\n",
            "iter 3400: loss 2.0754, time 895.2ms\n",
            "iter 3450: loss 1.9725, time 893.2ms\n",
            "step 3500: train loss 2.0073, val loss 2.0015, lr 9.00e-05\n",
            "‚úì Saved checkpoint (val_loss: 2.0015)\n",
            "iter 3500: loss 1.9927, time 2380.1ms\n",
            "iter 3550: loss 2.0347, time 891.4ms\n",
            "iter 3600: loss 2.0682, time 890.9ms\n",
            "iter 3650: loss 2.1567, time 1124.3ms\n",
            "iter 3700: loss 2.0453, time 1214.0ms\n",
            "iter 3750: loss 2.0204, time 891.0ms\n",
            "iter 3800: loss 2.1319, time 888.0ms\n",
            "iter 3850: loss 2.0342, time 894.0ms\n",
            "iter 3900: loss 2.0528, time 889.7ms\n",
            "iter 3950: loss 2.0457, time 894.7ms\n",
            "step 4000: train loss 1.9920, val loss 1.9970, lr 5.79e-05\n",
            "‚úì Saved checkpoint (val_loss: 1.9970)\n",
            "iter 4000: loss 1.9704, time 2409.0ms\n",
            "iter 4050: loss 2.0417, time 900.1ms\n",
            "iter 4100: loss 2.0439, time 904.7ms\n",
            "iter 4150: loss 2.0353, time 900.9ms\n",
            "iter 4200: loss 2.0926, time 1089.7ms\n",
            "iter 4250: loss 1.9225, time 1219.0ms\n",
            "iter 4300: loss 1.9672, time 928.1ms\n",
            "iter 4350: loss 2.0403, time 879.8ms\n",
            "iter 4400: loss 2.0106, time 882.7ms\n",
            "iter 4450: loss 2.0181, time 896.3ms\n",
            "step 4500: train loss 1.9946, val loss 1.9926, lr 3.72e-05\n",
            "‚úì Saved checkpoint (val_loss: 1.9926)\n",
            "iter 4500: loss 2.1350, time 2443.1ms\n",
            "iter 4550: loss 2.1388, time 1023.5ms\n",
            "iter 4600: loss 1.9172, time 998.9ms\n",
            "iter 4650: loss 2.1168, time 869.6ms\n",
            "iter 4700: loss 1.9269, time 878.7ms\n",
            "iter 4750: loss 2.0259, time 1101.0ms\n",
            "iter 4800: loss 2.0197, time 1205.9ms\n",
            "iter 4850: loss 1.9831, time 908.5ms\n",
            "iter 4900: loss 2.0825, time 885.7ms\n",
            "iter 4950: loss 2.0736, time 877.4ms\n",
            "\n",
            "Final: train loss 1.9892, val loss 1.9749\n",
            "Training completed! Best val loss: 1.9926\n",
            "Model saved to: out_number_commands/ckpt.pt\n"
          ]
        }
      ],
      "source": [
        "# ALL-IN-ONE TRAINING SCRIPT FOR NUMBER COMMAND TRANSFORMER\n",
        "#\n",
        "\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "import pickle\n",
        "import platform\n",
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "print(\"PyTorch version:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device:\", torch.cuda.get_device_name())\n",
        "\n",
        "# Check if we have the required data\n",
        "data_dir = 'data/number_commands'\n",
        "if not os.path.exists(os.path.join(data_dir, 'train.bin')):\n",
        "    raise FileNotFoundError(f\"Training data not found at {data_dir}. Please run dataset generation first.\")\n",
        "\n",
        "print(\"‚úì Training data found\")\n",
        "\n",
        "# Load metadata\n",
        "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "with open(meta_path, 'rb') as f:\n",
        "    meta = pickle.load(f)\n",
        "\n",
        "vocab_size = meta['vocab_size']\n",
        "max_length = meta['max_length']\n",
        "\n",
        "print(f\"‚úì Vocab size: {vocab_size}\")\n",
        "print(f\"‚úì Max sequence length: {max_length}\")\n",
        "\n",
        "# Configuration\n",
        "config = {\n",
        "    'vocab_size': vocab_size,\n",
        "    'block_size': min(max_length, 32),  # Use actual max length or 32, whichever is smaller\n",
        "    'n_layer': 4,\n",
        "    'n_head': 4,\n",
        "    'n_embd': 8, #worked iwth 128, let's try smaller for our massive 1011 token vocab\n",
        "    'dropout': 0.1,\n",
        "    'bias': True,\n",
        "    'batch_size': 64,\n",
        "    'learning_rate': 3e-4,\n",
        "    'max_iters': 5000,\n",
        "    'eval_interval': 500,\n",
        "    'log_interval': 50,\n",
        "    'warmup_iters': 200,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    'compile': torch.cuda.is_available() and platform.system() != 'Windows',\n",
        "    'out_dir': 'out_number_commands'\n",
        "}\n",
        "\n",
        "print(f\"‚úì Using device: {config['device']}\")\n",
        "print(f\"‚úì Model compilation: {config['compile']}\")\n",
        "\n",
        "# Set up device and dtype\n",
        "device = config['device']\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
        "\n",
        "print(f\"‚úì Using dtype: {dtype}\")\n",
        "\n",
        "# Create output directory\n",
        "os.makedirs(config['out_dir'], exist_ok=True)\n",
        "\n",
        "# Set seeds\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "# Data loading function\n",
        "def get_batch(split, batch_size, block_size):\n",
        "    if split == 'train':\n",
        "        data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "    else:\n",
        "        data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "\n",
        "    if device == 'cuda':\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "# Test data loading\n",
        "print(\"‚úì Testing data loading...\")\n",
        "test_x, test_y = get_batch('train', 2, config['block_size'])\n",
        "print(f\"‚úì Batch shape: {test_x.shape}, {test_y.shape}\")\n",
        "\n",
        "# Create and initialize model (model classes should be defined already)\n",
        "try:\n",
        "    model_config = NumberCommandConfig(\n",
        "        vocab_size=config['vocab_size'],\n",
        "        block_size=config['block_size'],\n",
        "        n_layer=config['n_layer'],\n",
        "        n_head=config['n_head'],\n",
        "        n_embd=config['n_embd'],\n",
        "        dropout=config['dropout'],\n",
        "        bias=config['bias']\n",
        "    )\n",
        "\n",
        "    model = NumberCommandTransformer(model_config)\n",
        "    model.to(device)\n",
        "\n",
        "    print(f\"‚úì Model created with {model.get_num_params()/1e6:.2f}M parameters\")\n",
        "\n",
        "except NameError:\n",
        "    print(\"‚ùå Model classes not found! Please run the model definition cell first.\")\n",
        "    raise\n",
        "\n",
        "# Initialize training components\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "optimizer = model.configure_optimizers(\n",
        "    weight_decay=1e-2,\n",
        "    learning_rate=config['learning_rate'],\n",
        "    betas=(0.9, 0.95),\n",
        "    device_type=device\n",
        ")\n",
        "\n",
        "# Compile model with error handling\n",
        "if config['compile']:\n",
        "    try:\n",
        "        print(\"Compiling model...\")\n",
        "        unoptimized_model = model\n",
        "        model = torch.compile(model)\n",
        "        print(\"‚úì Model compilation successful!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Model compilation failed: {e}\")\n",
        "        print(\"Continuing without compilation...\")\n",
        "\n",
        "# Loss estimation function\n",
        "@torch.no_grad()\n",
        "def estimate_loss(eval_iters=100):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, config['batch_size'], config['block_size'])\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "# Learning rate scheduler\n",
        "def get_lr(it):\n",
        "    if it < config['warmup_iters']:\n",
        "        return config['learning_rate'] * (it + 1) / (config['warmup_iters'] + 1)\n",
        "    if it > config['max_iters']:\n",
        "        return config['learning_rate'] * 0.1\n",
        "    decay_ratio = (it - config['warmup_iters']) / (config['max_iters'] - config['warmup_iters'])\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return config['learning_rate'] * 0.1 + coeff * (config['learning_rate'] - config['learning_rate'] * 0.1)\n",
        "\n",
        "# Training loop\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING TRAINING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "iter_num = 0\n",
        "best_val_loss = float('inf')\n",
        "t0 = time.time()\n",
        "\n",
        "# Initial evaluation\n",
        "losses = estimate_loss()\n",
        "print(f\"Initial: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "while iter_num < config['max_iters']:\n",
        "    # Set learning rate\n",
        "    lr = get_lr(iter_num)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # Evaluate and save checkpoints\n",
        "    if iter_num % config['eval_interval'] == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, lr {lr:.2e}\")\n",
        "\n",
        "        if losses['val'] < best_val_loss:\n",
        "            best_val_loss = losses['val']\n",
        "            checkpoint = {\n",
        "                'model': model.state_dict(),\n",
        "                'optimizer': optimizer.state_dict(),\n",
        "                'model_args': config,\n",
        "                'iter_num': iter_num,\n",
        "                'best_val_loss': best_val_loss,\n",
        "            }\n",
        "            torch.save(checkpoint, os.path.join(config['out_dir'], 'ckpt.pt'))\n",
        "            print(f\"‚úì Saved checkpoint (val_loss: {best_val_loss:.4f})\")\n",
        "\n",
        "    # Training step\n",
        "    X, Y = get_batch('train', config['batch_size'], config['block_size'])\n",
        "\n",
        "    with ctx:\n",
        "        logits, loss = model(X, Y)\n",
        "\n",
        "    # Backward pass\n",
        "    scaler.scale(loss).backward()\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # Logging\n",
        "    if iter_num % config['log_interval'] == 0:\n",
        "        t1 = time.time()\n",
        "        dt = t1 - t0\n",
        "        t0 = t1\n",
        "        print(f\"iter {iter_num}: loss {loss.item():.4f}, time {dt*1000:.1f}ms\")\n",
        "\n",
        "    iter_num += 1\n",
        "\n",
        "# Final evaluation\n",
        "losses = estimate_loss()\n",
        "print(f\"\\nFinal: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "print(f\"Training completed! Best val loss: {best_val_loss:.4f}\")\n",
        "print(f\"Model saved to: {config['out_dir']}/ckpt.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0257dc83-bd1b-4a2a-b3ff-dfa851859f90",
      "metadata": {
        "id": "0257dc83-bd1b-4a2a-b3ff-dfa851859f90"
      },
      "source": [
        "# Now for Some Inference!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d35e581-ed3a-4a63-8e5f-621ff7e228f0",
      "metadata": {
        "id": "8d35e581-ed3a-4a63-8e5f-621ff7e228f0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "# Load model and tokenizer\n",
        "def load_trained_model(checkpoint_path: str = 'out_number_commands/ckpt.pt'):\n",
        "    \"\"\"Load the trained model and tokenizer\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Load metadata first\n",
        "        meta_path = 'data/number_commands/meta.pkl'\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"Dataset metadata not found. Please run dataset generation first.\")\n",
        "\n",
        "    # Recreate tokenizer\n",
        "    class CustomTokenizer:\n",
        "        def __init__(self, token2id, id2token):\n",
        "            self.token2id = token2id\n",
        "            self.id2token = id2token\n",
        "            self.vocab_size = len(token2id)\n",
        "\n",
        "        def encode(self, text: str) -> List[int]:\n",
        "            tokens = []\n",
        "            i = 0\n",
        "            while i < len(text):\n",
        "                if text[i] == \" \":\n",
        "                    i += 1\n",
        "                    continue\n",
        "\n",
        "                if text[i] == \"<\":\n",
        "                    j = text.find(\">\", i)\n",
        "                    if j != -1:\n",
        "                        tok = text[i:j+1]\n",
        "                        if tok in self.token2id:\n",
        "                            tokens.append(self.token2id[tok])\n",
        "                            i = j + 1\n",
        "                            continue\n",
        "\n",
        "                if text[i].isdigit():\n",
        "                    tokens.append(self.token2id[text[i]])\n",
        "                    i += 1\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected character: {text[i]}\")\n",
        "\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, token_ids: List[int]) -> str:\n",
        "            tokens = [self.id2token[i] for i in token_ids]\n",
        "            result = \"\"\n",
        "            current_number = \"\"\n",
        "\n",
        "            for i, token in enumerate(tokens):\n",
        "                if token.startswith(\"<\") and token.endswith(\">\"):\n",
        "                    # Finish any current number\n",
        "                    if current_number:\n",
        "                        if result and not result.endswith(\" \"):\n",
        "                            result += \" \"\n",
        "                        result += current_number\n",
        "                        current_number = \"\"\n",
        "\n",
        "                    # Add special token with spaces\n",
        "                    if result and not result.endswith(\" \"):\n",
        "                        result += \" \"\n",
        "                    result += token\n",
        "\n",
        "                else:\n",
        "                    # Accumulate digits into current number\n",
        "                    current_number += token\n",
        "\n",
        "            # Add final number if exists\n",
        "            if current_number:\n",
        "                if result and not result.endswith(\" \"):\n",
        "                    result += \" \"\n",
        "                result += current_number\n",
        "\n",
        "            return result\n",
        "\n",
        "    tokenizer = CustomTokenizer(meta['token2id'], meta['id2token'])\n",
        "\n",
        "    # Load model\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}. Please train the model first.\")\n",
        "\n",
        "    model_args = checkpoint['model_args']\n",
        "\n",
        "    # Handle both old and new checkpoint formats\n",
        "    if isinstance(model_args, dict):\n",
        "        # New format - direct config dict\n",
        "        try:\n",
        "            config = NumberCommandConfig(**model_args)\n",
        "        except TypeError:\n",
        "            # Fallback for missing config class\n",
        "            print(\"Warning: Using fallback model configuration\")\n",
        "            config = type('Config', (), model_args)()\n",
        "    else:\n",
        "        # Old format - might be a config object\n",
        "        config = model_args\n",
        "\n",
        "    try:\n",
        "        model = NumberCommandTransformer(config)\n",
        "    except NameError:\n",
        "        raise NameError(\"Model classes not found. Please run the model definition cell first.\")\n",
        "\n",
        "    # Remove potential compilation prefix\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"‚úì Model loaded from {checkpoint_path}\")\n",
        "    print(f\"‚úì Model has {model.get_num_params()/1e6:.2f}M parameters\")\n",
        "    print(f\"‚úì Using device: {device}\")\n",
        "\n",
        "    return model, tokenizer, device\n",
        "\n",
        "def test_model_completion(model, tokenizer, device, prompt: str, max_new_tokens: int = 15, temperature: float = 0.1):\n",
        "    \"\"\"Test model completion given a prompt\"\"\"\n",
        "\n",
        "    # Encode prompt\n",
        "    prompt_tokens = tokenizer.encode(prompt)\n",
        "    prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate completion\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "\n",
        "    # Decode result\n",
        "    generated_tokens = generated[0].tolist()\n",
        "    full_response = tokenizer.decode(generated_tokens)\n",
        "\n",
        "    return full_response, generated_tokens\n",
        "\n",
        "def calculate_expected_result(input_str: str) -> str:\n",
        "    \"\"\"Calculate what the expected output should be\"\"\"\n",
        "\n",
        "    # Parse the input\n",
        "    parts = input_str.strip().split()\n",
        "    if len(parts) < 3 or parts[0] != '<sos>':\n",
        "        return \"Invalid format\"\n",
        "\n",
        "    numbers_str = parts[1]\n",
        "    command = parts[2]\n",
        "\n",
        "    # Convert string to list of digits\n",
        "    try:\n",
        "        digits = [int(d) for d in numbers_str]\n",
        "    except ValueError:\n",
        "        return \"Invalid numbers\"\n",
        "\n",
        "    if command == '<sum>':\n",
        "        result = str(sum(digits))\n",
        "    elif command == '<reverse>':\n",
        "        result = numbers_str[::-1]\n",
        "    elif command == '<ascending>':\n",
        "        result = ''.join(map(str, sorted(digits)))\n",
        "    elif command == '<descending>':\n",
        "        result = ''.join(map(str, sorted(digits, reverse=True)))\n",
        "    elif command == '<even>':\n",
        "        # For <even>, we need to find a subsequence and repeat it\n",
        "        # This is harder to predict exactly, so we'll just note it\n",
        "        result = \"SUBSEQUENCE_REPEATED\"\n",
        "    else:\n",
        "        return \"Unknown command\"\n",
        "\n",
        "    return f\"<sos> {numbers_str} {command} {result} <eos>\"\n",
        "\n",
        "def run_comprehensive_test(model, tokenizer, device):\n",
        "    \"\"\"Run comprehensive tests on the model\"\"\"\n",
        "\n",
        "    test_cases = [\n",
        "        # Sum tests\n",
        "        \"<sos> 123 <sum>\",\n",
        "        \"<sos> 1232 <sum>\",\n",
        "        \"<sos> 92999 <sum>\",\n",
        "\n",
        "        # Reverse tests\n",
        "        \"<sos> 1765 <reverse>\",\n",
        "        \"<sos> 9235 <reverse>\",\n",
        "        \"<sos> 505 <reverse>\",\n",
        "\n",
        "        # Ascending sort tests\n",
        "        \"<sos> 5132 <ascending>\",\n",
        "        \"<sos> 19283 <ascending>\",\n",
        "        \"<sos> 5231 <ascending>\",\n",
        "\n",
        "        # Descending sort tests\n",
        "        \"<sos> 1562 <descending>\",\n",
        "        \"<sos> 91735 <descending>\",\n",
        "        \"<sos> 482 <descending>\",\n",
        "    ]\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPREHENSIVE MODEL TESTING\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        print(f\"\\nTest {i+1}: {test_case}\")\n",
        "\n",
        "        # Get model prediction\n",
        "        prediction, tokens = test_model_completion(model, tokenizer, device, test_case)\n",
        "        print(f\"Model output: {prediction}\")\n",
        "\n",
        "        # Calculate expected result\n",
        "        expected = calculate_expected_result(test_case)\n",
        "        print(f\"Expected:     {expected}\")\n",
        "\n",
        "        # Check if correct (for non-even commands)\n",
        "        if not '<even>' in test_case:\n",
        "            is_correct = prediction.strip() == expected.strip()\n",
        "            print(f\"Correct: {'‚úì' if is_correct else '‚úó'}\")\n",
        "            if is_correct:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        else:\n",
        "            print(\"Correct: (even command - manual check needed)\")\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    if total > 0:\n",
        "        accuracy = correct / total * 100\n",
        "        print(f\"\\nOverall Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
        "\n",
        "    return correct, total\n",
        "\n",
        "def interactive_test(model, tokenizer, device):\n",
        "    \"\"\"Interactive testing interface\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INTERACTIVE TESTING\")\n",
        "    print(\"Enter prompts like: <sos> 1234 <sum>\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            prompt = input(\"\\nEnter prompt: \").strip()\n",
        "            if prompt.lower() == 'quit':\n",
        "                break\n",
        "\n",
        "            if not prompt:\n",
        "                continue\n",
        "\n",
        "            # Test the prompt\n",
        "            prediction, tokens = test_model_completion(model, tokenizer, device, prompt)\n",
        "            print(f\"Model output: {prediction}\")\n",
        "\n",
        "            # Show expected if possible\n",
        "            try:\n",
        "                expected = calculate_expected_result(prompt)\n",
        "                if \"SUBSEQUENCE\" not in expected and \"Invalid\" not in expected and \"Unknown\" not in expected:\n",
        "                    print(f\"Expected:     {expected}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nExiting...\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab3006ce-2af5-41fb-b346-d352a80b34bc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab3006ce-2af5-41fb-b346-d352a80b34bc",
        "outputId": "75a5435e-434c-4f18-d0c3-1f19cdba71b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Using fallback model configuration\n",
            "number of parameters: 0.01M\n",
            "‚úì Model loaded from out_number_commands/ckpt.pt\n",
            "‚úì Model has 0.01M parameters\n",
            "‚úì Using device: cuda\n",
            "================================================================================\n",
            "COMPREHENSIVE MODEL TESTING\n",
            "================================================================================\n",
            "\n",
            "Test 1: <sos> 123 <sum>\n",
            "Error loading model: can't assign a NoneType to a torch.cuda.LongTensor\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    # Load the trained model\n",
        "    model, tokenizer, device = load_trained_model()\n",
        "\n",
        "    # Run comprehensive tests\n",
        "    correct, total = run_comprehensive_test(model, tokenizer, device)\n",
        "\n",
        "    # Start interactive testing\n",
        "    interactive_test(model, tokenizer, device)\n",
        "\n",
        "except FileNotFoundError as e:\n",
        "    print(f\"Error: Could not find model files. Make sure you've trained the model first.\")\n",
        "    print(\"Expected files:\")\n",
        "    print(\"  - out_number_commands/ckpt.pt\")\n",
        "    print(\"  - data/number_commands/meta.pkl\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading model: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# trying again to load the model"
      ],
      "metadata": {
        "id": "AYhyre4kyM2I"
      },
      "id": "AYhyre4kyM2I"
    },
    {
      "cell_type": "code",
      "source": [
        "#needs to be fixed, won't load model\n",
        "\n",
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "from typing import List\n",
        "\n",
        "# Load model and tokenizer\n",
        "def load_trained_model(checkpoint_path: str = 'out_number_commands/ckpt.pt'):\n",
        "    \"\"\"Load the trained model and tokenizer\"\"\"\n",
        "\n",
        "    try:\n",
        "        # Load metadata first\n",
        "        meta_path = 'data/number_commands/meta.pkl'\n",
        "        with open(meta_path, 'rb') as f:\n",
        "            meta = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(\"Dataset metadata not found. Please run dataset generation first.\")\n",
        "\n",
        "    # Recreate tokenizer\n",
        "    class CustomTokenizer:\n",
        "        def __init__(self, token2id, id2token):\n",
        "            self.token2id = token2id\n",
        "            self.id2token = id2token\n",
        "            self.vocab_size = len(token2id)\n",
        "\n",
        "        def encode(self, text: str) -> List[int]:\n",
        "            tokens = []\n",
        "            i = 0\n",
        "            while i < len(text):\n",
        "                if text[i] == \" \":\n",
        "                    i += 1\n",
        "                    continue\n",
        "\n",
        "                if text[i] == \"<\":\n",
        "                    j = text.find(\">\", i)\n",
        "                    if j != -1:\n",
        "                        tok = text[i:j+1]\n",
        "                        if tok in self.token2id:\n",
        "                            tokens.append(self.token2id[tok])\n",
        "                            i = j + 1\n",
        "                            continue\n",
        "\n",
        "                if text[i] == \",\":\n",
        "                    tokens.append(self.token2id[\",\"])\n",
        "                    i += 1\n",
        "                    continue\n",
        "\n",
        "                if text[i].isdigit():\n",
        "                    # Extract full number (could be 1-3 digits)\n",
        "                    j = i\n",
        "                    while j < len(text) and text[j].isdigit():\n",
        "                        j += 1\n",
        "                    number_str = text[i:j]\n",
        "                    number = int(number_str)\n",
        "\n",
        "                    if str(number) in self.token2id:\n",
        "                        tokens.append(self.token2id[str(number)])\n",
        "                        i = j\n",
        "                    else:\n",
        "                        raise ValueError(f\"Number out of range: {number}\")\n",
        "                else:\n",
        "                    raise ValueError(f\"Unexpected character: {text[i]}\")\n",
        "\n",
        "            return tokens\n",
        "\n",
        "        def decode(self, token_ids: List[int]) -> str:\n",
        "            tokens = [self.id2token[i] for i in token_ids]\n",
        "            result = \"\"\n",
        "\n",
        "            for i, token in enumerate(tokens):\n",
        "                if token.startswith(\"<\") and token.endswith(\">\"):\n",
        "                    # Add special token with spaces\n",
        "                    if result and not result.endswith(\" \"):\n",
        "                        result += \" \"\n",
        "                    result += token\n",
        "                    if i < len(tokens) - 1:  # not last token\n",
        "                        result += \" \"\n",
        "                elif token == \",\":\n",
        "                    result += \",\"\n",
        "                else:\n",
        "                    # Number token\n",
        "                    result += token\n",
        "\n",
        "            return result\n",
        "\n",
        "    tokenizer = CustomTokenizer(meta['token2id'], meta['id2token'])\n",
        "\n",
        "    # Load model\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    try:\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "    except FileNotFoundError:\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}. Please train the model first.\")\n",
        "\n",
        "    model_args = checkpoint['model_args']\n",
        "\n",
        "    # Handle both old and new checkpoint formats\n",
        "    if isinstance(model_args, dict):\n",
        "        # New format - direct config dict\n",
        "        try:\n",
        "            config = NumberCommandConfig(**model_args)\n",
        "        except TypeError:\n",
        "            # Fallback for missing config class\n",
        "            print(\"Warning: Using fallback model configuration\")\n",
        "            config = type('Config', (), model_args)()\n",
        "    else:\n",
        "        # Old format - might be a config object\n",
        "        config = model_args\n",
        "\n",
        "    try:\n",
        "        model = NumberCommandTransformer(config)\n",
        "    except NameError:\n",
        "        raise NameError(\"Model classes not found. Please run the model definition cell first.\")\n",
        "\n",
        "    # Remove potential compilation prefix\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"‚úì Model loaded from {checkpoint_path}\")\n",
        "    print(f\"‚úì Model has {model.get_num_params()/1e6:.2f}M parameters\")\n",
        "    print(f\"‚úì Using device: {device}\")\n",
        "\n",
        "    return model, tokenizer, device\n",
        "\n",
        "def test_model_completion(model, tokenizer, device, prompt: str, max_new_tokens: int = 15, temperature: float = 0.1):\n",
        "    \"\"\"Test model completion given a prompt\"\"\"\n",
        "\n",
        "    # Encode prompt\n",
        "    prompt_tokens = tokenizer.encode(prompt)\n",
        "    prompt_tensor = torch.tensor(prompt_tokens, dtype=torch.long, device=device).unsqueeze(0)\n",
        "\n",
        "    # Generate completion\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate(prompt_tensor, max_new_tokens=max_new_tokens, temperature=temperature)\n",
        "\n",
        "    # Decode result\n",
        "    generated_tokens = generated[0].tolist()\n",
        "    full_response = tokenizer.decode(generated_tokens)\n",
        "\n",
        "    return full_response, generated_tokens\n",
        "\n",
        "def calculate_expected_result(input_str: str) -> str:\n",
        "    \"\"\"Calculate what the expected output should be\"\"\"\n",
        "\n",
        "    # Parse the input\n",
        "    parts = input_str.strip().split()\n",
        "    if len(parts) < 3 or parts[0] != '<sos>':\n",
        "        return \"Invalid format\"\n",
        "\n",
        "    numbers_str = parts[1]\n",
        "    command = parts[2]\n",
        "\n",
        "    # Convert comma-separated string to list of numbers\n",
        "    try:\n",
        "        numbers = [int(n) for n in numbers_str.split(',')]\n",
        "    except ValueError:\n",
        "        return \"Invalid numbers\"\n",
        "\n",
        "    if command == '<sum>':\n",
        "        result = str(sum(numbers))\n",
        "    elif command == '<reverse>':\n",
        "        result = ','.join(map(str, numbers[::-1]))\n",
        "    elif command == '<ascending>':\n",
        "        result = ','.join(map(str, sorted(numbers)))\n",
        "    elif command == '<descending>':\n",
        "        result = ','.join(map(str, sorted(numbers, reverse=True)))\n",
        "    elif command == '<even_repeat>':\n",
        "        even_numbers = [num for num in numbers if num % 2 == 0]\n",
        "        if not even_numbers:\n",
        "            result = \"\"\n",
        "        else:\n",
        "            repeated_evens = []\n",
        "            for i in range(10):\n",
        "                repeated_evens.append(even_numbers[i % len(even_numbers)])\n",
        "            result = ','.join(map(str, repeated_evens))\n",
        "    elif command == '<odd_repeat>':\n",
        "        odd_numbers = [num for num in numbers if num % 2 == 1]\n",
        "        if not odd_numbers:\n",
        "            result = \"\"\n",
        "        else:\n",
        "            repeated_odds = []\n",
        "            for i in range(10):\n",
        "                repeated_odds.append(odd_numbers[i % len(odd_numbers)])\n",
        "            result = ','.join(map(str, repeated_odds))\n",
        "    else:\n",
        "        return \"Unknown command\"\n",
        "\n",
        "    return f\"<sos> {numbers_str} {command} {result} <eos>\"\n",
        "\n",
        "def run_comprehensive_test(model, tokenizer, device):\n",
        "    \"\"\"Run comprehensive tests on the model\"\"\"\n",
        "\n",
        "    test_cases = [\n",
        "        # Sum tests\n",
        "        \"<sos> 55,50,14 <sum>\",\n",
        "        \"<sos> 123,456,789 <sum>\",\n",
        "        \"<sos> 92,99,1 <sum>\",\n",
        "\n",
        "        # Reverse tests\n",
        "        \"<sos> 82,77,9,57 <reverse>\",\n",
        "        \"<sos> 190,676,440 <reverse>\",\n",
        "        \"<sos> 505,100 <reverse>\",\n",
        "\n",
        "        # Ascending sort tests\n",
        "        \"<sos> 425,616,162 <ascending>\",\n",
        "        \"<sos> 190,676,440 <ascending>\",\n",
        "        \"<sos> 5,231,44 <ascending>\",\n",
        "\n",
        "        # Descending sort tests\n",
        "        \"<sos> 425,616,162,221,244 <descending>\",\n",
        "        \"<sos> 917,35,88 <descending>\",\n",
        "        \"<sos> 48,2,99 <descending>\",\n",
        "\n",
        "        # Even repeat tests\n",
        "        \"<sos> 44,30,61 <even_repeat>\",\n",
        "        \"<sos> 12,37,84 <even_repeat>\",\n",
        "\n",
        "        # Odd repeat tests\n",
        "        \"<sos> 50,82,41 <odd_repeat>\",\n",
        "        \"<sos> 13,28,55 <odd_repeat>\",\n",
        "    ]\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"COMPREHENSIVE MODEL TESTING\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i, test_case in enumerate(test_cases):\n",
        "        print(f\"\\nTest {i+1}: {test_case}\")\n",
        "\n",
        "        # Get model prediction\n",
        "        prediction, tokens = test_model_completion(model, tokenizer, device, test_case)\n",
        "        print(f\"Model output: {prediction}\")\n",
        "\n",
        "        # Calculate expected result\n",
        "        expected = calculate_expected_result(test_case)\n",
        "        print(f\"Expected:     {expected}\")\n",
        "\n",
        "        # Check if correct\n",
        "        is_correct = prediction.strip() == expected.strip()\n",
        "        print(f\"Correct: {'‚úì' if is_correct else '‚úó'}\")\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"\\nOverall Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
        "\n",
        "    return correct, total\n",
        "\n",
        "def test_with_generated_examples(model, tokenizer, device, num_tests=20):\n",
        "    \"\"\"Test model using your existing generator functions\"\"\"\n",
        "\n",
        "    # Import/call your existing functions\n",
        "    from your_module import (\n",
        "        generate_random_numbers_and_sum,\n",
        "        generate_even_repeat_sequence,\n",
        "        generate_odd_repeat_sequence,\n",
        "        generate_random_numbers_and_reverse,\n",
        "        generate_ascending_sort,\n",
        "        generate_descending_sort\n",
        "    )\n",
        "\n",
        "    generators = [\n",
        "        generate_random_numbers_and_sum,\n",
        "        generate_even_repeat_sequence,\n",
        "        generate_odd_repeat_sequence,\n",
        "        generate_random_numbers_and_reverse,\n",
        "        generate_ascending_sort,\n",
        "        generate_descending_sort\n",
        "    ]\n",
        "\n",
        "    print(\"=\" * 80)\n",
        "    print(\"TESTING WITH GENERATED EXAMPLES\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for i in range(num_tests):\n",
        "        # Pick random generator\n",
        "        generator = generators[i % len(generators)]\n",
        "\n",
        "        # Generate full example\n",
        "        full_example = generator()\n",
        "\n",
        "        # Split into prompt and expected\n",
        "        parts = full_example.split(' <eos>')\n",
        "        prompt_part = parts[0]  # Everything before <eos>\n",
        "\n",
        "        # Find where the command ends to create prompt\n",
        "        command_tokens = ['<sum>', '<reverse>', '<ascending>', '<descending>', '<even_repeat>', '<odd_repeat>']\n",
        "        prompt = None\n",
        "        expected = full_example\n",
        "\n",
        "        for cmd in command_tokens:\n",
        "            if cmd in prompt_part:\n",
        "                cmd_index = prompt_part.find(cmd)\n",
        "                prompt = prompt_part[:cmd_index + len(cmd)]\n",
        "                break\n",
        "\n",
        "        if prompt is None:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\nTest {i+1}:\")\n",
        "        print(f\"Prompt: {prompt}\")\n",
        "\n",
        "        # Get model prediction\n",
        "        prediction, tokens = test_model_completion(model, tokenizer, device, prompt, max_new_tokens=20)\n",
        "        print(f\"Model output: {prediction}\")\n",
        "        print(f\"Expected:     {expected}\")\n",
        "\n",
        "        # Check if correct\n",
        "        is_correct = prediction.strip() == expected.strip()\n",
        "        print(f\"Correct: {'‚úì' if is_correct else '‚úó'}\")\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "    accuracy = correct / total * 100\n",
        "    print(f\"\\nGenerated Examples Accuracy: {correct}/{total} = {accuracy:.1f}%\")\n",
        "\n",
        "    return correct, total\n",
        "\n",
        "def interactive_test(model, tokenizer, device):\n",
        "    \"\"\"Interactive testing interface\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"INTERACTIVE TESTING\")\n",
        "    print(\"Enter prompts like: <sos> 55,50,14 <sum>\")\n",
        "    print(\"Type 'quit' to exit\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            prompt = input(\"\\nEnter prompt: \").strip()\n",
        "            if prompt.lower() == 'quit':\n",
        "                break\n",
        "\n",
        "            if not prompt:\n",
        "                continue\n",
        "\n",
        "            # Test the prompt\n",
        "            prediction, tokens = test_model_completion(model, tokenizer, device, prompt)\n",
        "            print(f\"Model output: {prediction}\")\n",
        "\n",
        "            # Show expected if possible\n",
        "            try:\n",
        "                expected = calculate_expected_result(prompt)\n",
        "                if \"Invalid\" not in expected and \"Unknown\" not in expected:\n",
        "                    print(f\"Expected:     {expected}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nExiting...\")\n",
        "            break\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "# Main testing function\n",
        "def main():\n",
        "    \"\"\"Run all tests\"\"\"\n",
        "    try:\n",
        "        # Load model\n",
        "        model, tokenizer, device = load_trained_model()\n",
        "\n",
        "        # Run comprehensive tests\n",
        "        run_comprehensive_test(model, tokenizer, device)\n",
        "\n",
        "        # Uncomment to test with generated examples (requires importing your functions)\n",
        "        # test_with_generated_examples(model, tokenizer, device)\n",
        "\n",
        "        # Run interactive test\n",
        "        interactive_test(model, tokenizer, device)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during testing: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUPZpZXKxYtA",
        "outputId": "79804c43-877c-4916-a745-01f5d2c95bf5"
      },
      "id": "yUPZpZXKxYtA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Using fallback model configuration\n",
            "number of parameters: 0.01M\n",
            "‚úì Model loaded from out_number_commands/ckpt.pt\n",
            "‚úì Model has 0.01M parameters\n",
            "‚úì Using device: cuda\n",
            "================================================================================\n",
            "COMPREHENSIVE MODEL TESTING\n",
            "================================================================================\n",
            "\n",
            "Test 1: <sos> 55,50,14 <sum>\n",
            "Error during testing: can't assign a NoneType to a torch.cuda.LongTensor\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import pickle\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_on_validation_set():\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "\n",
        "    # Load metadata\n",
        "    data_dir = 'data/number_commands'\n",
        "    meta_path = os.path.join(data_dir, 'meta.pkl')\n",
        "    with open(meta_path, 'rb') as f:\n",
        "        meta = pickle.load(f)\n",
        "\n",
        "    # Load model\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    checkpoint_path = 'out_number_commands/ckpt.pt'\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "    model_args = checkpoint['model_args']\n",
        "\n",
        "    # Filter to only NumberCommandConfig parameters\n",
        "    config_params = {\n",
        "        'block_size': model_args['block_size'],\n",
        "        'vocab_size': model_args['vocab_size'],\n",
        "        'n_layer': model_args['n_layer'],\n",
        "        'n_head': model_args['n_head'],\n",
        "        'n_embd': model_args['n_embd'],\n",
        "        'dropout': model_args['dropout'],\n",
        "        'bias': model_args['bias']\n",
        "    }\n",
        "\n",
        "    config = NumberCommandConfig(**config_params)\n",
        "    model = NumberCommandTransformer(config)\n",
        "\n",
        "    # Remove compilation prefix if exists\n",
        "    state_dict = checkpoint['model']\n",
        "    unwanted_prefix = '_orig_mod.'\n",
        "    for k,v in list(state_dict.items()):\n",
        "        if k.startswith(unwanted_prefix):\n",
        "            state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "\n",
        "    model.load_state_dict(state_dict)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load validation data\n",
        "    val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "    block_size = config.block_size\n",
        "\n",
        "    print(f\"Model loaded: {model.get_num_params()/1e6:.2f}M parameters\")\n",
        "    print(f\"Validation data size: {len(val_data)} tokens\")\n",
        "\n",
        "    # Evaluate loss on validation set\n",
        "    total_loss = 0\n",
        "    num_batches = 0\n",
        "    batch_size = 32\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(val_data) - block_size, batch_size * block_size):\n",
        "            # Get batch\n",
        "            batch_end = min(i + batch_size * block_size, len(val_data) - block_size)\n",
        "            batch_indices = range(i, batch_end, block_size)\n",
        "\n",
        "            if len(batch_indices) == 0:\n",
        "                break\n",
        "\n",
        "            x = torch.stack([\n",
        "                torch.from_numpy(val_data[j:j+block_size].astype(np.int64))\n",
        "                for j in batch_indices\n",
        "            ])\n",
        "            y = torch.stack([\n",
        "                torch.from_numpy(val_data[j+1:j+1+block_size].astype(np.int64))\n",
        "                for j in batch_indices\n",
        "            ])\n",
        "\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            logits, loss = model(x, y)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "    avg_loss = total_loss / num_batches\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "\n",
        "    print(f\"Validation Loss: {avg_loss:.4f}\")\n",
        "    print(f\"Perplexity: {perplexity:.2f}\")\n"
      ],
      "metadata": {
        "id": "cBC8I5plzu24"
      },
      "id": "cBC8I5plzu24",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "evaluate_on_validation_set()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MIAEmAmxzxqQ",
        "outputId": "8175469a-b201-4f2b-be25-47cbf3e61c4d"
      },
      "id": "MIAEmAmxzxqQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of parameters: 0.01M\n",
            "Model loaded: 0.01M parameters\n",
            "Validation data size: 198000 tokens\n",
            "Validation Loss: 1.9876\n",
            "Perplexity: 7.30\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}